{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Document Structure\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'exmaple.txt', 'pages': 1, 'author': 'Krish Naik', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"chapter 1.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Ratnam Ojha\",\n",
    "        \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "#the benefit of having metadata in our document, is that we can filter our search results based on metadata fields.\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a txt file\n",
    "import os \n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created successfully.\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,\"w\") as f:\n",
    "        f.write(content)\n",
    "print(\"Sample text files created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we could've added .txt files manually as well but masti nahi rukni chahiye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### reading these files using TextLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    ')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory loader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "dir_loader = DirectoryLoader( \n",
    "    \"../data/text_files\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\":\"utf8\"},\n",
    "    show_progress = False)\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 0}, page_content='Speech and Language Processing.\\nDaniel Jurafsky & James H. Martin.\\nCopyright © 2025.\\nAll\\nrights reserved.\\nDraft of August 24, 2025.\\nCHAPTER\\n6\\nNeural Networks\\n“[M]achines of this character can behave in a very complicated manner when\\nthe number of units is large.”\\nAlan Turing (1948) “Intelligent Machines”, page 6\\nNeural networks are a fundamental computational tool for language process-\\ning, and a very old one. They are called neural because their origins lie in the\\nMcCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the\\nbiological neuron as a kind of computing element that could be described in terms\\nof propositional logic. But the modern use in language processing no longer draws\\non these early biological inspirations.\\nInstead, a modern neural network is a network of small computing units, each\\nof which takes a vector of input values and produces a single output value. In this\\nchapter we introduce the neural net applied to classiﬁcation. The architecture we\\nintroduce is called a feedforward network because the computation proceeds iter-\\nfeedforward\\natively from one layer of units to the next. The use of modern neural nets is often\\ncalled deep learning, because modern networks are often deep (have many layers).\\ndeep learning\\nNeural networks share much of the same mathematics as logistic regression. But\\nneural networks are a more powerful classiﬁer than logistic regression, and indeed a\\nminimal neural network (technically one with a single ‘hidden layer’) can be shown\\nto learn any function.\\nNeural net classiﬁers are different from logistic regression in another way. With\\nlogistic regression, we applied the regression classiﬁer to many different tasks by\\ndeveloping many rich kinds of feature templates based on domain knowledge. When\\nworking with neural networks, it is more common to avoid most uses of rich hand-\\nderived features, instead building neural networks that take raw tokens as inputs\\nand learn to induce features as part of the process of learning to classify. We saw\\nexamples of this kind of representation learning for embeddings in Chapter 5, and\\nwe’ll see lots of examples once we start studying deep transformers networks. Nets\\nthat are very deep are particularly good at representation learning. For that reason\\ndeep neural nets are the right tool for tasks that offer sufﬁcient data to learn features\\nautomatically.\\nIn this chapter we’ll introduce feedforward networks as classiﬁers, ﬁrst with\\nhand-built features, and then using the embeddings that we studied in Chapter 5.\\nIn subsequent chapters we’ll introduce many other kinds of neural models, most\\nimportantly the transformer and attention, (Chapter 8), but also recurrent neural\\nnetworks (Chapter 13) and convolutional neural networks (Chapter 15). And in\\nthe next chapter we’ll introduce the paradigm of neural large language models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 1}, page_content='2\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\n6.1\\nUnits\\nThe building block of a neural network is a single computational unit. A unit takes\\na set of real valued numbers as input, performs some computation on them, and\\nproduces an output.\\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one addi-\\ntional term in the sum called a bias term. Given a set of inputs x1...xn, a unit has\\nbias term\\na set of corresponding weights w1...wn and a bias b, so the weighted sum z can be\\nrepresented as:\\nz = b+\\nX\\ni\\nwixi\\n(6.1)\\nOften it’s more convenient to express this weighted sum using vector notation; recall\\nfrom linear algebra that a vector is, at heart, just a list or array of numbers. Thus\\nvector\\nwe’ll talk about z in terms of a weight vector w, a scalar bias b, and an input vector\\nx, and we’ll replace the sum with the convenient dot product:\\nz = w ·x+b\\n(6.2)\\nAs deﬁned in Eq. 6.2, z is just a real valued number.\\nFinally, instead of using z, a linear function of x, as the output, neural units\\napply a non-linear function f to z. We will refer to the output of this function as\\nthe activation value for the unit, a. Since we are just modeling a single unit, the\\nactivation\\nactivation for the node is in fact the ﬁnal output of the network, which we’ll generally\\ncall y. So the value y is deﬁned as:\\ny = a = f(z)\\nWe’ll discuss three popular non-linear functions f below (the sigmoid, the tanh, and\\nthe rectiﬁed linear unit or ReLU) but it’s pedagogically convenient to start with the\\nsigmoid function since we saw it in Chapter 4:\\nsigmoid\\ny = σ(z) =\\n1\\n1+e−z\\n(6.3)\\nThe sigmoid (shown in Fig. 6.1) has a number of advantages; it maps the output\\ninto the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s\\ndifferentiable, which as we saw in Section ?? will be handy for learning.\\nFigure 6.1\\nThe sigmoid function takes a real value and maps it to the range (0,1). It is\\nnearly linear around 0 but outlier values get squashed toward 0 or 1.\\nSubstituting Eq. 6.2 into Eq. 6.3 gives us the output of a neural unit:\\ny = σ(w ·x+b) =\\n1\\n1+exp(−(w ·x+b))\\n(6.4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 2}, page_content='6.1\\n•\\nUNITS\\n3\\nFig. 6.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit\\ntakes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each\\nvalue by a weight (w1, w2, and w3, respectively), adds them to a bias term b, and then\\npasses the resulting sum through a sigmoid function to result in a number between 0\\nand 1.\\nx1\\nx2\\nx3\\ny\\nw1\\nw2\\nw3\\n∑\\nb\\nσ\\n+1\\nz\\na\\nFigure 6.2\\nA neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a\\nweight for an input clamped at +1) and producing an output y. We include some convenient\\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\\nthis case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to\\nmean the ﬁnal output of the entire network, leaving a as the activation of an individual node.\\nLet’s walk through an example just to get an intuition. Let’s suppose we have a\\nunit with the following weight vector and bias:\\nw = [0.2,0.3,0.9]\\nb = 0.5\\nWhat would this unit do with the following input vector:\\nx = [0.5,0.6,0.1]\\nThe resulting output y would be:\\ny = σ(w ·x+b) =\\n1\\n1+e−(w·x+b) =\\n1\\n1+e−(.5∗.2+.6∗.3+.1∗.9+.5) =\\n1\\n1+e−0.87 = .70\\nIn practice, the sigmoid is not commonly used as an activation function. A function\\nthat is very similar but almost always better is the tanh function shown in Fig. 6.3a;\\ntanh\\ntanh is a variant of the sigmoid that ranges from -1 to +1:\\ny = tanh(z) = ez −e−z\\nez +e−z\\n(6.5)\\nThe simplest activation function, and perhaps the most commonly used, is the rec-\\ntiﬁed linear unit, also called the ReLU, shown in Fig. 6.3b. It’s just the same as z\\nReLU\\nwhen z is positive, and 0 otherwise:\\ny = ReLU(z) = max(z,0)\\n(6.6)\\nThese activation functions have different properties that make them useful for differ-\\nent language applications or network architectures. For example, the tanh function\\nhas the nice properties of being smoothly differentiable and mapping outlier values\\ntoward the mean. The rectiﬁer function, on the other hand, has nice properties that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 3}, page_content='4\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\n(a)\\n(b)\\nFigure 6.3\\nThe tanh and ReLU activation functions.\\nresult from it being very close to linear. In the sigmoid or tanh functions, very high\\nvalues of z result in values of y that are saturated, i.e., extremely close to 1, and have\\nsaturated\\nderivatives very close to 0. Zero derivatives cause problems for learning, because as\\nwe’ll see in Section 6.6, we’ll train networks by propagating an error signal back-\\nwards, multiplying gradients (partial derivatives) from each layer of the network;\\ngradients that are almost 0 cause the error signal to get smaller and smaller until it is\\ntoo small to be used for training, a problem called the vanishing gradient problem.\\nvanishing\\ngradient\\nRectiﬁers don’t have this problem, since the derivative of ReLU for high values of z\\nis 1 rather than very close to 0.\\n6.2\\nThe XOR problem\\nEarly in the history of neural networks it was realized that the power of neural net-\\nworks, as with the real neurons that inspired them, comes from combining these\\nunits into larger networks.\\nOne of the most clever demonstrations of the need for multi-layer networks was\\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\\nsome very simple functions of its input. Consider the task of computing elementary\\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\\nthe truth tables for those functions:\\nAND\\nOR\\nXOR\\nx1 x2 y\\nx1 x2 y\\nx1 x2 y\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n0\\nThis example was ﬁrst shown for the perceptron, which is a very simple neural\\nperceptron\\nunit that has a binary output and has a very simple step function as its non-linear\\nactivation function. The output y of a perceptron is 0 or 1, and is computed as\\nfollows (using the same weight w, input x, and bias b as in Eq. 6.2):\\ny =\\n\\x1a 0, if w ·x+b ≤0\\n1, if w ·x+b > 0\\n(6.7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 4}, page_content='6.2\\n•\\nTHE XOR PROBLEM\\n5\\nIt’s very easy to build a perceptron that can compute the logical AND and OR\\nfunctions of its binary inputs; Fig. 6.4 shows the necessary weights.\\nx1\\nx2\\n+1\\n-1\\n1\\n1\\nx1\\nx2\\n+1\\n0\\n1\\n1\\n(a)\\n(b)\\nFigure 6.4\\nThe weights w and bias b for perceptrons for computing logical functions. The\\ninputs are shown as x1 and x2 and the bias as a special node with value +1 which is multiplied\\nwith the bias weight b. (a) logical AND, with weights w1 = 1 and w2 = 1 and bias weight\\nb = −1. (b) logical OR, with weights w1 = 1 and w2 = 1 and bias weight b = 0. These\\nweights/biases are just one from an inﬁnite number of possible sets of weights and biases that\\nwould implement the functions.\\nIt turns out, however, that it’s not possible to build a perceptron to compute\\nlogical XOR! (It’s worth spending a moment to give it a try!)\\nThe intuition behind this important result relies on understanding that a percep-\\ntron is a linear classiﬁer. For a two-dimensional input x1 and x2, the perceptron\\nequation, w1x1 +w2x2 +b = 0 is the equation of a line. (We can see this by putting\\nit in the standard linear format: x2 = (−w1/w2)x1 + (−b/w2).) This line acts as a\\ndecision boundary in two-dimensional space in which the output 0 is assigned to all\\ndecision\\nboundary\\ninputs lying on one side of the line, and the output 1 to all input points lying on the\\nother side of the line. If we had more than 2 inputs, the decision boundary becomes\\na hyperplane instead of a line, but the idea is the same, separating the space into two\\ncategories.\\nFig. 6.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn\\nby one possible set of parameters for an AND and an OR classiﬁer. Notice that there\\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)\\nfrom the negative cases (00 and 11). We say that XOR is not a linearly separable\\nlinearly\\nseparable\\nfunction. Of course we could draw a boundary with a curve, or some other function,\\nbut not a single line.\\n6.2.1\\nThe solution: neural networks\\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\\nculated by a layered network of perceptron units. Rather than see this with networks\\nof simple perceptrons, however, let’s see how to compute XOR using two layers of\\nReLU-based units following Goodfellow et al. (2016). Fig. 6.6 shows a ﬁgure with\\nthe input being processed by two layers of neural units. The middle layer (called\\nh) has two units, and the output layer (called y) has one unit. A set of weights and\\nbiases are shown that allows the network to correctly compute the XOR function.\\nLet’s walk through what happens with the input x = [0, 0]. If we multiply each\\ninput value by the appropriate weight, sum, and then add the bias b, we get the vector\\n[0, -1], and we then apply the rectiﬁed linear transformation to give the output of the\\nh layer as [0, 0]. Now we once again multiply by the weights, sum, and add the\\nbias (0 in this case) resulting in the value 0. The reader should work through the\\ncomputation of the remaining 3 possible input pairs to see that the resulting y values\\nare 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 5}, page_content='6\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\n0\\n0\\n1\\n1\\nx1\\nx2\\n0\\n0\\n1\\n1\\nx1\\nx2\\n0\\n0\\n1\\n1\\nx1\\nx2\\na)  x1 AND x2\\nb)  x1 OR x2\\nc)  x1 XOR x2\\n?\\nFigure 6.5\\nThe functions AND, OR, and XOR, represented with input x1 on the x-axis and input x2 on the\\ny-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\\n(2002).\\nx1\\nx2\\nh1\\nh2\\ny1\\n+1\\n1\\n-1\\n1\\n1\\n1\\n-2\\n0\\n1\\n+1\\n0\\nFigure 6.6\\nXOR solution after Goodfellow et al. (2016). There are three ReLU units, in\\ntwo layers; we’ve called them h1, h2 (h for “hidden layer”) and y1. As before, the numbers\\non the arrows represent the weights w for each unit, and we represent the bias b as a weight\\non a unit clamped to +1, with the bias weights/units in gray.\\nIt’s also instructive to look at the intermediate results, the outputs of the two\\nhidden nodes h1 and h2. We showed in the previous paragraph that the h vector for\\nthe inputs x = [0, 0] was [0, 0]. Fig. 6.7b shows the values of the h layer for all\\n4 inputs. Notice that hidden representations of the two input points x = [0, 1] and\\nx = [1, 0] (the two cases with XOR output = 1) are merged to the single point h =\\n[1, 0]. The merger makes it easy to linearly separate the positive and negative cases\\nof XOR. In other words, we can view the hidden layer of the network as forming a\\nrepresentation of the input.\\nIn this example we just stipulated the weights in Fig. 6.6. But for real examples\\nthe weights for neural networks are learned automatically using the error backprop-\\nagation algorithm to be introduced in Section 6.6. That means the hidden layers will\\nlearn to form useful representations. This intuition, that neural networks can auto-\\nmatically learn useful representations of the input, is one of their key advantages,\\nand one that we will return to again and again in later chapters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 6}, page_content='6.3\\n•\\nFEEDFORWARD NEURAL NETWORKS\\n7\\n0\\n0\\n1\\n1\\nx1\\nx2\\na) The original x space\\n0\\n0\\n1\\n1\\nh1\\nh2\\n2\\nb) The new (linearly separable) h space\\nFigure 6.7\\nThe hidden layer forming a new representation of the input. (b) shows the\\nrepresentation of the hidden layer, h, compared to the original input representation x in (a).\\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\\npossible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.\\n(2016).\\n6.3\\nFeedforward Neural Networks\\nLet’s now walk through a slightly more formal presentation of the simplest kind of\\nneural network, the feedforward network. A feedforward network is a multilayer\\nfeedforward\\nnetwork\\nnetwork in which the units are connected with no cycles; the outputs from units in\\neach layer are passed to units in the next higher layer, and no outputs are passed\\nback to lower layers. (In Chapter 13 we’ll introduce networks with cycles, called\\nrecurrent neural networks.)\\nFor historical reasons multilayer networks, especially feedforward networks, are\\nsometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,\\nmulti-layer\\nperceptrons\\nMLP\\nsince the units in modern multilayer networks aren’t perceptrons (perceptrons have a\\nsimple step-function as their activation function, but modern networks are made up\\nof units with many kinds of non-linearities like ReLUs and sigmoids), but at some\\npoint the name stuck.\\nSimple feedforward networks have three kinds of nodes: input units, hidden\\nunits, and output units.\\nFig. 6.8 shows a picture. The input layer x is a vector of simple scalar values just\\nas we saw in Fig. 6.2.\\nThe core of the neural network is the hidden layer h formed of hidden units hi,\\nhidden layer\\neach of which is a neural unit as described in Section 6.1, taking a weighted sum of\\nits inputs and then applying a non-linearity. In the standard architecture, each layer\\nis fully-connected, meaning that each unit in each layer takes as input the outputs\\nfully-connected\\nfrom all the units in the previous layer, and there is a link between every pair of units\\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.\\nRecall that a single hidden unit has as parameters a weight vector and a bias. We\\nrepresent the parameters for the entire hidden layer by combining the weight vector\\nand bias for each unit i into a single weight matrix W and a single bias vector b for\\nthe whole layer (see Fig. 6.8). Each element Wji of the weight matrix W represents\\nthe weight of the connection from the ith input unit xi to the jth hidden unit hj.\\nThe advantage of using a single matrix W for the weights of the entire layer is\\nthat now the hidden layer computation for a feedforward network can be done very'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 7}, page_content='8\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\nx1\\nx2\\nxn0\\n…\\n…\\n+1\\nb\\n…\\nU\\nW\\ninput layer\\nhidden layer\\noutput layer\\nh1\\ny1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\nFigure 6.8\\nA simple 2-layer feedforward network, with one hidden layer, one output layer,\\nand one input layer (the input layer is usually not counted when enumerating layers).\\nefﬁciently with simple matrix operations. In fact, the computation only has three\\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,\\nand applying the activation function g (such as the sigmoid, tanh, or ReLU activation\\nfunction deﬁned above).\\nThe output of the hidden layer, the vector h, is thus the following (for this exam-\\nple we’ll use the sigmoid function σ as our activation function):\\nh = σ(Wx+b)\\n(6.8)\\nNotice that we’re applying the σ function here to a vector, while in Eq. 6.3 it was\\napplied to a scalar. We’re thus allowing σ(·), and indeed any activation function\\ng(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].\\nLet’s introduce some constants to represent the dimensionalities of these vectors\\nand matrices. We’ll refer to the input layer as layer 0 of the network, and have\\nn0 represent the number of inputs, so x is a vector of real numbers of dimension\\nn0, or more formally x ∈Rn0, a column vector of dimensionality [n0 × 1]. Let’s\\ncall the hidden layer layer 1 and the output layer layer 2. The hidden layer has\\ndimensionality n1, so h ∈Rn1 and also b ∈Rn1 (since each hidden unit can take a\\ndifferent bias value). And the weight matrix W has dimensionality W ∈Rn1×n0, i.e.\\n[n1 ×n0].\\nTake a moment to convince yourself that the matrix multiplication in Eq. 6.8 will\\ncompute the value of each h j as σ\\n\\x00Pn0\\ni=1 Wjixi +b j\\n\\x01\\n.\\nAs we saw in Section 6.2, the resulting value h (for hidden but also for hypoth-\\nesis) forms a representation of the input. The role of the output layer is to take\\nthis new representation h and compute a ﬁnal output. This output could be a real-\\nvalued number, but in many cases the goal of the network is to make some sort of\\nclassiﬁcation decision, and so we will focus on the case of classiﬁcation.\\nIf we are doing a binary task like sentiment classiﬁcation, we might have a sin-\\ngle output node, and its scalar value y is the probability of positive versus negative\\nsentiment. If we are doing multinomial classiﬁcation, such as assigning a part-of-\\nspeech tag, we might have one output node for each potential part-of-speech, whose\\noutput value is the probability of that part-of-speech, and the values of all the output\\nnodes must sum to one. The output layer is thus a vector y that gives a probability\\ndistribution across the output nodes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 8}, page_content='6.3\\n•\\nFEEDFORWARD NEURAL NETWORKS\\n9\\nLet’s see how this happens. Like the hidden layer, the output layer has a weight\\nmatrix (let’s call it U), but some models don’t include a bias vector b in the output\\nlayer, so we’ll simplify by eliminating the bias vector in this example. The weight\\nmatrix is multiplied by its input vector (h) to produce the intermediate output z:\\nz = Uh\\nThere are n2 output nodes, so z ∈Rn2, weight matrix U has dimensionality U ∈\\nRn2×n1, and element Ui j is the weight from unit j in the hidden layer to unit i in the\\noutput layer.\\nHowever, z can’t be the output of the classiﬁer, since it’s a vector of real-valued\\nnumbers, while what we need for classiﬁcation is a vector of probabilities. There is\\na convenient function for normalizing a vector of real values, by which we mean\\nnormalizing\\nconverting it to a vector that encodes a probability distribution (all the numbers lie\\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page ?? of\\nsoftmax\\nChapter 4. More generally for any vector z of dimensionality d, the softmax is\\ndeﬁned as:\\nsoftmax(zi) =\\nexp(zi)\\nPd\\nj=1 exp(z j)\\n1 ≤i ≤d\\n(6.9)\\nThus for example given a vector\\nz = [0.6,1.1,−1.5,1.2,3.2,−1.1],\\n(6.10)\\nthe softmax function will normalize it to a probability distribution (shown rounded):\\nsoftmax(z) = [0.055,0.090,0.0067,0.10,0.74,0.010]\\n(6.11)\\nYou may recall that we used softmax to create a probability distribution from a\\nvector of real-valued numbers (computed from summing weights times features) in\\nthe multinomial version of logistic regression in Chapter 4.\\nThat means we can think of a neural network classiﬁer with one hidden layer\\nas building a vector h which is a hidden layer representation of the input, and then\\nrunning standard multinomial logistic regression on the features that the network\\ndevelops in h. By contrast, in Chapter 4 the features were mainly designed by hand\\nvia feature templates. So a neural network is like multinomial logistic regression,\\nbut (a) with many layers, since a deep neural network is like layer after layer of lo-\\ngistic regression classiﬁers; (b) with those intermediate layers having many possible\\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll\\ncontinue to use σ for convenience to mean any activation function); (c) rather than\\nforming the features by feature templates, the prior layers of the network induce the\\nfeature representations themselves.\\nHere are the ﬁnal equations for a feedforward network with a single hidden layer,\\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-\\nized by weight matrices W and U and a bias vector b:\\nh = σ(Wx+b)\\nz = Uh\\ny = softmax(z)\\n(6.12)\\nAnd just to remember the shapes of all our variables, x ∈Rn0, h ∈Rn1, b ∈Rn1,\\nW ∈Rn1×n0, U ∈Rn2×n1, and the output vector y ∈Rn2. We’ll call this network a 2-\\nlayer network (we traditionally don’t count the input layer when numbering layers,\\nbut do count the output layer). So by this terminology logistic regression is a 1-layer\\nnetwork.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 9}, page_content='10\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\n6.3.1\\nMore details on feedforward networks\\nLet’s now set up some notation to make it easier to talk about deeper networks of\\ndepth more than 2. We’ll use superscripts in square brackets to mean layer num-\\nbers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the\\n(ﬁrst) hidden layer, and b[1] will mean the bias vector for the (ﬁrst) hidden layer. n j\\nwill mean the number of units at layer j. We’ll use g(·) to stand for the activation\\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\\nfor output layers. We’ll use a[i] to mean the output from layer i, and z[i] to mean the\\ncombination of previous layer output, weights and biases W[i]a[i−1] + b[i]. The 0th\\nlayer is for inputs, so we’ll refer to the inputs x more generally as a[0].\\nThus we can re-represent our 2-layer net from Eq. 6.12 as follows:\\nz[1] = W[1]a[0] +b[1]\\na[1] = g[1](z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = g[2](z[2])\\nˆy = a[2]\\n(6.13)\\nNote that with this notation, the equations for the computation done at each layer are\\nthe same. The algorithm for computing the forward step in an n-layer feedforward\\nnetwork, given the input vector a[0] is thus simply:\\nfor i in 1,...,n\\nz[i] = W[i] a[i−1] + b[i]\\na[i] = g[i](z[i])\\nˆy = a[n]\\nIt’s often useful to have a name for the ﬁnal set of activations right before the ﬁnal\\nsoftmax. So however many layers we have, we’ll generally call the unnormalized\\nvalues in the ﬁnal vector z[n], the vector of scores right before the ﬁnal softmax, the\\nlogits (see Eq. ??).\\nlogits\\nThe need for non-linear activation functions\\nOne of the reasons we use non-\\nlinear activation functions for each layer in a neural network is that if we did not, the\\nresulting network is exactly equivalent to a single-layer network. Let’s see why this\\nis true. Imagine the ﬁrst two layers of such a network of purely linear layers:\\nz[1] = W[1]x+b[1]\\nz[2] = W[2]z[1] +b[2]\\nWe can rewrite the function that the network is computing as:\\nz[2] = W[2]z[1] +b[2]\\n= W[2](W[1]x+b[1])+b[2]\\n= W[2]W[1]x+W[2]b[1] +b[2]\\n= W′x+b′\\n(6.14)\\nThis generalizes to any number of layers. So without non-linear activation functions,\\na multilayer network is just a notational variant of a single layer network with a\\ndifferent set of weights, and we lose all the representational power of multilayer\\nnetworks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 10}, page_content='6.4\\n•\\nFEEDFORWARD NETWORKS FOR NLP: CLASSIFICATION\\n11\\nReplacing the bias unit\\nIn describing networks, we will sometimes use a slightly\\nsimpliﬁed notation that represents exactly the same function without referring to an\\nexplicit bias node b. Instead, we add a dummy node a0 to each layer whose value\\nwill always be 1. Thus layer 0, the input layer, will have a dummy node a[0]\\n0 = 1,\\nlayer 1 will have a[1]\\n0 = 1, and so on. This dummy node still has an associated weight,\\nand that weight represents the bias value b. For example instead of an equation like\\nh = σ(Wx+b)\\n(6.15)\\nwe’ll use:\\nh = σ(Wx)\\n(6.16)\\nBut now instead of our vector x having n0 values: x = x1,...,xn0, it will have n0 +\\n1 values, with a new 0th dummy value x0 = 1: x = x0,...,xn0. And instead of\\ncomputing each h j as follows:\\nh j = σ\\n n0\\nX\\ni=1\\nWji xi +b j\\n!\\n,\\n(6.17)\\nwe’ll instead use:\\nh j = σ\\n n0\\nX\\ni=0\\nWji xi\\n!\\n,\\n(6.18)\\nwhere the value Wj0 replaces what had been b j. Fig. 6.9 shows a visualization.\\nx1\\nx2\\nxn0\\n…\\n…\\n+1\\nb\\n…\\nU\\nW\\nh1\\ny1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\nx1\\nx2\\nxn0\\n…\\n…\\nx0=1\\n…\\nU\\nW\\nh1\\ny1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\n(a)\\n(b)\\nFigure 6.9\\nReplacing the bias node (shown in a) with x0 (b).\\nWe’ll continue showing the bias as b when we go over the learning algorithm\\nin Section 6.6, but going forward in the book, for most ﬁgures and some equations\\nwe’ll use this simpliﬁed notation without explicit bias terms.\\n6.4\\nFeedforward networks for NLP: Classiﬁcation\\nLet’s see how to apply feedforward networks to NLP classiﬁcation tasks. In practice,\\nsimple feedforward networks aren’t the way we do text classiﬁcation; for real appli-\\ncations we would use more sophisticated architectures like the BERT transformers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 11}, page_content='12\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\nof Chapter 10. Nonetheless seeing a feedforward network text classiﬁer will let us\\nintroduce key ideas that will play a role throughout the rest of the book, includ-\\ning the ideas of the embedding matrix, representation pooling, and representation\\nlearning.\\nBut before introducing any of these ideas, let’s start with a classiﬁer by making\\nonly minimal change from the sentiment classiﬁers we saw in Chapter 4. Like them,\\nwe’ll take hand-built features, pass them through a classiﬁer, and produce a class\\nprobability. The only difference is that we’ll use a neural network instead of logistic\\nregression as the classiﬁer.\\n6.4.1\\nNeural net classiﬁers with hand-built features\\nLet’s begin with a simple 2-layer sentiment classiﬁer by taking our logistic regres-\\nsion classiﬁer from Chapter 4, which corresponds to a 1-layer network, and just\\nadding a hidden layer. The input element xi can be scalar features like those in\\nFig. ??, e.g., x1 = count(words ∈doc), x2 = count(positive lexicon words ∈doc),\\nx3 = 1 if “no” ∈doc, and so on, for a total of d features. And the output layer\\nˆy could have two nodes (one each for positive and negative), or 3 nodes (positive,\\nnegative, neutral), in which case ˆy1 would be the estimated probability of positive\\nsentiment, ˆy2 the probability of negative and ˆy3 the probability of neutral. The re-\\nsulting equations would be just what we saw above for a 2-layer network (as always,\\nwe’ll continue to use the σ to stand for any non-linearity, whether sigmoid, ReLU\\nor other).\\nx = [x1,x2,...xd]\\n(each xi is a hand-designed feature)\\nh = σ(Wx+b)\\nz = Uh\\nˆy = softmax(z)\\n(6.19)\\nFig. 6.10 shows a sketch of this architecture. As we mentioned earlier, adding this\\nhidden layer to our logistic regression classiﬁer allows the network to represent the\\nnon-linear interactions between features. This alone might give us a better sentiment\\nclassiﬁer.\\nU\\nW\\n[d⨉1]\\nHidden layer\\nOutput layer\\nsoftmax\\n[dh⨉d]\\n[dh⨉1]\\n[3⨉dh]\\nInput words\\np(+)\\nh1\\nh2\\nh3\\nhdh\\n…\\ny1\\n^\\ny2\\n^\\ny3\\n^\\nx\\nh\\ny\\nInput layer \\nd=3 features\\n[3⨉1]\\nx1\\nx2\\nx3\\ndessert\\nwas\\ngreat\\npositive lexicon\\nwords = 1\\ncount of “no” \\n= 0\\nwordcount\\n=3\\np(-)\\np(neut)\\nFigure 6.10\\nFeedforward network sentiment analysis using traditional hand-built features\\nof the input text.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 12}, page_content='6.5\\n•\\nEMBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS\\n13\\n6.4.2\\nVectorizing for parallelizing inference\\nWhile Eq. 6.19 shows how to classify a single example x, in practice we want to\\nefﬁciently classify an entire test set of m examples. We do this by vectorizing the\\nprocess, just as we saw with logistic regression; instead of using for-loops to go\\nthrough each example, we’ll use matrix multiplication to do the entire computation\\nof an entire test set at once. First, we pack all the input feature vectors for each input\\nx into a single input matrix X, with each row i a row vector consisting of the features\\nfor input example x(i) (i.e., the vector x(i)). If the dimensionality of our input feature\\nvector is d, X will be a matrix of shape [m×d].\\nBecause we are now modeling each input as a row vector rather than a column\\nvector, we also need to slightly modify Eq. 6.19. X is of shape [m×d] and W is of\\nshape [dh ×d], so we’ll reorder how we multiply X and W and transpose W so they\\ncorrectly multiply to yield a matrix H of shape [m×dh]. 1\\nThe bias vector b from Eq. 6.19 of shape [1×dh] will now have to be replicated\\ninto a matrix of shape [m × dh]. We’ll need to similarly reorder the next step and\\ntranspose U. Finally, our output matrix ˆY will be of shape [m × 3] (or more gen-\\nerally [m × do], where do is the number of output classes), with each row i of our\\noutput matrix ˆY consisting of the output vector ˆy(i). Here are the ﬁnal equations for\\ncomputing the output class distribution for an entire test set:\\nH = σ(XW⊺+b)\\nZ = HU⊺\\nˆY = softmax(Z)\\n(6.20)\\nIn this book, we’ll sometimes see orderings like WX + b and sometimes XW + b.\\nThat’s why it’s always important to be very aware of the shapes of your weight\\nmatrices participating in any given equation.\\n6.5\\nEmbeddings as the input to neural net classiﬁers\\nWhile hand-built features are a traditional way to design classiﬁers, most applica-\\ntions of neural networks for NLP don’t use hand-built human-engineered features as\\ninputs. Instead, we draw on deep learning’s ability to learn features from the data by\\nrepresenting tokens as embeddings. For this section we’ll represent each token by\\nits static word2vec or GloVe embeddings that we saw how to compute in Chapter 5.\\nBy static embedding, we mean that each token is represented by a ﬁxed vector that\\nwe train once, and then just put into a big dictionary. When we want to refer to that\\ntoken, we grab its embedding out of the dictionary.\\nHowever when we apply neural models to the task of language modeling (as\\nwe’ll see in Chapter 8) the situation is more complex, and we’ll use a more power-\\nful kind of embedding called a contextual embedding. Contextual embeddings are\\ndifferent for each time a word occurs in a different context. Furthermore, we’ll have\\nthe network learn these embeddings as part of the task of word prediction.\\nSo let’s explore the text classiﬁcation domain above, but using static embeddings\\nas features instead of the hand-designed features. Let’s focus on the inference stage,\\n1\\nNote that we could have kept the original order of our products if we had instead made our input\\nmatrix X represent each input as a column vector instead of a row vector, making it of shape [d ×m]. But\\nrepresenting inputs as row vectors is convenient and common in neural network models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 13}, page_content='14\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\nin which we have already learned embeddings for all the input tokens. An embed-\\nding is a vector of dimension d that represents the input token. The dictionary of\\nstatic embeddings in which we store these embeddings is the embedding matrix\\nembedding\\nmatrix\\nE. Each row of the embedding matrix represents each token of the vocabulary V\\nas a (row) vector of dimensionality d. Since E has a row for each of the |V| to-\\nkens in the vocabulary, E has shape [|V|×d]. This embedding matrix E plays a role\\nwhenever we are using embeddings as input to neural NLP systems, including in the\\ntransformer-based large language models we will introduce over the next chapters.\\nGiven an input token string like dessert was great we ﬁrst convert the tokens\\ninto vocabulary indices (these were created when we ﬁrst tokenized the input using\\nBPE or SentencePiece). So the representation of dessert was great might be\\nw = [3,9824,226]. Next we use indexing to select the corresponding rows from E\\n(row 3, row 4000, row 10532).\\nAnother way to think about selecting token embeddings from the embedding\\nmatrix is to represent input tokens as one-hot vectors of shape [1 × |V|], i.e., with\\none dimension for each word in the vocabulary. Recall that in a one-hot vector all\\none-hot vector\\nthe elements are 0 except one, the element whose dimension is the word’s index\\nin the vocabulary, which has value 1. So if the word “dessert” has index 3 in the\\nvocabulary, x3 = 1, and xi = 0 ∀i ̸= 3, as shown here:\\n[0 0 1 0 0 0 0 ... 0 0 0 0]\\n1 2 3 4 5 6 7 ...\\n... |V|\\nMultiplying by a one-hot vector that has only one non-zero element xi = 1 simply\\nselects out the relevant row vector for word i, resulting in the embedding for word i,\\nas depicted in Fig. 6.11.\\nE\\n|V|\\nd\\n1\\n|V|\\nd\\n=\\n✕\\n3\\n3\\n0 0 1 0 0 0 0 … 0 0 0 0 \\n1\\nFigure 6.11\\nSelecting the embedding vector for word V3 by multiplying the embedding\\nmatrix E with a one-hot vector with a 1 in index 3.\\nWe can extend this idea to represent the entire input token sequence as a matrix\\nof one-hot vectors, one for each of the N input positions as shown in Fig. 6.12.\\nE\\n|V|\\nd\\nd\\nN\\n=\\n✕\\n|V|\\nN\\n0 0 0 0 0 0 0 … 0 0 1 0 \\n0 0 1 0 0 0 0 … 0 0 0 0 \\n1 0 0 0 0 0 0 … 0 0 0 0 \\n0 0 0 0 1 0 0 … 0 0 0 0 \\n…\\nFigure 6.12\\nSelecting the embedding matrix for the input sequence of token ids W by mul-\\ntiplying a one-hot matrix corresponding to W by the embedding matrix E.\\nWe now need to classify this input of N [1×d] embeddings, representing a win-\\ndow of N tokens, into a single class (like positive or negative).\\nThere are two common ways to to pass embeddings to a classiﬁer: concate-\\nnation and pooling. First, we can take this input of shape [N × d] and reshape it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 14}, page_content='6.5\\n•\\nEMBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS\\n15\\nby concatenating all the input vectors into one very long vector of shape [1×dN].\\nThen we pass this input to our classiﬁer and let it make its decision. This gives\\nus lots of information, at the cost of using a pretty large network. Second, we can\\npool the N embeddings into a single embedding and then pass that single pooled\\npool\\nembedding to the classiﬁer. Pooling gives us less information than would have been\\npresent in all the original embeddings, but has the advantage of being small and ef-\\nﬁcient and is especially useful in tasks for which we don’t care as much about the\\noriginal word order. Let’s give an example of each: pooling for the sentiment task,\\nand concatenation for the language modeling task.\\nPooling input embeddings for sentiment\\nSo let’s begin with seeing how pooling\\ncan work for the sentiment classiﬁcation task. The intuition of pooling is that for\\nsentiment, the exact position of the input (is some word like great the ﬁrst word?\\nthe second word?) is less important than the identity of the word itself.\\nA pooling function is a way to turn a set of embeddings into a single embedding.\\nFor example, for a text with N input words/tokens w1,...,wN, we want to turn\\nthe N row embeddings e(w1),...,e(wN) (each of dimensionality d) into a single\\nembedding also of dimensionality d.\\nThere are various ways to pool. The simplest is mean-pooling: taking the mean\\nmean-pooling\\nby summing the embeddings and then dividing by N:\\nxmean = 1\\nN\\nN\\nX\\ni=1\\ne(wi)\\n(6.21)\\nHere are the equations for this classiﬁer assuming mean pooling:\\nx = mean(e(w1),e(w2),...,e(wn))\\nh = σ(xW +b)\\nz = hU\\nˆy = softmax(z)\\n(6.22)\\nThe architecture is sketched in Fig. 6.13, where we also give the shapes for all the\\nrelevant matrices.\\nThere are many other options for pooling, like max-pooling, in which case for\\nmax-pooling\\neach dimension we take the element-wise max over all the inputs. The element-wise\\nmax of a set of N vectors is a new vector whose kth element is the max of the kth\\nelements of all the N vectors.\\nConcatenating input embeddings for language modeling\\nFor sentiment analy-\\nsis we saw how to generate an output vector with probabilities over three classes:\\npositive, negative, or neutral, given as input a window of N input tokens, by ﬁrst\\npooling those token embeddings into a single embedding vector.\\nNow let’s consider language modeling: predicting upcoming words from prior\\nwords. In this task we are given the same window of N input tokens, but our task\\nnow is to predict the next token that should follow the window. We’ll sketch a\\nsimple feedforward neural language model, drawing on an algorithm ﬁrst introduced\\nby Bengio et al. (2003). The feedforward language model introduces many of the\\nimportant concepts of large language modeling that we will return to in Chapter 7\\nand Chapter 8.\\nNeural language models have many advantages over the n-gram language mod-\\nels of Chapter 3. Neural language models can handle much longer histories, can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 15}, page_content='16\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\n“dessert” = V3\\n“was” = V524\\n“great” = V902\\nembedding for “dessert”\\nembedding for “was”\\nembedding for “great”\\nU\\nW\\n[1⨉d]\\nHidden layer\\nOutput layer\\n[d⨉dh]\\n[1⨉dh]\\n[dh⨉3]\\nInput words\\np(+)\\nh1\\nh2\\nh3\\nhdh\\n…\\ny1\\n^\\ny2\\n^\\ny3\\n^\\nx\\nh\\ny\\nInput layer \\n[1⨉3]\\npooling\\n+\\np(-) p(neut)\\nembeddings\\none-hot vectors\\ndessert\\nwas\\ngreat\\nN⨉d\\n0 0\\n1\\n0\\n0\\n1\\n|V|\\n3\\n0 0\\n1\\n0\\n0\\n1\\n|V|\\n902\\n0 0\\n1\\n0\\n0\\n1\\n|V|\\n524\\n0\\n0\\nE\\nN⨉|V|\\n|V|⨉d\\nE\\nE\\nE matrix\\nshared across words\\nOutput probabilities\\nweights\\nweights\\nsoftmax\\npooled embedding\\nFigure 6.13\\nFeedforward network sentiment analysis using a pooled embedding of the input words. At each\\ntimestep the network computes a d-dimensional embedding for each context word (by multiplying a one-hot\\nvector by the embedding matrix E), and pools the resulting N embeddings to get a single embedding that\\nrepresents the context window as the layer e.\\ngeneralize better over contexts of similar words, and are far more accurate at word-\\nprediction. On the other hand, neural net language models are slower, more com-\\nplex, need vast amounts of energy to train, and are less interpretable than n-gram\\nmodels, so for some smaller tasks an n-gram language model is still the right tool.\\nA feedforward neural language model is a feedforward network that takes as\\ninput at time t a representation of some number of previous words (wt−1,wt−2, etc.)\\nand outputs a probability distribution over possible next words. Thus—like the n-\\ngram LM—the feedforward neural LM approximates the probability of a word given\\nthe entire prior context P(wt|w1:t−1) by approximating based on the N −1 previous\\nwords:\\nP(wt|w1,...,wt−1) ≈P(wt|wt−N+1,...,wt−1)\\n(6.23)\\nIn the following examples we’ll use a 4-gram example, so we’ll show a neural net to\\nestimate the probability P(wt = i|wt−3,wt−2,wt−1).\\nNeural language models represent words in this prior context by their embed-\\ndings, rather than just by their word identity as used in n-gram language models.\\nUsing embeddings allows neural language models to generalize better to unseen\\ndata. For example, suppose we’ve seen this sentence in training:\\nI have to make sure that the cat gets fed.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 16}, page_content='6.5\\n•\\nEMBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS\\n17\\nbut have never seen the words “gets fed” after the word “dog”. Our test set has the\\npreﬁx “I forgot to make sure that the dog gets”. What’s the next word? An n-gram\\nlanguage model will predict “fed” after “that the cat gets”, but not after “that the dog\\ngets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will\\nbe able to generalize from the “cat” context to assign a high enough probability to\\n“fed” even after seeing “dog”.\\nh1\\nh2\\ny1\\nh3\\nhdh\\n…\\n…\\nU\\nW\\ny34\\ny|V|\\nembedding layer e\\n1⨉Nd\\nhidden layer h\\noutput layer y\\nsoftmax\\n…\\n...\\nwt-1\\nwt-2\\nwt\\nwt-3\\nNd⨉dh\\n1⨉dh\\ndh⨉|V|\\n1⨉|V|\\nInput layer\\none-hot \\nvectors\\n“for” = V35\\n0 0\\n1\\n0\\n0\\n1\\n|V|\\n35\\n0 0\\n1\\n0\\n0\\n1\\n|V|\\n451\\n0 0\\n1\\n0\\n0\\n1\\n|V|\\n992\\n0\\n0\\n“all” = V992\\n“the” = V451\\nE\\nN⨉|V|\\nE is shared\\nacross words\\n|V|⨉d\\n…\\np(wt=do|…)\\np(wt=aardvark|wt-3,wt-2,wt-1)\\np(wt=zebra|…)\\np(wt=fish|…)\\n…\\ny42\\ny35102\\n^\\n^\\n^\\n^\\n^\\nE\\nE\\nfor\\nall\\nthe\\n?\\nthanks\\nand\\n…\\n…\\nFigure 6.14\\nForward inference in a feedforward neural language model. At each timestep\\nt the network computes a d-dimensional embedding for each of the N = 3 context tokens (by\\nmultiplying a one-hot vector by the embedding matrix E), and concatenates the three to get\\nthe embedding e. This embedding e is multiplied by weight matrix W and then an activation\\nfunction is applied element-wise to produce the hidden layer h, which is then multiplied by\\nanother weight matrix U. A softmax layer predicts at each output node i the probability that\\nthe next word wt will be vocabulary word Vi. We show the context window size N as 3 just to\\nﬁt on the page, but in practice language modeling requires a much longer context.\\nThis prediction task requires an output vector that expresses |V| probabilities:\\none probability value for each possible next token. We might have a vocabulary\\nbetween 60,000 and 300,000 tokens, so the output vector for the task of language\\nmodeling is much longer than 3. Another difference for language modeling is that\\ninstead of pooling the embeddings of the N input tokens to create a single embed-\\nding, we concatenate the inputs into one very long input vector. To predict the next\\ntoken, it helps to know each of the preceding tokens and what order they were in.\\nFig. 6.14 shows the language modeling task, sketched with a very short context\\nwindow of N = 3 just to ﬁt on the page. These 3 embedding vectors are concatenated\\nto produce e, the embedding layer. This is multiplied by a weight matrix W to pro-\\nduce a hidden layer, and another weight matrix U to produce an output layer whose\\nsoftmax gives a probability distribution over words. For example y42, the value of\\noutput node 42, is the probability of the next word wt being V42, the vocabulary word\\nwith index 42 (which is the word ‘ﬁsh’ in our example).\\nThe equations for a simple feedforward neural language model with a window'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 17}, page_content='18\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\nsize of 3, given one-hot input vectors for each input context word, are:\\ne = [Ext−3;Ext−2;Ext−1]\\nh = σ(We+b)\\nz = Uh\\nˆy = softmax(z)\\n(6.24)\\nNote that we we use semicolons to mean concatenation of vectors, so we form the\\nembedding layer e by concatenating the 3 embeddings for the three context vectors.\\nWe’ll return to this idea of using neural networks to do language modeling in\\nChapter 7 and Chapter 8 when we introduce transformer language models.\\n6.6\\nTraining Neural Nets\\nA feedforward neural net is an instance of supervised machine learning in which we\\nknow the correct output y for each observation x. What the system produces, via\\nEq. 6.13, is ˆy, the system’s estimate of the true y. The goal of the training procedure\\nis to learn parameters W[i] and b[i] for each layer i that make ˆy for each training\\nobservation as close as possible to the true y.\\nIn general, we do all this by drawing on the methods we introduced in Chapter 4\\nfor logistic regression, so the reader should be comfortable with that chapter before\\nproceeding. We’ll explore the algorithm on simple generic networks rather than\\nnetworks designed for sentiment or language modeling.\\nFirst, we’ll need a loss function that models the distance between the system\\noutput and the gold output, and it’s common to use the loss function used for logistic\\nregression, the cross-entropy loss.\\nSecond, to ﬁnd the parameters that minimize this loss function, we’ll use the\\ngradient descent optimization algorithm introduced in Chapter 4.\\nThird, gradient descent requires knowing the gradient of the loss function, the\\nvector that contains the partial derivative of the loss function with respect to each\\nof the parameters. In logistic regression, for each observation we could directly\\ncompute the derivative of the loss function with respect to an individual w or b. But\\nfor neural networks, with millions of parameters in many layers, it’s much harder to\\nsee how to compute the partial derivative of some weight in layer 1 when the loss\\nis attached to some much later layer. How do we partial out the loss over all those\\nintermediate layers? The answer is the algorithm called error backpropagation or\\nbackward differentiation.\\n6.6.1\\nLoss function\\nThe cross-entropy loss that is used in neural networks is the same one we saw for\\ncross-entropy\\nloss\\nlogistic regression. If the neural network is being used as a binary classiﬁer, with\\nthe sigmoid at the ﬁnal layer, the loss function is the same logistic regression loss\\nwe saw in Eq. ??:\\nLCE(ˆy,y) = −log p(y|x) = −[ylog ˆy+(1−y)log(1−ˆy)]\\n(6.25)\\nIf we are using the network to classify into 3 or more classes, the loss function is\\nexactly the same as the loss for multinomial regression that we saw in Chapter 4 on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 18}, page_content='6.6\\n•\\nTRAINING NEURAL NETS\\n19\\npage ??. Let’s brieﬂy summarize the explanation here for convenience. First, when\\nwe have more than 2 classes we’ll need to represent both y and ˆy as vectors. Let’s\\nassume we’re doing hard classiﬁcation, where only one class is the correct one.\\nThe true label y is then a vector with K elements, each corresponding to a class,\\nwith yc = 1 if the correct class is c, with all other elements of y being 0. Recall that\\na vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector.\\nAnd our classiﬁer will produce an estimate vector with K elements ˆy, each element\\nˆyk of which represents the estimated probability p(yk = 1|x).\\nThe loss function for a single example x is the negative sum of the logs of the K\\noutput classes, each weighted by their probability yk:\\nLCE(ˆy,y) = −\\nK\\nX\\nk=1\\nyk log ˆyk\\n(6.26)\\nWe can simplify this equation further; let’s ﬁrst rewrite the equation using the func-\\ntion 1{} which evaluates to 1 if the condition in the brackets is true and to 0 oth-\\nerwise. This makes it more obvious that the terms in the sum in Eq. 6.26 will be 0\\nexcept for the term corresponding to the true class for which yk = 1:\\nLCE(ˆy,y) = −\\nK\\nX\\nk=1\\n1{yk = 1}log ˆyk\\nIn other words, the cross-entropy loss is simply the negative log of the output proba-\\nbility corresponding to the correct class, and we therefore also call this the negative\\nlog likelihood loss:\\nnegative log\\nlikelihood loss\\nLCE(ˆy,y) = −log ˆyc\\n(where c is the correct class)\\n(6.27)\\nPlugging in the softmax formula from Eq. 6.9, and with K the number of classes:\\nLCE(ˆy,y) = −log\\nexp(zc)\\nPK\\nj=1 exp(z j)\\n(where c is the correct class)\\n(6.28)\\nLet’s think about the negative log probability as a loss function. A perfect clas-\\nsiﬁer would assign the correct class i probability 1 and all the incorrect classes prob-\\nability 0. That means the higher p(ˆyi) (the closer it is to 1), the better the classiﬁer;\\np(ˆyi) is (the closer it is to 0), the worse the classiﬁer. The negative log of this prob-\\nability is a beautiful loss metric since it goes from 0 (negative log of 1, no loss)\\nto inﬁnity (negative log of 0, inﬁnite loss). This loss function also insures that as\\nprobability of the correct answer is maximized, the probability of all the incorrect\\nanswers is minimized; since they all sum to one, any increase in the probability of\\nthe correct answer is coming at the expense of the incorrect answers.\\nThe number K of classes of the output vector ˆy can be small or large. Perhaps\\nour task is 3-way sentiment, and then the classes might be positive, negative, and\\nneutral. Or if our task is deciding the part of speech of a word (i.e., whether it is a\\nnoun or verb or adjective, etc.), then K is set of possible parts of speech in our tagset\\n(of which there are 17 in the tagset we will deﬁne in Chapter 17). And if our task\\nis language modeling, and our classiﬁer is trying to predict which word is next, then\\nour set of classes is the set of words, which might be 50,000 or 100,000.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 19}, page_content='20\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\n6.6.2\\nComputing the Gradient\\nHow do we compute the gradient of this loss function? Computing the gradient\\nrequires the partial derivative of the loss function with respect to each parameter.\\nFor a network with one weight layer and sigmoid output (which is what logistic\\nregression is), we could simply use the derivative of the loss that we used for logistic\\nregression in Eq. 6.29 (and derived in Section ??):\\n∂LCE(ˆy,y)\\n∂wj\\n= (ˆy−y)x j\\n= (σ(w ·x+b)−y)xj\\n(6.29)\\nOr for a network with one weight layer and softmax output (=multinomial logistic\\nregression), we could use the derivative of the softmax loss from Eq. ??, shown for\\na particular weight wk and input xi\\n∂LCE(ˆy,y)\\n∂wk,i\\n= −(yk −ˆyk)xi\\n= −(yk −p(yk = 1|x))xi\\n= −\\n \\nyk −\\nexp(wk ·x+bk)\\nPK\\nj=1 exp(w j ·x+b j)\\n!\\nxi\\n(6.30)\\nBut these derivatives only give correct updates for one weight layer: the last one!\\nFor deep networks, computing the gradients for each weight is much more complex,\\nsince we are computing the derivative with respect to weight parameters that appear\\nall the way back in the very early layers of the network, even though the loss is\\ncomputed only at the very end of the network.\\nThe solution to computing this gradient is an algorithm called error backprop-\\nagation or backprop (Rumelhart et al., 1986). While backprop was invented spe-\\nerror back-\\npropagation\\ncially for neural networks, it turns out to be the same as a more general procedure\\ncalled backward differentiation, which depends on the notion of computation\\ngraphs. Let’s see how that works in the next subsection.\\n6.6.3\\nComputation Graphs\\nA computation graph is a representation of the process of computing a mathematical\\nexpression, in which the computation is broken down into separate operations, each\\nof which is modeled as a node in a graph.\\nConsider computing the function L(a,b,c) = c(a+2b). If we make each of the\\ncomponent addition and multiplication operations explicit, and add names (d and e)\\nfor the intermediate outputs, the resulting series of computations is:\\nd = 2∗b\\ne = a+d\\nL = c∗e\\nWe can now represent this as a graph, with nodes for each operation, and di-\\nrected edges showing the outputs from each operation as the inputs to the next, as\\nin Fig. 6.15. The simplest use of computation graphs is to compute the value of\\nthe function with some given inputs. In the ﬁgure, we’ve assumed the inputs a = 3,\\nb = 1, c = −2, and we’ve shown the result of the forward pass to compute the re-\\nsult L(3,1,−2) = −10. In the forward pass of a computation graph, we apply each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 20}, page_content='6.6\\n•\\nTRAINING NEURAL NETS\\n21\\noperation left to right, passing the outputs of each computation as the input to the\\nnext node.\\ne=a+d\\nd = 2b\\nL=ce\\na=3\\nb=1\\nc=-2\\ne=5\\nd=2\\nL=-10\\nforward pass\\na\\nb\\nc\\nFigure 6.15\\nComputation graph for the function L(a,b,c) = c(a+2b), with values for input\\nnodes a = 3, b = 1, c = −2, showing the forward pass computation of L.\\n6.6.4\\nBackward differentiation on computation graphs\\nThe importance of the computation graph comes from the backward pass, which\\nis used to compute the derivatives that we’ll need for the weight update. In this\\nexample our goal is to compute the derivative of the output function L with respect\\nto each of the input variables, i.e., ∂L\\n∂a, ∂L\\n∂b, and ∂L\\n∂c . The derivative ∂L\\n∂a tells us how\\nmuch a small change in a affects L.\\nBackwards differentiation makes use of the chain rule in calculus, so let’s re-\\nchain rule\\nmind ourselves of that. Suppose we are computing the derivative of a composite\\nfunction f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect\\nto v(x) times the derivative of v(x) with respect to x:\\nd f\\ndx = du\\ndv · dv\\ndx\\n(6.31)\\nThe chain rule extends to more than two functions. If computing the derivative of a\\ncomposite function f(x) = u(v(w(x))), the derivative of f(x) is:\\nd f\\ndx = du\\ndv · dv\\ndw · dw\\ndx\\n(6.32)\\nThe intuition of backward differentiation is to pass gradients back from the ﬁnal\\nnode to all the nodes in the graph. Fig. 6.16 shows part of the backward computation\\nat one node e. Each node takes an upstream gradient that is passed in from its parent\\nnode to the right, and for each of its inputs computes a local gradient (the gradient\\nof its output with respect to its input), and uses the chain rule to multiply these two\\nto compute a downstream gradient to be passed on to the next earlier node.\\nLet’s now compute the 3 derivatives we need. Since in the computation graph\\nL = ce, we can directly compute the derivative ∂L\\n∂c :\\n∂L\\n∂c = e\\n(6.33)\\nFor the other two, we’ll need to use the chain rule:\\n∂L\\n∂a = ∂L\\n∂e\\n∂e\\n∂a\\n∂L\\n∂b = ∂L\\n∂e\\n∂e\\n∂d\\n∂d\\n∂b\\n(6.34)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 21}, page_content='22\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\ne\\nd\\nL\\ne\\nd\\n∂L\\n∂d\\n∂L\\n∂e\\n=\\n∂e\\n∂d\\n∂L\\n∂e\\n∂e\\n∂d\\nupstream\\n gradient\\ndownstream\\n gradient\\nlocal\\n gradient\\nFigure 6.16\\nEach node (like e here) takes an upstream gradient, multiplies it by the local\\ngradient (the gradient of its output with respect to its input), and uses the chain rule to compute\\na downstream gradient to be passed on to a prior node. A node may have multiple local\\ngradients if it has multiple inputs.\\nEq. 6.34 and Eq. 6.33 thus require ﬁve intermediate derivatives: ∂L\\n∂e , ∂L\\n∂c , ∂e\\n∂a, ∂e\\n∂d , and\\n∂d\\n∂b, which are as follows (making use of the fact that the derivative of a sum is the\\nsum of the derivatives):\\nL = ce :\\n∂L\\n∂e = c, ∂L\\n∂c = e\\ne = a+d :\\n∂e\\n∂a = 1, ∂e\\n∂d = 1\\nd = 2b :\\n∂d\\n∂b = 2\\nIn the backward pass, we compute each of these partials along each edge of the\\ngraph from right to left, using the chain rule just as we did above. Thus we begin by\\ncomputing the downstream gradients from node L, which are ∂L\\n∂e and ∂L\\n∂c . For node e,\\nwe then multiply this upstream gradient ∂L\\n∂e by the local gradient (the gradient of the\\noutput with respect to the input), ∂e\\n∂d to get the output we send back to node d: ∂L\\n∂d .\\nAnd so on, until we have annotated the graph all the way to all the input variables.\\nThe forward pass conveniently already will have computed the values of the forward\\nintermediate variables we need (like d and e) to compute these derivatives. Fig. 6.17\\nshows the backward pass.\\ne=d+a\\nd = 2b\\nL=ce\\na=3\\nb=1\\ne=5\\nd=2\\nL=-10\\n \\na\\nb\\nc\\n∂L=5\\n∂c\\n∂L =-2\\n∂e\\n∂e =1\\n∂d\\n∂d =2\\n∂b\\n∂e =1\\n∂a\\nbackward pass\\nc=-2\\n∂L =-2\\n∂e\\n∂L =5\\n∂c\\n∂L\\n∂d\\n=-2\\n∂e\\n∂d\\n∂L\\n∂e\\n=\\n∂L\\n∂a\\n=-2\\n∂e\\n∂a\\n∂L\\n∂e\\n=\\n∂L\\n∂b\\n=-4\\n∂d\\n∂b\\n∂L\\n∂d\\n=\\nFigure 6.17\\nComputation graph for the function L(a,b,c) = c(a+2b), showing the backward pass computa-\\ntion of ∂L\\n∂a , ∂L\\n∂b , and ∂L\\n∂c .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 22}, page_content='6.6\\n•\\nTRAINING NEURAL NETS\\n23\\nBackward differentiation for a neural network\\nOf course computation graphs for real neural networks are much more complex.\\nFig. 6.18 shows a sample computation graph for a 2-layer neural network with n0 =\\n2, n1 = 2, and n2 = 1, assuming binary classiﬁcation and hence using a sigmoid\\noutput unit for simplicity. The function that the computation graph is computing is:\\nz[1] = W[1]x+b[1]\\na[1] = ReLU(z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = σ(z[2])\\nˆy = a[2]\\n(6.35)\\nFor the backward pass we’ll also need to compute the loss L. The loss function\\nfor binary sigmoid output from Eq. 6.25 is\\nLCE(ˆy,y) = −[ylog ˆy+(1−y)log(1−ˆy)]\\n(6.36)\\nOur output ˆy = a[2], so we can rephrase this as\\nLCE(a[2],y) = −\\nh\\nyloga[2] +(1−y)log(1−a[2])\\ni\\n(6.37)\\nz[2] = \\n+\\na[2] = σ\\n \\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1]\\n*\\n*\\n*\\n*\\nx1\\nx2\\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1]\\n*\\n*\\nw[2]\\n11\\nw[1]\\n11\\nw[1]\\n12\\nw[1]\\n21\\nw[1]\\n22\\nb[2]\\nw[2]\\n12\\nL (a[2],y)\\n1\\n2\\n1\\n1\\n1\\n2\\n2\\nFigure 6.18\\nSample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units\\nand 2 hidden units. We’ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning\\nthe function that is being computed, and the resulting variable name. Thus the * to the right of node w[1]\\n11 means\\nthat w[1]\\n11 is to be multiplied by x1, and the node z[1] = + means that the value of z[1] is computed by summing\\nthe three nodes that feed into it (the two products, and the bias term b[1]\\ni ).\\nThe weights that need updating (those for which we need to know the partial\\nderivative of the loss function) are shown in teal. In order to do the backward pass,\\nwe’ll need to know the derivatives of all the functions in the graph. We already saw\\nin Section ?? the derivative of the sigmoid σ:\\ndσ(z)\\ndz\\n= σ(z)(1−σ(z))\\n(6.38)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 23}, page_content='24\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\nWe’ll also need the derivatives of each of the other activation functions. The\\nderivative of tanh is:\\nd tanh(z)\\ndz\\n= 1−tanh2(z)\\n(6.39)\\nThe derivative of the ReLU is2\\nd ReLU(z)\\ndz\\n=\\n\\x1a 0 for z < 0\\n1 for z ≥0\\n(6.40)\\nWe’ll give the start of the computation, computing the derivative of the loss function\\nL with respect to z, or ∂L\\n∂z (and leaving the rest of the computation as an exercise for\\nthe reader). By the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z\\n(6.41)\\nSo let’s ﬁrst compute\\n∂L\\n∂a[2] , taking the derivative of Eq. 6.37, repeated here:\\nLCE(a[2],y) = −\\nh\\nyloga[2] +(1−y)log(1−a[2])\\ni\\n∂L\\n∂a[2] = −\\n  \\ny∂log(a[2])\\n∂a[2]\\n!\\n+(1−y)∂log(1−a[2])\\n∂a[2]\\n!\\n= −\\n\\x12\\x12\\ny 1\\na[2]\\n\\x13\\n+(1−y)\\n1\\n1−a[2] (−1)\\n\\x13\\n= −\\n\\x12 y\\na[2] + y−1\\n1−a[2]\\n\\x13\\n(6.42)\\nNext, by the derivative of the sigmoid:\\n∂a[2]\\n∂z\\n= a[2](1−a[2])\\nFinally, we can use the chain rule:\\n∂L\\n∂z\\n=\\n∂L\\n∂a[2]\\n∂a[2]\\n∂z\\n= −\\n\\x12 y\\na[2] + y−1\\n1−a[2]\\n\\x13\\na[2](1−a[2])\\n= a[2] −y\\n(6.43)\\nContinuing the backward computation of the gradients (next by passing the gra-\\ndients over b[2]\\n1 and the two product nodes, and so on, back to all the teal nodes), is\\nleft as an exercise for the reader.\\n6.6.5\\nMore details on learning\\nOptimization in neural networks is a non-convex optimization problem, more com-\\nplex than for logistic regression, and for that and other reasons there are many best\\npractices for successful learning.\\n2\\nThe derivative is actually undeﬁned at the point z = 0, but by convention we treat it as 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 24}, page_content='6.7\\n•\\nSUMMARY\\n25\\nFor logistic regression we can initialize gradient descent with all the weights and\\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\\nweights with small random numbers. It’s also helpful to normalize the input values\\nto have 0 mean and unit variance.\\nVarious forms of regularization are used to prevent overﬁtting. One of the most\\nimportant is dropout: randomly dropping some units and their connections from\\ndropout\\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\\niteration of training (whenever we update parameters, i.e. each mini-batch if we are\\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\\neach unit we replace its output with zero with probability p (and renormalize the\\nrest of the outputs from that layer).\\nTuning of hyperparameters is also important. The parameters of a neural net-\\nhyperparameter\\nwork are the weights W and biases b; those are learned by gradient descent. The\\nhyperparameters are things that are chosen by the algorithm designer; optimal val-\\nues are tuned on a devset rather than by gradient descent learning on the training\\nset. Hyperparameters include the learning rate η, the mini-batch size, the model\\narchitecture (the number of layers, the number of hidden nodes per layer, the choice\\nof activation functions), how to regularize, and so on. Gradient descent itself also\\nhas many architectural variants such as Adam (Kingma and Ba, 2015).\\nFinally, most modern neural networks are built using computation graph for-\\nmalisms that make it easy and natural to do gradient computation and parallelization\\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)\\nand TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\\nreader should consult a neural network textbook for further details; some sugges-\\ntions are at the end of the chapter.\\n6.7\\nSummary\\n• Neural networks are built out of neural units, originally inspired by biological\\nneurons but now simply an abstract computational device.\\n• Each neural unit multiplies input values by a weight vector, adds a bias, and\\nthen applies a non-linear activation function like sigmoid, tanh, or rectiﬁed\\nlinear unit.\\n• In a fully-connected, feedforward network, each unit in layer i is connected\\nto each unit in layer i+1, and there are no cycles.\\n• The power of neural networks comes from the ability of early layers to learn\\nrepresentations that can be utilized by later layers in the network.\\n• Neural networks are trained by optimization algorithms like gradient de-\\nscent.\\n• Error backpropagation, backward differentiation on a computation graph,\\nis used to compute the gradients of the loss function for a network.\\n• Neural language models use a neural network as a probabilistic classiﬁer, to\\ncompute the probability of the next word given the previous n words.\\n• Neural language models can use pretrained embeddings, or can learn embed-\\ndings from scratch in the process of language modeling.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 25}, page_content='26\\nCHAPTER 6\\n•\\nNEURAL NETWORKS\\nHistorical Notes\\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-\\nloch and Pitts, 1943), a simpliﬁed model of the biological neuron as a kind of com-\\nputing element that could be described in terms of propositional logic. By the late\\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\\nBernard Widrow at Stanford) developed research into neural networks; this phase\\nsaw the development of the perceptron (Rosenblatt, 1958), and the transformation\\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\\nThe ﬁeld of neural networks declined after it was shown that a single perceptron\\nunit was unable to model functions as simple as XOR (Minsky and Papert, 1969).\\nWhile some small amount of work continued during the next two decades, a major\\nrevival for the ﬁeld didn’t come until the 1980s, when practical tools for building\\ndeeper networks like error backpropagation became widespread (Rumelhart et al.,\\n1986). During the 1980s a wide variety of neural network and related architec-\\ntures were developed, particularly for applications in psychology and cognitive sci-\\nence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart\\nand McClelland 1986a, Elman 1990), for which the term connectionist or paral-\\nconnectionist\\nlel distributed processing was often used (Feldman and Ballard 1982, Smolensky\\n1988). Many of the principles and techniques developed in this period are foun-\\ndational to modern work, including the ideas of distributed representations (Hinton,\\n1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\\n(Smolensky, 1990).\\nBy the 1990s larger neural networks began to be applied to many practical lan-\\nguage processing tasks as well, like handwriting recognition (LeCun et al. 1989) and\\nspeech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements\\nin computer hardware and advances in optimization and training techniques made it\\npossible to train even larger and deeper networks, leading to the modern term deep\\nlearning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in\\nChapter 13 and Chapter 15.\\nThere are a number of excellent books on neural networks, including Goodfellow\\net al. (2016) and Nielsen (2015).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'source': '../data/text_files/pdf/neural networks.pdf', 'file_path': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '', 'modDate': \"D:20250824115053-07'00'\", 'creationDate': \"D:20250824115053-07'00'\", 'page': 26}, page_content='Historical Notes\\n27\\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-\\nard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-\\nberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah,\\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-\\nwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi´egas,\\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu,\\nand X. Zheng. 2015. TensorFlow: Large-scale machine\\nlearning on heterogeneous systems. Software available\\nfrom tensorﬂow.org.\\nBengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\\nA neural probabilistic language model. JMLR, 3:1137–\\n1155.\\nBengio, Y., P. Lamblin, D. Popovici, and H. Larochelle.\\n2007.\\nGreedy layer-wise training of deep networks.\\nNeurIPS.\\nElman, J. L. 1990. Finding structure in time. Cognitive sci-\\nence, 14(2):179–211.\\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist mod-\\nels and their properties. Cognitive Science, 6:205–254.\\nGoodfellow, I., Y. Bengio, and A. Courville. 2016. Deep\\nLearning. MIT Press.\\nHinton, G. E. 1986. Learning distributed representations of\\nconcepts. COGSCI.\\nHinton, G. E., S. Osindero, and Y.-W. Teh. 2006. A fast\\nlearning algorithm for deep belief nets. Neural computa-\\ntion, 18(7):1527–1554.\\nHinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. 2012. Improving neural networks\\nby preventing co-adaptation of feature detectors. ArXiv\\npreprint arXiv:1207.0580.\\nKingma, D. and J. Ba. 2015. Adam: A method for stochastic\\noptimization. ICLR 2015.\\nLeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backprop-\\nagation applied to handwritten zip code recognition. Neu-\\nral computation, 1(4):541–551.\\nMcClelland, J. L. and J. L. Elman. 1986. The TRACE model\\nof speech perception. Cognitive Psychology, 18:1–86.\\nMcCulloch, W. S. and W. Pitts. 1943. A logical calculus of\\nideas immanent in nervous activity. Bulletin of Mathe-\\nmatical Biophysics, 5:115–133.\\nMinsky, M. and S. Papert. 1969. Perceptrons. MIT Press.\\nMorgan, N. and H. Bourlard. 1990.\\nContinuous speech\\nrecognition using multilayer perceptrons with hidden\\nmarkov models. ICASSP.\\nNielsen, M. A. 2015. Neural networks and Deep learning.\\nDetermination Press USA.\\nPaszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\\n2017. Automatic differentiation in pytorch. NIPS-W.\\nRosenblatt, F. 1958. The perceptron: A probabilistic model\\nfor information storage and organization in the brain. Psy-\\nchological review, 65(6):386–408.\\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\\nLearning internal representations by error propagation. In\\nD. E. Rumelhart and J. L. McClelland, eds, Parallel Dis-\\ntributed Processing, volume 2, 318–362. MIT Press.\\nRumelhart, D. E. and J. L. McClelland. 1986a. On learning\\nthe past tense of English verbs. In D. E. Rumelhart and\\nJ. L. McClelland, eds, Parallel Distributed Processing,\\nvolume 2, 216–271. MIT Press.\\nRumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel\\nDistributed Processing. MIT Press.\\nRussell, S. and P. Norvig. 2002. Artiﬁcial Intelligence: A\\nModern Approach, 2nd edition. Prentice Hall.\\nSmolensky, P. 1988. On the proper treatment of connection-\\nism. Behavioral and brain sciences, 11(1):1–23.\\nSmolensky, P. 1990. Tensor product variable binding and\\nthe representation of symbolic structures in connectionist\\nsystems. Artiﬁcial intelligence, 46(1-2):159–216.\\nSrivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever,\\nand R. R. Salakhutdinov. 2014.\\nDropout:\\na simple\\nway to prevent neural networks from overﬁtting. JMLR,\\n15(1):1929–1958.\\nWidrow, B. and M. E. Hoff. 1960. Adaptive switching cir-\\ncuits. IRE WESCON Convention Record, volume 4.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 0}, page_content='LLM Hallucinations in Practical Code Generation:\\nPhenomena, Mechanism, and Mitigation\\nZIYAO ZHANG, Sun Yat-sen University, China\\nCHONG WANG, Nanyang Technological University, Singapore\\nYANLIN WANG∗, Sun Yat-sen University, China\\nENSHENG SHI, Huawei Cloud Computing Technologies Co., Ltd, China\\nYUCHI MA, Huawei Cloud Computing Technologies Co., Ltd, China\\nWANJUN ZHONG, Sun Yat-sen University, China\\nJIACHI CHEN, Sun Yat-sen University, China\\nMINGZHI MAO, Sun Yat-sen University, China\\nZIBIN ZHENG, Sun Yat-sen University, China\\nCode generation aims to automatically generate code from input requirements, significantly enhancing\\ndevelopment efficiency. Recent large language models (LLMs) based approaches have shown promising results\\nand revolutionized code generation task. Despite the promising performance, LLMs often generate contents\\nwith hallucinations, especially for the code generation scenario requiring the handling of complex contextual\\ndependencies in practical development process. Although previous study has analyzed hallucinations in\\nLLM-powered code generation, the study is limited to standalone function generation. In this paper, we\\nconduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within\\nmore practical and complex development contexts in repository-level generation scenario. First, we manually\\nexamine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-\\ngenerated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across\\ndifferent models. We then analyze causes of hallucinations and identify four potential factors contributing\\nto hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent\\neffectiveness in all studied LLMs.\\nCCS Concepts: • Software and its engineering →Automatic programming.\\nAdditional Key Words and Phrases: Repository-Level Code Generation, Hallucination, Large Language Models\\nACM Reference Format:\\nZiyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi\\nMao, and Zibin Zheng. 2025. LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and\\n∗Yanlin Wang is the corresponding author.\\nAuthors’ Contact Information: Ziyao Zhang, zhangzy373@mail2.sysu.edu.cn, Zhuhai Key Laboratory of Trusted Large\\nLanguage Models, Sun Yat-sen University, Zhuhai, China; Chong Wang, chong.wang@ntu.edu.sg, Nanyang Technological\\nUniversity, Nanyang Avenue, Singapore; Yanlin Wang, wangylin36@mail.sysu.edu.cn, Zhuhai Key Laboratory of Trusted\\nLarge Language Models, Sun Yat-sen University, Zhuhai, China; Ensheng Shi, shiensheng@huawei.com, Huawei Cloud\\nComputing Technologies Co., Ltd, Beijing, China; Yuchi Ma, mayuchi1@huawei.com, Huawei Cloud Computing Technologies\\nCo., Ltd, Shenzhen, China; Wanjun Zhong, zhongwj25@mail2.sysu.edu.cn, Sun Yat-sen University, Guangzhou, China; Jiachi\\nChen, chenjch86@mail.sysu.edu.cn, Zhuhai Key Laboratory of Trusted Large Language Models, Sun Yat-sen University,\\nZhuhai, China; Mingzhi Mao, mcsmmz@mail.sysu.edu.cn, Zhuhai Key Laboratory of Trusted Large Language Models,\\nSun Yat-sen University, Zhuhai, China; Zibin Zheng, zhzibin@mail.sysu.edu.cn, Zhuhai Key Laboratory of Trusted Large\\nLanguage Models, Sun Yat-sen University, Zhuhai, China.\\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\\n© 2025 Copyright held by the owner/author(s).\\nACM 2994-970X/2025/7-ARTISSTA022\\nhttps://doi.org/10.1145/3728894\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 1}, page_content='ISSTA022:2\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\nMitigation. Proc. ACM Softw. Eng. 2, ISSTA, Article ISSTA022 (July 2025), 23 pages. https://doi.org/10.1145/\\n3728894\\n1\\nIntroduction\\nCode generation aims at efficiently producing code from specifications described in natural language.\\nThis process significantly reduces the manual coding workload for developers [7, 11, 51], allowing\\nthem to focus more on solving advanced technical challenges and engaging in innovative tasks.\\nRecent developments have introduced a variety of large language models (LLMs) [5, 8–10, 14, 16,\\n25, 26, 32, 34, 39, 43, 44, 48] built upon the Transformer architecture [38]. These models, trained on\\nextensive code corpora, can automatically generate code from natural language inputs and have\\nshown high efficacy in code generation. For example, GPT-4 has achieved state-of-the-art results\\non evaluation benchmarks such as HumanEval [10] and MBPP [6], demonstrating high functional\\ncorrectness, particularly in generating standalone functions based on detailed specifications.\\nHowever, in practical development scenarios, the requirements for code generation are more com-\\nplex than simply generating standalone functions [46]. To address this complexity, new benchmarks\\nsuch as CoderEval [46] and EvoCodeBench [24] have been proposed to better reflect real-world\\nrepository-level development scenarios. Evaluations based on these benchmarks have revealed that\\nLLMs face challenges in generating non-standalone functions with contextual dependencies, such as\\ncalls to user-defined functions and project-defined data protocol. While these benchmarks provide\\nvaluable insights into the effectiveness of LLMs in practical code generation, they mainly emphasize\\nfunctional correctness of the LLM-generated code—measured by test case pass rates—without a\\ncomprehensive examination of the underlying mechanism of failure.\\nTo bridge this gap, this work aims to systematically investigate issues in practical LLM-based code\\ngeneration from the perspective of hallucinations. Hallucination is one of the most significant issues\\nfor state-of-the-art generative LLMs [19]. For general natural language tasks, LLM hallucinations\\nhave been explored to a certain extent [19, 37, 45, 50] and are typically categorized into three\\ntypes: Input-Conflicting Hallucination, Fact-Conflicting Hallucination, and Context-Conflicting\\nHallucination [50]. In the domain of code generation, Liu et al. [27] conducted a study to analyze\\nhallucinations in LLM-generated code and established a taxonomy that includes Intent Conflicting,\\nContext Deviation, and Knowledge Conflicting. While Liu et al.’s study provided insightful findings,\\nit still faces two potential limitations. First, as this study relies on benchmarks (i.e., HumanEval [10]\\nand DS-1000 [23]) focusing on standalone function or script generation, the resulting taxonomy may\\nnot fully align with practical repository-level code generation scenarios involving non-standalone\\ndependencies. For instance, project contexts such as user-defined dependencies and non-code\\nresources (e.g., configuration files) are not taken into account. Second, their study primarily catego-\\nrized fine-grained hallucinations in LLM-generated code and attempted to detect them, without\\nthoroughly analyzing potential causes or exploring mitigation feasibility. In contrast, we investigate\\nhallucinations in repository-level code generation within more practical development contexts,\\nadopting a holistic perspective that encompasses phenomena, mechanism, and mitigation.\\nIn this work, we conduct an empirical study to uncover the status quo and root causes of halluci-\\nnations in LLM-based code generation within real-world projects. The study aims at answering the\\nfollowing research questions (RQs):\\n• RQ1 (Hallucination Taxonomy): What are the specific manifestations of hallucinations in\\npractical code generation, and how are they distributed?\\n• RQ2 (LLM Comparison): How do different LLMs compare in terms of hallucination occur-\\nrences and patterns?\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 2}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:3\\n• RQ3 (Potential Cause Discussion): What are the potential causes of hallucinations in practical\\nLLM-based code generation?\\nTo answer the questions, we experiment on six mainstream LLMs (ChatGPT [33], CodeGen [32],\\nPanGu-𝛼[48], StarCoder2 [28], DeepSeekCoder [16], and CodeLlama [34]) with the CoderEval\\ndataset [46]. To obtain the hallucination taxonomy of practical LLM-based code generation, we\\nmanually perform open coding [21] on the LLM-generated code. Specifically, we first extract 10% of\\nthe coding tasks from the CoderEval dataset in the initial stage. Then, from the initial annotation and\\ndiscussion, we obtain preliminary taxonomy. Finally, we obtain the fully hallucination taxonomy\\nwith iterative labelling the remaining 90% coding tasks and continuously refining the taxonomy in\\nthe process. After obtaining the taxonomy, we conduct extensive analysis based on the research\\nquestions aforementioned.\\nFindings. Our study reveals the following findings. 1○LLM hallucinations in code generation can\\nbe divided into three major categories (Task Requirement Conflicts, Factual Knowledge Conflicts,\\nand Project Context Conflicts) with eight subcategories: Functional Requirement Violation, Non-\\nFunctional Requirement Violation, Background Knowledge Conflicts, Library knowledge Conflicts,\\nAPI Knowledge Conflicts, Environment Conflicts, Dependency Conflicts, and Non-code Resource\\nConflicts. 2○We analyze the hallucination distribution in different LLMs and find that Task Require-\\nment Conflicts are the most prevalent type of hallucination across all models. 3○We identify four\\npotential factors that cause hallucinations: training data quality, intention understanding capacity,\\nknowledge acquisition capacity, and repository-level context awareness.\\nMitigation. Based on the findings, we explore a lightweight mitigation approach based on\\nretrieval augmented generation (RAG) and evaluate its effectiveness.\\nIn this approach, we take two main steps. First, we construct a retrieval library. This library is\\nbuilt based on the code repository relevant to the development scenario of each code generation task.\\nSecond, we identify useful code snippets by detecting the similarity between the task description of\\nthe current generation task and the code snippets stored in the retrieval library. The code snippet\\nthat has the highest relevance to the current task, as determined by this similarity check, is then used\\nas a prompt to guide the code generation process. Experimental results show that this lightweight\\nmitigation can consistently improve the performance of all studied LLMs.\\nIn summary, this paper makes the following contributions:\\n• We conduct an empirical study to analyze the hallucinations in LLM-based code generation\\nwithin real development scenarios and establish a hallucination taxonomy.\\n• We elaborate on the phenomenon of hallucinations, analyze the distribution of hallucinations\\non different models.\\n• We further analyze causes of hallucinations and identify four possible factors.\\n• We propose a RAG-based mitigation approach based on the causes of hallucinations and\\nexperiment on various LLMs to study its effectiveness.\\n2\\nBackground & Related Work\\n2.1\\nLLM-based Code Generation\\nFor developers, a realistic scenario is to use a code repository to write code, which is very common\\nin practice [15]. For example, due to security and functionality considerations, companies often\\nonly build code warehouses internally. The code repository provides many private APIs that are\\nnot seen by the language model and are not public on any code hosting platform. Therefore, it is\\nworth exploring whether pre-trained language models can adapt to real development needs and\\ngenerate correct and efficient code. Previous studies such as MBPP [6] and HumanEval [10] have\\nevaluated LLMs for standalone functions, which do not depend on functions in other files. However,\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 3}, page_content='ISSTA022:4\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\nin real-world development scenarios, the development of a function not only relies on the text\\ndescription and function signature of the function, but also requires calling a custom API in the\\ncode repository. Such non-independent functions are commonly found in real-world generation\\nscenarios. By analyzing the 100 most popular projects written in Java and Python on GitHub [46],\\nprevious work found that dependent functions account for more than 70% of the functions in open\\nsource projects. In order to better simulate real development scenarios and to check the correctness\\nof LLMs, CoderEval [46], ClassEval [13], and EvoCodeBench [24] collected code snippets and text\\ndescriptions from real code repositories and used test cases to check the correctness of the code\\nrepositories in their corresponding environments. However, the performance of the model on these\\nbenchmarks is extremely poor. LLMs cannot generate correct code based on the problem description,\\nand the model prefers to generate independent code segments rather than using existing functions\\nin the current development scenario.\\n2.2\\nHallucinations in LLMs\\nIn the field of natural language processing (NLP), hallucination refers specifically to situations\\nwhere the content produced by a language model in the process of generating text is inconsistent\\nwith the given input or expected output environment, lacks meaning, or violates the facts [20]. This\\nkind of phenomenon is particularly prominent in text generation models, especially in tasks such\\nas text completion, summary generation, and machine translation. The output of the model must\\nmaintain a high degree of consistency and authenticity to ensure its practicality and reliability.\\nHallucination phenomena can be divided into the following categories according to their nature [50]:\\n(1) Input-Conflicting Hallucinations: When the text generated by the model deviates from the\\noriginal input source, input-conflicting hallucinations will occur. This hallucination may result\\nfrom the model’s incorrect parsing or inaccurate internal representation of the input information,\\ncausing the output content to deviate from the intent and context of the source input. (2) Context-\\nConflicting Hallucinations: This type of hallucination occurs when the text generated by the\\nmodel is contradictory or inconsistent with its previously generated content. Contextual conflict\\nhallucinations reflect the model’s challenges in maintaining textual coherence and consistency,\\nwhich may be due to the model’s insufficient processing of contextual information or limitations of\\nits memory mechanism. (3) Fact-Conflicting Hallucinations: When the content generated by LLM\\nis inconsistent with established knowledge or facts in the real world, fact-conflicting hallucinations\\nwill occur. This hallucination reveals the model’s inadequacy in understanding and applying\\nknowledge about the external world, and may be caused by limitations in model training data, lags\\nin knowledge updates, or limitations in the model’s reasoning capabilities.\\nHowever, there is a lack of research on hallucination phenomena in the field of code generation.\\nAlthough there have been a large number of LLM-based methods to optimize code generation tasks,\\nthese works do not have a clear definition of the code generation hallucination. The presence of\\nhallucination problems can be detrimental to the overall quality of the generated code. This may not\\nonly affect the performance and maintainability of the code, but may also lead to unexpected errors\\nand security vulnerabilities, thus posing a threat to the stability and security of the software. In\\norder to make up for the gaps in the definition of hallucination problems, Liu et al. [27] conducted\\na study based on a data set of standalone code generation to define hallucinations for LLMs in code\\ngeneration tasks. They divided hallucinations in LLM-based code generation into five main types,\\nbut they ignored that LLMs in real-world code generation tasks will involve complex repository-\\nlevel contexts such as development environment, system resources, external constraints, code\\nwarehouses, etc. These factors often cause LLMs to fail in actual development, leading to problems\\nsuch as low usability and low accuracy. We believe that focusing solely on generating standalone\\nfunctions is inadequate for developers in real-world development scenarios, often requiring the\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 4}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:5\\nintegration of code into a comprehensive code repository by using LLMs. In order to better explore\\nhallucinations that exist in LLMs in practical development scenarios, our work obtained data sets\\nin real development scenarios for empirical study, and defined new types of hallucinations, which\\nopened up new ideas for subsequent research on hallucinations.\\n3\\nEvaluation Setup\\n3.1\\nDataset\\nExisting work only discusses the hallucination in the function-level development scenario, but does\\nnot discuss hallucinations of LLMs in practical development scenarios. To better simulate practical\\ndevelopment scenarios, we use a set of coding tasks from real-world Python repositories based\\non CoderEval [46]. It comprises 230 Python code generation tasks. Each task consists of a natural\\nlanguage description, a ground-truth code snippet, and a set of test cases, along with the project\\ncontext.\\n3.2\\nStudied LLMs\\nWe utilize several mainstream LLMs to perform code generation for the studied programming\\ntasks. The LLMs being used cover both open-source and closed-source models and span various\\nparameter sizes, listed as follows.\\n• ChatGPT [33]: ChatGPT is a versatile text generation model for multilingualism with powerful\\ncode generation capabilities, we use the GPT-3.5-Turbo in our experiments.\\n• CodeGen [31]: CodeGen is a family of auto-regressive language models for program synthesis\\nwith several different versions. To better accomplish the generation task, we use the CodeGen-\\n350M-Mono model.\\n• PanGu-𝛼[48]: PanGu-𝛼can perform code generation tasks in multiple languages. We use the\\nPanGu-𝛼-2.6B model.\\n• DeepSeekCoder [16]: DeepSeekCoder performs well in open source models across multiple\\nprogramming languages and various benchmarks. We use the DeepSeekCoder-6.7B base model.\\n• CodeLlama [34]: CodeLlama is a set of pre-trained and fine-tuned generative text models\\nranging in size from 7 to 34 billion parameters. We use the CodeLlama-7b-Python-hf model.\\n• StarCoder2 [25]: StarCoder2 is a family of open code-oriented models for large languages,\\nproviding three scales of models, we use the StarCoder2-7B model.\\nFor each task, we use the LLMs to generate 10 code snippets by employing the nucleus sampling\\nstrategy [18] and setting temperature to 0.6, following the same setting as CoderEval.\\n3.3\\nTaxonomy Annotation\\nIn order to analyze the hallucination types in the LLM-generated code, we manually perform open\\ncoding [21] on the generated code to obtain the hallucination taxonomy.\\n(1) Initial Open Coding. Firstly, in the initial open-coding stage, we select 10% of the 230\\ncoding tasks in CoderEval Python dataset for preliminary manual analysis. We randomly collect\\n23 generative tasks from CoderEval, we employ CodeGen, Pangu-𝛼, ChatGPT, DeepSeekCoder,\\nCodeLlama, and StarCoder2, with each model generating ten code snippets for each code generation\\ntask, culminating in a total of 1,380 code snippets to be analysed for hallucination taxonomy frame-\\nwork. For each code snippet, we further run its test cases in the actual development environment\\ncorresponding to the task to determine its correctness. Next, two annotators manually review the\\nLLM-generated code snippets that fail the tests, identifying specific code issues by referring to the\\nground truth and execution results. Note that an LLM-generated snippet may fail test cases due to\\nmultiple code issues. Ultimately, this stage identifies a total of seventeen types of issues, such as\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 5}, page_content='ISSTA022:6\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\nCode Generation Hallucination\\nTask Requirement Conflicts (§4.1.1)\\nFunctional Requirement Violation\\nExample: Wrong Functionality, Missing Functionality\\nNon-functional Requirement Violation\\nExample: Security, Performance, Style, Code Smell\\nFactual Knowledge Conflicts (§4.1.2)\\nBackground Knowledge Conflicts\\nLibrary Knowledge Conflicts\\nAPI Knowledge Conflicts\\nExample: Parameters, Guard Conditions\\nSimilar-but-wrong APIs, API Call Exceptions\\nProject Context Conflicts (§4.1.3)\\nEnvironment Conflicts\\nDependency Conflicts\\nExample: Undefined Methods, API Version Conflict\\nNon-code Resource Conflicts\\nExample: Data, Configs, Assets, Connections\\nFig. 1. Taxonomy of Hallucinations in LLM-based Code Generation.\\nMissing Functionality, Code Smell, Similar-but-Wrong APIs, Data Resource Conflicts and Undefined\\nMethods, after deduplication.\\n(2) Preliminary Taxonomy Construction. Secondly, based on the specific issues identified,\\nwe conduct a manual analysis to group related code issues into broader hallucination types. For\\nexample, code issues involving API Parameters, API Guard Conditions, Similar-but-Wrong APIs, and\\nAPI Call Exceptions are grouped under the type API Knowledge Conflicts, as they all represent API\\nmisuses and conflicts with factual API knowledge. This grouping process, conducted by two authors\\nin consultation with the original annotators, results in eight hallucination types. Building on this\\ngrouping, we further categorize the types into three higher-level categories that generally align\\nwith definitions in the NLP domain. Finally, we establish a preliminary hallucination taxonomy\\ncomprising three major categories divided into eight subcategories, each including some code\\nissues identified in the previous stage.\\n(3) Full Taxonomy Construction. Finally, after obtaining the taxonomy categorization, the\\nremaining code snippets will be independently annotated by three newly invited volunteers with\\nextensive Python programming experience, two with more than ten years of experience and one\\nwith four years of programming experience. The goal of the annotation is to identify issues in the\\nLLM-generated code snippets and assess whether these issues fit within the established preliminary\\ntaxonomy. If any previously unknown code issues are identified, we will discuss whether existing\\nhallucination (sub)categories can accommodate them or if a new type should be added to the\\ntaxonomy. Ultimately, we identify two new code issues Asset Conflicts and Connection Conflicts, but\\nno new hallucination categories or subcategories.\\n3.4\\nHallucination Statistics\\nDuring the process of taxonomy annotation, we record the specific code issues identified in\\neach LLM-generated code snippet and count the frequency of each hallucination based on the\\nrelationships between hallucination (sub)categories and specific code issues. It is important to note\\nthat for the code snippets that contain multiple code issues, all hallucinations present will be included\\nin the statistics.\\n4\\nEvaluation Results\\nIn this section, we present the evaluation results and answer the three aforementioned research\\nquestions.\\n4.1\\nRQ1: Hallucination Taxonomy\\nThe overall LLM coding hallucination taxonomy we obtained from Section 3.3 is presented in\\nFigure 1. Through manual annotation, we identify three primary hallucination categories: Task\\nRequirement Conflicts, Factual Knowledge Conflicts, and Project Context Conflicts, which can be\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 6}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:7\\nFig. 2. Hallucination Distribution.\\nfurther divided into eight specific types. Note that our three primary categories align well with\\nthe hallucination types in the general domain [20]. Task requirement conflicts correspond to\\ninput-conflicting hallucinations in the general domain, indicating that the generated code does\\nnot meet the functional or non-functional requirements of the coding tasks. Factual knowledge\\nconflicts correspond to knowledge-conflicting hallucinations in the general domain, indicating that\\nthe generated code does not comply with background knowledge, library/framework knowledge,\\nor API knowledge. Project context conflicts correspond to context-conflicting hallucinations in the\\ngeneral domain, indicating that the generated code incorrectly uses project contexts, including\\nenvironments, dependencies, and resources.\\nFigure 2 shows the distribution of the hallucination types, where the denominator is the sum of\\nall annotated hallucination instances across the six LLMs. We present the detailed hallucination\\ntypes in our taxonomy as follows.\\n4.1.1\\nTask Requirement Conflicts (43.53%). In the general domain, input-conflicting hallucinations\\noccur when the answers generated by LLMs deviate from the original intentions of user inputs [19].\\nIn the context of code generation tasks, the primary intentions of inputs typically revolve around the\\nfunctional and non-functional requirements of the coding tasks. When the code generated by LLMs\\ndoes not align with these requirements, hallucinations related to Task Requirement Conflicts occur.\\nSpecifically, these conflicts can be categorized into two types: Functional Requirement Violation and\\nNon-functional Requirement Violation.\\nFunctional Requirement Violation (36.66%). Functional requirements are typically expressed in\\nnatural language and describe the desired functionality of the generated code. When these re-\\nquirements are not correctly and comprehensively understood, the resulting code may fail to\\nmeet expected functionality, leading to logic bugs (such as unexpected execution behaviors) or\\nruntime errors (such as the KeyError during dictionary access). More specifically, the functional\\nrequirement mismatch can be subdivided into two typical types: Wrong Functionality and Missing\\nFunctionality. For example, as illustrated in Figure 3, the functional requirement involves handling\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 7}, page_content='ISSTA022:8\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\ndef hydrate_time(nanoseconds, tz=None):\\nfrom pytz import FixedOffset\\nseconds, nanoseconds = map(int, divmod(nanoseconds, 1000000000))\\nminutes, seconds = map(int, divmod(seconds, 60))\\nhours, minutes = map(int, divmod(minutes, 60))\\nt = Time(hours, minutes, seconds, nanoseconds)\\nif tz is None:\\nreturn t\\ntz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\\nzone = FixedOffset(tz_offset_minutes)\\nreturn zone.localize(t)\\nGround-truth\\nDocstring \\n# similarity_filter.py\\nHydrator for `Time` and `LocalTime` values. \\n:param nanoseconds: \\n:param tz: \\n:return: Time\\ndef hydrate_time(nanoseconds, tz=None):\\nfrom .time import Time\\nreturn Time.from_nanoseconds(nanoseconds, tz)\\nLLM Generation\\nHandle `LocalTime` \\nin functional \\nrequirement\\nOverlook \\n`LocalTime`\\nFig. 3. Example: Functional Requirement Violation.\\ndef validate_from_content(cls, file_content=None):\\nif file_content is None:\\nraise IRValidatorException(\\n \"Registry YAML content is missing\")\\nregistry_dict = yaml.safe_load(file_content)\\nif not isinstance(registry_dict, dict):\\nraise IRValidatorException(\\n  \"Registry file is empty or corrupted: {}\".format(file_content))\\ntry:\\njsonschema.validate(registry_dict,\\ncls.SCHEMA_REGISTRY)\\nexcept jsonschema.exceptions.ValidationError as error:\\nraise IRValidatorException(\\n\"{} in file:\\\\n{}\".format(error.message, file_content))\\nreturn registry_dict\\nGround-truth\\nDocstring \\nvalidates that Registry YAML content has all required fields\\n:param file_content: content of the Registry YAML file\\n:raise IRValidatorException: when mandatory data is missing in Registry\\n:return: Dictionary with data loaded from a Registry YAML file\\nAvoiding safety \\nhazards\\ndef validate_from_content(cls, file_content=None):\\nif  file _content is None:\\nraise IRValidatorException(\\'file content is missing’)\\nfile_data = yaml.load(spec_content)\\nvalidate_data(cls, file_data)\\nreturn  file_data\\nLLM Generation\\nLeading to system \\nsecurity risks\\nFig. 4. Example: Non-functional Requirement Violation.\\nLocalTime based on the specific timezone tz. In the ground-truth code, this requirement is ad-\\ndressed by the lines highlighted in the green rectangle. However, the code generated by PanGu-𝛼\\noverlooks this requirement, resulting in a hallucination of Functional Requirement Violation.\\nNon-functional Requirement Violation (6.86%). Besides functional requirements, developers of-\\nten have non-functional requirements for the generated code, such as security concerns or perfor-\\nmance considerations. Although these non-functional requirements are often more implicit than\\nfunctional ones and are not explicitly outlined in the user descriptions [41], they reflect common\\ndefault requirements in real-world software development.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 8}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:9\\nOur open coding annotation reveals that non-functional requirements in coding tasks can be\\nmainly divided into the following aspects: Security, Performance, Style, and Code Smell. Generated\\ncode that violates these non-functional requirements may introduce safety risks or increase the\\nmaintenance complexity to the project.\\nSpecifically, on the security side, the generated code may introduce vulnerabilities such as\\nunsanitized inputs, which can lead to insecure deserialization or SQL injection attacks. As shown in\\nFigure 4, the ground-truth code uses the safe_load function to safely read YAML files. In contrast,\\nthe LLM-generated code utilizes the load function, thereby introducing a potential security risk.\\nRegarding performance, the generated code may lack optimization for execution efficiency, for\\nexample, by using inefficient loop structures that lead to unnecessary overhead in computing and\\nmemory resources. Style violations often occur when the generated code fails to follow established\\nprogramming conventions or style guides, such as inconsistent naming conventions or inappropriate\\ncode layout, which can negatively affect code readability and maintainability. Code smell violations\\ninclude issues such as overly complex functions or excessive use of global variables, which increase\\nthe complexity and potential risks associated with future maintenance.\\n4.1.2\\nFactual Knowledge Conflicts (31.91%). In the field of NLP, the term “factual conflicts” refers\\nto content generated by LLMs that does not align with established knowledge or facts about the\\nreal world. Practical software development similarly relies on various types and levels of factual\\nknowledge to produce correct code. Consequently, when LLMs fail to accurately understand and\\napply background knowledge [40], library/framework knowledge, or API knowledge, hallucinations\\non Factual Knowledge Conflicts arise. We further divide this hallucination category into three types:\\nBackground Knowledge Conflicts, Library Knowledge Conflicts, and API Knowledge Conflicts.\\nBackground Knowledge Conflicts (8.82%). Background Knowledge Conflicts are a common issue\\nwhen using large language models. These conflicts refer to the situation that the generated code is\\ninconsistent with existing domain-specific knowledge, potentially rendering the code invalid or\\nintroducing logic bugs and risks. For instance, in automotive software development, if the generated\\ncode fails to adhere to certain industry standards (e.g., AUTOSAR [1]), it can result in significant\\ncompliance issues or safety risks.\\nBackground knowledge typically includes Domain Concepts (e.g., specific data formats or pro-\\ntocols) and related Standards and Specifications (e.g., standard parameters or configurations). For\\nexample, Figure 5 shows an example about OCFL (Oxford Common File Layout), a specification for\\ndata storage and transformation. According to the official description [3], an OCFL storage root\\nmust contain a “Root Conformance Declaration” following the “NAMASTE” specification and may\\ninclude a file named ocfl_layout.json to describe the root layout arrangement. However, the\\ngenerated code might incorrectly focus on other OCFL aspects that are irrelevant to storage root.\\nLibrary Knowledge Conflicts (2.68%). In modern software development, developers frequently\\nemploy frameworks or third-party libraries (e.g., Django [2] for web applications) to expedite the\\ndevelopment process by reusing the features or functionalities that these frameworks or libraries\\nprovide. When utilizing these frameworks or libraries, LLMs may encounter factual errors that\\nlead to unexpected behaviors or even security risks. For example, As depicted in Figure 6, the\\ntask requires the model to generate a decorator that caches the return value of the function upon\\neach invocation. In the code generated by the DeepSeekCoder model, the APIs from the asyncio\\nframework are utilized. This framework is designed for asynchronous processing, and the model’s\\nmisuse of this framework poses unexpected behaviors to the developed application.\\nAPI Knowledge Conflicts (20.41%). API Knowledge Conflicts are a common hallucination in\\nLLM-generated code caused by various types of API misuses, such as parameter errors, improper\\nguard conditions, similar-but-incorrect/deprecated API usage, and improper exception handling.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 9}, page_content='ISSTA022:10\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\ndef initialize(self):\\n    self.root = File(self.path, “ro”, \"ocfl_storage.json\")\\n    self.root.open()\\n    self.root.create_child(\"ocfl_storage\", \"ocfl_storage\")\\n    self.root.create_child(\"ocfl_storage \", \"ocfl_storage.json\")\\n    ... //omitted\\n    self.root.create_child(\"ocfl_storage\", \"ocfl_storage.tar.bz2\")\\ndef initialize(self):\\n (parent, root_dir) = fs.path.split(self.root)\\nparent_fs = open_fs(parent)\\nif parent_fs.exists(root_dir):\\nraise StoreException(\"OCFL storage root %s already exists, aborting!\" % (self.root))\\nself.root_fs = parent_fs.makedir(root_dir)\\nNamaste(d=0, content=self.declaration_tvalue).write(pyfs=self.root_fs)\\n if self.disposition is not None:\\nwith self.root_fs.open(self.layout_file, \\'w\\') as fh:\\nlayout = {\\'extension\\': self.disposition,\\n\\'description’: \"...\"}\\njson.dump(layout, fh, sort_keys=True, indent=2)\\nGround-truth\\nDocstring \\nCreate and initialize a new OCFL storage root.\\n:param self: an OCFL storage root\\n:return:\\nOCFL storage root\\nLLM Generation\\nCreate an OCFL \\nstorage root\\nMisunderstood \\nOCFL and \\ngenerated wrong \\nfile structure\\nFig. 5. Example: Background Knowledge Conflicts.\\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\\n    timer = timer or (lambda: perf_counter())\\n    is_async = asyncio.iscoroutinefunction\\n    is_async_gen = asyncio.iscoroutine\\n    get_loop = asyncio.get_event_loop\\n   if is_async(func):\\n        return _cache(TTLCache(maxsize, ttl, timer), typed)(maxsize)\\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\\n if maxsize is None:\\nreturn _cache(_UnboundTTLCache(ttl, timer), typed)\\nelif callable(maxsize):\\nreturn _cache(TTLCache(128, ttl, timer), typed)(maxsize)\\nelse:\\nreturn _cache(TTLCache(maxsize, ttl, timer), typed)\\nGround-truth\\nDocstring \\nDecorator to wrap a function with a memoizing callable that saves up to `maxsize` \\nresults based on a Least Recently Used (LRU) algorithm with a per-item time-to-live \\n(TTL) value.\\n:param maxsize ttl timer typed\\n:type\\nint int object boolean\\n:return Decorator\\nLLM Generation\\nCorrectly handle \\nthe decorator\\nIncorrectly adopt the \\nasyncio framework\\nFig. 6. Example: Library Knowledge Conflicts.\\nFor example, parameter errors can occur when inappropriate parameter types or values are used in\\nthe generated code, causing API calls to fail or return unexpected results. This case is especially\\ncommon in dynamically typed programming language such as Python. Improper guard conditions\\nmean that the generated code does not correctly implement pre-condition checks. If the validity of\\nthe pre-conditions of certain APIs is not verified before calling them (e.g., file existence), runtime\\nerrors may occur. In terms of similar-but-wrong/deprecated API usage, LLMs may mistakenly\\nchoose APIs with similar functions but different applicable scenarios. Although this choice is\\nsyntactically correct, it cannot meet actual application needs. Improper exception handling involves\\ngenerating code that fails to properly handle potential exceptions throwed by certain APIs, which\\ncan cause the program to crash or behave abnormally when faced with an error condition. This\\nkind of API knowledge conflict will not only directly lead to program functional errors, but may\\nalso affect the stability of the system and the usability of the code.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 10}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:11\\ndef parse_frequency(frequency):\\nif not frequency:\\n… … //omitted\\nif not time_unit.endswith(\\'s\\'):\\ntime_unit += \\'s\\'\\nif time_unit == \\'months\\':\\nnumber *= 30\\ntime_unit = \\'days’\\nelif time_unit == \\'years\\':\\nnumber *= 365\\ntime_unit = \\'days\\'\\ntry:\\nreturn datetime.timedelta(**{time_unit: number})\\nexcept TypeError:\\nraise ValueError(f\"Could not parse consistency check frequency \\'{frequency}\\'\")\\ndef parse_frequency(frequency):\\nif frequency == \"always\":\\nreturn None\\nelif frequency == \"years\":\\nreturn datetime.timedelta(year=1)\\n... //omitted\\nelse:\\nraise ValueError(\"Unknown frequency: \\'%s\\'\", frequency)\\nGround-truth\\nDocstring \\nGiven a frequency string with a number and a unit of time, return a corresponding datetime.timedelta instance or None if the frequency \\nis None or “always”. For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\\nRaise ValueError if the given frequency cannot be parsed.\\nLLM Generation\\nCorrect use of parameter ‘days\\' \\nin datetime.timedelta()\\nIncorrect use of a non-existent \\nparameter \\'years\\' in \\ndatetime.timedelta()\\nFig. 7. Example: API Knowledge Conflicts.\\nWe present an example in Figure 7. In this generation task, CodeGen correctly identifies the task\\nintent and utilizes the datetime.timedelta() function. However, the code snippet generated by\\nCodeGen uses a non-existing parameter year.\\n4.1.3\\nProject Context Conflicts (24.56%). Project Context Conflict hallucination refers to the phenom-\\nenon where the code generated by LLMs is inconsistent with the specific context of a given project.\\nIn a sense, this type of hallucination is also a type of factual conflict, where facts within the current\\nproject context are violated. The key difference is that Factual Knowledge Conflicts involve common\\nfacts (e.g., libraries and APIs) that are publicly accessible, while Project Context Conflicts pertain\\nto facts that are specific to the corresponding project, which are generally unavailable for public\\naccess. Project Context Conflicts are often caused by LLMs not aware of such project-specific facts\\nwhen generating code. This hallucination can be divided into Environment Conflicts, Dependency\\nConflicts, and Non-code Resource Conflicts.\\nEnvironment Conflicts (0.94%). In the process of software development, conflicts between the\\ngenerated code and the development environment are common, especially regarding version\\ndifferences in platforms, operating systems, drivers, languages, compilers/interpreters, frameworks,\\nand libraries. When generating code, such environmental concerns are often not considered, leading\\nto problematic code if there are environment-sensitive operations. For example, if the generated\\ncode uses language features (e.g., f-string expressions) from higher Python versions that are not\\nsupported by the current development environment, a conflict arises. For example, Figure 8 shows\\na code snippet generated by CodeGen that attempts to use the package _lfu_cache, which does\\nnot exist in the current environment.\\nDependency Conflicts (11.26%). Dependency Conflicts arise when the generated code relies on\\nundefined or unimported dependencies, such as user-defined attributes and functions. This often\\nresults in errors such as undefined variables or no-member errors. In practical software development,\\n70% of functions are non-standalone and depend on entities defined elsewhere in the project or\\nimported from third-party libraries [46]. Due to the inability of LLMs to access the entire project\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 11}, page_content=\"ISSTA022:12\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\ndef lfu_cache(maxsize=128, typed=False):\\n       import _lfu_cache\\n  return _lfu_cache.lfu_cache(maxsize, typed)\\ndef lfu_cache(maxsize=128, typed=False):\\n  if maxsize is None:\\n    return _cache(_UnboundCache(), typed)\\n  elif callable(maxsize):\\n    \\nreturn _cache(LFUCache(128), typed)(maxsize)\\n  else:\\n       return _cache(LFUCache(maxsize), typed)\\nGround-truth\\nDocstring \\nDecorator to wrap a function with a memoizing callable that saves up to `maxsize` results based on a Least \\nFrequently Used (LFU) algorithm.\\nLLM Generation\\nUsing a non-existent \\npackage  in the current \\nenvironment\\nUse the current \\ndevelopment \\nenvironment’s classes \\nFig. 8. Example: Environment Conflicts.\\ndef generate_default_observer_schema(app):\\napp.status.mangled_observer_schema = deepcopy(app.observer_schema)\\n   observer_schema = {}\\nfor resource_manifest in app.spec.manifest:\\ntry:\\nif resource_manifest not in app.observer_schemas:\\n    observer_schema[resource] = generate_default_observer_schema_dict()\\nexcept IndexError:\\napp.status.mangled_observer_schema.append(\\ngenerate_default_observer_schema_dict(\\nresource_manifest,\\nfirst_level=True))\\n   return observer_schema\\ndef generate_default_observer_schema(app):\\nobserver_schema = {}\\nresources = app.spec.manifest\\nfor resource in resources:\\nif resource not in app.observer_schemas:\\nobserver_schema[resource] = generate_default_schema()\\nreturn observer_schema \\nGround-truth\\nDocstring \\nGenerate the default observer schema for each Kubernetes resource present \\nin``spec.manifest`` for which a custom observer schema hasn't been specified.\\n:param app (krake.data.kubernetes.Application): The application for which to generate \\na default observer schema\\n:return \\nLLM Generation\\nProper use of the\\ngenerate_default_\\nschema function\\nError using non-existing \\ngenerate_default_observer_\\nschema_dict function.\\nFig. 9. Example: Dependency Conflicts.\\ncontext, they often resort to using non-existent APIs, functions, attributes, and variables when\\ndealing with non-standalone functions.\\nFor example, Figure 9 illustrates a scenario involving a user-defined function generate_default\\n_observer_schema_dict(). In this case, the PanGu-𝛼erroneously uses a function with a similar\\nbut incorrect name, generate_default_schema(), which does not exist in the project. This leads\\nto a Dependency Conflict, as the code fails to execute correctly due to the missing definition.\\nNon-code Resource Conflicts (12.36%). Non-code Resource Conflicts can be further categorized\\ninto four main types: Data, Configs, Assets, and Connections. Each type of conflict can undermine the\\ncorrectness and reliability of the system. Data conflicts often involve mishandling of data formats,\\nfields, or content. For example, if the generated code incorrectly parses a data file or attempts to\\naccess a non-existent data field, it can lead to runtime errors or data inconsistencies. Config conflicts\\narise from incorrect settings or options in configuration files. This might include using undefined\\nconfiguration fields or options, which can prevent the generated code from properly applying\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.\"),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 12}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:13\\ndef get_repo_archive(self, url: str, destination_path: Path) -> Path:\\n  archive_name = url.split(\\'/\\')[-1]\\n  archive_path = os.path.join(os.getcwd(), \"archive\", archive_name)\\n  if not os.path.exists(archive_path):\\n    os.makedirs(archive_path)\\n    tar = tarfile.open(archive_path, \"w:gz\")\\n  for i in range(len(url.split(\\'/\\'))-1):\\n    tar.add(url.split(\\'/\\')[i+1], arcname=url.split(\\'/\\')[i+1])\\n    tar.close()\\n  return archive_path\\ndef get_repo_archive(self, url: str, destination_path: Path) -> Path:\\n  res = self.http_request(url)\\n  destination_path.parent.mkdir(parents=True, exist_ok=True)\\n  destination_path.write_bytes(res.content)\\n  extract_to = Path(str(destination_path).split(\".tar.gz\")[0])\\n  tar = tarfile.open(destination_path)\\n  tar.extractall(path=extract_to)\\n  tar.close()\\n  return extract_to\\nGround-truth\\nDocstring \\nGiven an url and a destination path, retrieve and extract.tar.gz archive which contains \\n\\'desc\\' file for each package\\n    Args:\\n      url: url of the.tar.gz archive to download\\n      destination_path: the path on disk where to extract archive\\n    Returns:\\n        path where the archive is extracted to\\nLLM Generation\\nSuccessfully fetched the \\nfile with the correct path \\nand filename\\nFail to fetch the \\nfile due to the use \\nof a non-existent \\nfile path\\nFig. 10. Example: Non-code Resource Conflicts.\\nthe configuration and affect system behavior. Asset conflicts here involve improper handling of\\nasset files and their properties. For instance, if the generated code fails to set the correct size and\\nresolution for images or videos, it can result in display issues or severe bugs, such as application\\ncrashes. Connection conflicts relate to wrong settings of various connection resources, such as\\nincorrect IP addresses, port numbers, or database tables. These issues often lead to failed connections\\nor operations being performed on the wrong server or database, potentially causing data leaks or\\nsecurity incidents.\\nFor example, Figure 10 illustrates the generation task hopes that LLM can generate a function\\nfor a given URL and target path to retrieve and extract the tar.gz compressed package containing\\neach package’s “description” file. However, in the code snippet generated by the model, the model\\nadds “archive” as a path in the target path, which causes the code snippet to point to a non-existent\\nfile path. This will not allow the tar.gz compressed package to be correctly obtained, resulting in\\na program error.\\nRQ1 Summary: We have established a hallucination taxonomy in LLM-based code generation,\\ncomprising three main categories (i.e., Task Requirement Conflicts, Factual Knowledge Conflicts,\\nand Project Context Conflicts) with eight subtypes. Among these, Task Requirement Conflicts are\\nthe most frequently occurring category.\\n4.2\\nRQ2: LLM Comparison\\nBased on the obtained hallucination taxonomy for LLM-based code generation, we further analyze\\nthe hallucination distribution comparison across different models. Figure 11 shows the distribution\\nof the number of hallucinations of different models based on the breakdown analysis of the three\\nhallucination types. We find that Task Requirement Conflicts are the most common hallucination\\ntype for all models, while Factual Knowledge Conflicts and Project Context Conflicts remain at\\napproximately the same frequency.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 13}, page_content='ISSTA022:14\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n1400\\n1600\\nTask Requirement Conflicts\\nFactual Knowledge Conflicts\\nProject Context Conflicts\\nCodeGen\\nPanGu-α\\nChatGPT\\nDeepSeekCoder\\nCodeLlama\\nStarCoder2\\nCodegen\\nPanGu-α\\nChatGPT\\nDeepSeekCoder\\nCodeLlama\\nStarCoder2\\nFig. 11. Hallucination distribution of different models\\nAdditionally, we find that Code-\\nGen and StarCoder2 exhibit a notably\\nhigher frequency of hallucinations re-\\nlated to Task Requirement Conflicts,\\nwhereas DeepSeekCoder and CodeL-\\nlama demonstrates the lowest occur-\\nrence. This variation may be related\\nto the models’ ability to understand\\ntask requirements, potentially influ-\\nenced by factors such as the model\\nsize or the training corpora. For in-\\nstance, DeepSeekCoder and CodeL-\\nlama are trained on diverse corpora\\nincluding both extensive code and\\ntext data, while CodeGen and StarCoder2 are primarily trained on code-related data. In terms of\\nFactual Knowledge Conflicts, PanGu-𝛼demonstrates the highest frequency of factual hallucinations.\\nThis can be attributed to its extensive training on Chinese corpora, which may have led to a\\nrelatively limited exposure to factual knowledge, such as specific domain concepts, expressed in\\nEnglish.\\nRQ2 Summary:\\nTask Requirement Conflicts are the most prevalent type of hallucination across all models, with\\nCodeGen and StarCoder2 showing a notably higher frequency of this type compared to others.\\n4.3\\nRQ3: Potential Cause Discussion\\nIn this research question, we conduct further analysis on the possible root causes of the hallucina-\\ntions in practical LLM-based code generation.\\n4.3.1\\nTraining Data Quality. The quality of the training data is a crucial factor in the development\\nof LLMs, as it significantly affects models’ inference capabilities. Recent LLMs are often trained on\\nlarge-scale code corpora typically collected from open-source repositories. However, the quality\\nof these repositories is not always assured, leading to the inclusion of low-quality data in the\\ntraining corpora. Such issues include mismatches between docstrings and code [36], inefficient\\nor insecure code implementations [22], misused API calls, outdated library documentation and\\nusage [52], and a lack of domain diversity. When LLMs are trained on such corpora, they may\\nunintentionally incorporate these flaws into their knowledge base, leading to hallucinations in\\ncode generation. As shown in Figure 4, LLMs may generate code that uses unsafe APIs, reflecting\\nproblematic patterns commonly found in the training data (e.g., there are 256k lines of Python code\\nin the GitHub repository that use yaml.load() instead of the safer yaml.safe_load() API). This\\nindicates that the model may have been affected by low-quality data during the training phase.\\nMost hallucinations associated with Task Requirement Conflicts and Factual Knowledge Conflicts\\ncan be, to a certain extent, attributed to data quality issues in the training corpora. This highlights\\nthe importance of building a high-quality code-related training data to reduce hallucinations in\\ncode generation.\\n4.3.2\\nIntention Understanding Capacity. Although LLMs have shown great potential in code gener-\\nation, they still face challenges in accurately capturing and interpreting specific user intentions\\nand needs [30]. This limitation can result in generated code that is functionally or non-functionally\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 14}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:15\\ninaccurate, thereby affecting the overall effectiveness and trustworthiness of LLM-based code gen-\\neration [35]. The core advantage of LLMs lies in their excellent pattern recognition capabilities, but\\nthis is also the source of their limitations. LLMs tend to generate code based on common patterns\\nobserved in the training data rather than from a deep understanding of the specific requirements\\ncontext. As shown in Figure 3, the task description requires the LLM to handle LocalTime, which\\nis ignored in the LLM-generated code. This example highlights LLMs’ inadequacy in comprehen-\\nsively interpreting the intentions behind requirements. Furthermore, LLMs also show limitations in\\nhandling subtle requirements involving complex logic or multi-step operations [47]. Due to a poor\\nunderstanding of the overall scope and potential limitations of the task, LLM-generated code may\\nonly address part of the requirements or perform poorly in handling edge cases. This can result in\\ngenerated code snippets that seem correct on the surface but fail to meet specific business logic or\\nfunctional requirements in practice. As shown in Figure 4, although the code generated by LLMs\\nfunctionally matches the description of the problem, such code will be more vulnerable to attacks\\nin real development scenarios.\\n4.3.3\\nKnowledge Acquisition Capacity. LLMs may learn incorrect knowledge and miss certain\\ndomain-specific knowledge due to the aforementioned training data quality issues. There is a\\ndisparity in the distribution of data across different domains within the training dataset. The\\ncomputation domain constitutes a substantial 63% of the total data, while the network domain\\nis markedly less represented, comprising only 8% [53]. For example, as shown in Figure 5, the\\ntask description needs a piece of code for generating a data format that satisfies the OCFL storage\\nspecification, but LLM generates incorrect code, possibly due to its lack of the OCFL-related\\nknowledge. Moreover, as software development techniques evolve, such as library updates, relevant\\nknowledge developed after model training period cannot be acquired by LLMs. Unlike human\\ndevelopers who can continuously learn and integrate latest information during development, LLMs\\nare limited to the knowledge available at the time of training. This limitation in LLMs’ knowledge\\nacquisition capacity leads to hallucinations related to incorrect or outdated factual information\\nin the generated code. This highlights the need for a knowledge acquisition mechanism, such\\nas retrieval augmented generation (RAG), to allow LLMs to update, correct, and supplement the\\nknowledge they have learned.\\n4.3.4\\nRepository-level Context Awareness. Feeding all project contexts, including code, documents,\\nand non-code resources, into an LLM for repository-level code generation is challenging and\\nimpractical. This is because LLMs, typically based on the Transformer architecture [38], have\\ntoken number limits (e.g. 8k or 12k tokens) and experience quadratic computation growth as the\\nnumber of tokens increases. Additionally, including all project contexts can introduce a significant\\namount of irrelevant information, hindering LLMs’ ability to focus on the most relevant context for\\ncode generation. Therefore, it is crucial to develop methods that make LLMs aware of the project\\ncontexts (project-specific memory) that are precisely related to the current coding task. Recent\\nworks attempt to integrate static analysis tools [42] or apply retrieval-augmented generation (RAG)\\nbased on repository-level retrieval corpora [49] to address such context awareness issues.\\nRQ3 Summary: By further analyzing the causes of hallucinations, we identify four possible\\ncontributing factors: training data quality, intention understanding capacity, knowledge acquisi-\\ntion capacity, and repository-level context awareness. Deficiencies in any of these factors can\\nlead to hallucinations in practical development scenarios.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 15}, page_content='ISSTA022:16\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\n5\\nMitigation Approach\\n5.1\\nMotivation\\nThe aforementioned root causes of hallucinations in code generated by LLMs can be traced back\\nto three main factors at the inference stage: incorrect or insufficient understanding for task re-\\nquirements, the lack of factual knowledge pertinent to the generation tasks, and the inability to\\naccess the necessary code and non-code resources from the repository. These limitations create\\nsubstantial challenges for LLMs in code generation in practical development settings. Drawing\\ninspiration from existing work [49] on repository-level code generation, we explore the feasibility\\nof applying retrieval-augmented generation (RAG) to mitigate hallucinations. The idea is that by\\nproviding LLMs with code snippets relevant to the current task, they can better understand the\\nrequirements and gain awareness of specific factual knowledge and project contexts.\\n5.2\\nRAG-based Mitigation\\nTo implement the RAG method, we first collect all code repositories from the CoderEval dataset\\nand follow RepoCoder’s method [49] to construct the retrieval corpora. Specifically, for each\\nrepository, we apply a sliding window to scan all the source files in it. This scanning process\\nextracts consecutive lines of code based on a predefined window size. The sliding window moves\\nby a fixed number of lines (slicing step) at each iteration to ensure complete coverage of the code.\\nWe adhere to RepoCoder’s parameter settings, with a window size of 20 lines and a sliding step of\\n2 lines. To prevent answer leakage, code lines containing or following the ground-truth code are\\nexcluded from the scanning process. Once all files are processed, a retrieval corpus of code snippets\\nis generated for the repository.\\nWe employ a sparse bag-of-words (BOW) model for our retrieval mechanism, which simplifies\\ngauging similarity between textual data. This model transmutes both the query and the candidate\\ncode snippets into sets of tokens, which are compared using the Jaccard index. The Jaccard index\\nmeasures the similarity between two sets by dividing the size of their intersection by the size of\\ntheir union, we choose the code snippet that retrieves the top ten scores each time to return as the\\nprompt for the LLMs.\\n5.3\\nEvaluation\\nTable 1. Experimental results of mitigation method under Pass@1.\\nModel\\nRaw Method\\nRAG-based Mitigation\\nCodeGen\\n1.30%\\n2.61% (↑1.31%)\\nPanGu-𝛼\\n0.04%\\n1.74% (↑1.70%)\\nDeepSeekCoder\\n3.04%\\n3.91% (↑0.87%)\\nCodeLlama\\n2.17%\\n5.22% (↑3.05%)\\nStarCoder2\\n0.04%\\n2.61% (↑2.57%)\\nChatGPT\\n10.40%\\n14.78% (↑4.38%)\\nWe evaluate the effectiveness of the\\nRAG-based mitigation method with\\nthe six LLMs: CodeGen, PanGu-𝛼,\\nChatGPT, DeepSeekCoder, CodeL-\\nlama, and StarCoder2 on the CodeE-\\nval dataset. We compared our RAG-\\nbased mitigation method with the\\nRaw method. In the Raw method, we\\nonly provide LLMs basic docstrings\\nand function signatures. In the RAG-\\nbased mitigation, when providing\\ndocstrings and function signatures, we will obtain ten related code snippets from the above-\\nconstructed retrieval library through a similarity algorithm as prompts and provide them to LLMs.\\nWe use the Pass@1 metric to assess the functionality correctness of the generated code snippets\\naccording to test cases. As shown in Table 1, the Pass@1 scores of all six models are slightly\\nimproved with the RAG-based mitigation method. Note that the performance improvement in our\\nexperiments is modest, as the mitigation method we explored is preliminary. We consider this\\nexperiment as an pilot study to explore the potential effectiveness of RAG-based mitigation. In\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 16}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:17\\nTask Requirement Conflicts\\nFactual Knowledge Conflicts\\nProject Context Conflicts\\nCorrect\\n79\\n37\\n15\\n8\\n6\\n35\\n5\\n2\\n25\\n4\\n52\\n7\\n4\\n1\\n2\\n17\\n139\\n48\\n24\\n88\\nFig. 12. Hallucination mitigation of ChatGPT.\\nfuture work, there are more methods worth studying, such as model fine-tuning and multi-agent\\nframework with tool using, etc.\\nWe also investigate how hallucinations evolve before and after employing RAG-based mitigation.\\nWe manually track the hallucinations in the code snippets generated by ChatGPT in both the\\nRAW Method and RAG-based Mitigation. Figure 12 presents the analysis results in four sub-\\nfigures, each representing the evolution of a specific category of hallucination. The results reveal\\nseveral interesting findings. First, among the three categories of hallucinations, the RAG-based\\nmitigation shows the least impact on Factual Knowledge Conflicts (see the second sub-figure). This\\nis intuitive, as the RAG in our experiments is utilized to retrieve code contexts from the current\\nworking repositories rather than from external knowledge sources such as API documentation or\\nonline knowledge bases like Wikipedia. Consequently, it does not introduce substantial factual\\nknowledge regarding background, libraries, or APIs. Second, employing the RAG-based method\\ncan mitigate hallucinations related to Task Requirement Conflicts and Project Context Conflicts\\nto a greater extent than those related to Factual Knowledge Conflicts (see the first and third sub-\\nfigures). Specifically, RAG has successfully resolved approximately 8% of Project Context Conflicts\\nand about 6% of Task Requirement Conflicts. This improvement can be attributed to the code\\nsnippets retrieved using the current task descriptions, which enrich the information regarding the\\nrequirements and provide project-specific context. Third, while the RAG-based approach mitigates\\nsome hallucinations, it can also introduce new ones or alter existing hallucination types. For instance,\\napproximately 30% of code generation tasks that were accurately handled by the RAW Method\\nare now mishandled by the RAG-based Mitigation (see the last sub-figure) due to the introduction\\nof new hallucinations. This may occur because the retrieved code snippets can sometimes act as\\nnoise rather than providing informative context, highlighting the importance of effective retrieval\\nalgorithms when implementing RAG-based mitigation.\\nTo further illustrate the effectiveness of the hallucination mitigation, we conduct two case studies.\\nAs shown in Figure 13, in the Raw method, which only provides a docstring and a function signature,\\nCodeGen incorrectly uses the replace function and fails to convert scripts to one-line commands.\\nIn contrast, with the RAG-based method, CodeGen correctly uses the splitlines function, aligning\\nwith the ground-truth and successfully addressing the requirement. In addition, the RAG-based\\nmethod can also effectively mitigate Project Context Conflicts. As shown in Figure 14, in the Raw\\nmethod, ChatGPT attempts to use the self.items.popitem() API, which does not exist in the\\nrepository, leading to hallucinated generation. In contrast, with the RAG-based mitigation, ChatGPT\\ncorrectly implements the requirement using the self.pop() function.\\n6\\nDiscussion\\nWe provide implications for future research on the hallucinations in practical LLM-based code\\ngeneration.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 17}, page_content=\"ISSTA022:18\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\ndef oneline(script, separator=’&&’):\\nreturn seperator.join(script.splitlines())\\ndef oneline(script, separator=’&&’):\\nreturn seperator.join(textwrap.dedent(script).strip().splitlines())\\ndef oneline(script, separator=’&&'):\\nreturn script.replace(’\\\\t', seperator)\\nGround-truth\\nDocstring \\nconverts a script to one line command.THis is useful to run a single ssh command andp\\nass a one line script.\\n:param script:\\n:return:\\nRaw Method\\nCorrectly convert scripts to \\none-line commands\\nTask Requirement \\nConflicts\\nRAG-based Mitigation\\nFig. 13. Example: hallucination mitigation in Task Requirement Conflicts.\\ndef popitem(self):\\ntry:\\nkey = next(iter(self.__order))\\nexcept StopIteration:\\nraise KeyError('%s is empty' % type(self).__name__) from None\\nelse:\\n   return (key, self.pop(key))\\ndef popitem(self):\\ntry:\\nkey = next(iter(self.__order))\\nexcept StopIteration:\\nraise KeyError('%s is empty' % type(self).__name__) from None\\nelse:\\n   return (key, self.pop(key))\\ndef popitem(self):\\nreturn self.items.popitem( )\\nGround-truth\\nDocstring \\nRemove and return the `(key, value)` pair first inserted.\\nRaw Method\\nProper use of the \\nrepository's internal API\\nAttributeError: 'self ' \\nobject has no \\nattribute 'items'\\nRAG-based Mitigation\\nFig. 14. Example: hallucination mitigation in Project Context Conflicts.\\nDeveloping hallucination identification techniques: Through our study, we find three major\\ncategories of hallucinations in the LLM-based code generation. Some hallucinations like Dependency\\nConflicts, Environment Conflicts and API Knowledge Conflicts can be detected by using static analysis\\n(e.g., undefined variables or wrong API methods) or dynamic test execution (runtime errors or test\\nfailures), making it relatively easy for developers to recognize and locate the relevant code issues.\\nHowever, certain hallucinations, such as incomplete functionality and security issues, are very\\ndifficult for developers to detect and correct, as they can likely pass static checks and all test cases. As\\na result, LLM-generated code containing these hallucinations may be introduced into development\\nprojects and even real production environments, leading to unreliable software systems and severe\\nsecurity risks. Existing hallucination localization approaches [4, 29] based on LLM self-feedback\\nmethods can detect hallucinations to a certain extent. However, these approaches heavily rely on\\nthe current model’s capabilities and cannot address the fundamental limitations imposed by the\\ntraining corpora. Therefore, in future work, researchers may consider developing more effective\\ntechniques to quickly and precisely identify and localize hallucinations in LLM-generated code.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.\"),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 18}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:19\\nDeveloping more effective hallucination mitigation techniques: In Section 5, we explore\\nthe feasibility of applying a lightweight RAG-based method to mitigate hallucinations in LLM-based\\ncode generation. While the method demonstrates effectiveness in mitigating hallucinations such\\nas undefined attributes, the potentials of RAG need to be further explored. For example, we only\\nconstruct retrieval corpus using current code repository, leading to the augmented information\\nis insufficient to mitigate many hallucinations such as background knowledge conflicts. In the\\nfuture, we can integrate more comprehensive knowledge sources like online search engines, API\\ndocuments, and StackOverflow discussions. In addition to RAG techniques, other methods such as\\ninput query refinement [12, 30] and multi-agent systems [17] can also be leveraged to achieve an\\niterative process of (i) clarifying task requirements, (ii) generating code, (iii) running test cases,\\nand (iv) mitigating hallucinations. To achieve this, we need to design the appropriate interaction\\nprotocols between agents and relevant tools (e.g., search engines and static analysis tools) and\\napply suitable prompting strategies.\\n7\\nThreats to Validity\\nExternal Validity. Threats to external validity mainly concern the generalizability of our findings.\\nWe focused on Python when exploring the taxonomy and root causes of hallucinations in LLM-\\nbased code generation due to its simplicity and ease of use. Constructing hallucination taxonomies\\nfor other programming languages and comparing them with our current taxonomy is a valuable\\nfuture direction. Another potential threat is the limited scale of the adopted CoderEval dataset,\\nwhich contains only 230 coding tasks. To mitigate this, we selected six LLMs and had each generate\\n10 code snippets for each task to ensure a sufficient number of annotations.\\nInternal Validity. Threats to internal validity primarily concern the manual annotation process\\nin taxonomy construction. A key issue is the absence of formal inter-rater reliability measure for\\nannotating hallucinations. To address this, discrepancies were discussed and resolved in annotator\\nmeetings to ensure a consistent annotation protocol, with each identified hallucination receiving\\na mutually agreed-upon label. Additionally, to ensure consistency in our findings, one author\\nreviewed all labeled data. Another potential threat is model bias during the annotation process. To\\nmitigate this, we mixed the generation results of the six models before annotation.\\nConstruct Validity. Threats to construct validity are related to evaluating our hallucination\\nmitigation approach. To alleviate these threats, we conducted experiments on six models using\\ntest cases available in the CoderEval dataset, a standard method for evaluating the correctness of\\ngenerated code.\\n8\\nConclusion\\nIn this paper, we conduct an empirical study on code-generated hallucinations of large models in\\nthe practical development scenarios and through a full manual analysis, we construct a taxonomy of\\nhallucinations and follow up with further hallucination classifications. Based on the hallucinations\\nfound, we provide a deeper discussion of the causes of hallucinations and the distribution of\\nhallucinations in different LLMs. At last, we implement a RAG-based approach for hallucination\\nmitigation and further discuss potential hallucination mitigation approaches.\\n9\\nData Availability\\nWe provide the replication package for this study at https://github.com/DeepSoftwareAnalytics/\\nLLMCodingHallucination.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 19}, page_content='ISSTA022:20\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\nAcknowledgments\\nThis work is supported by CCF-Huawei Populus Grove Fund CCF-HuaweiSE202403. This work is\\nsupported by the Guangdong Basic and Applied Basic Research Foundation (2023A1515012292)\\nand CCF - Sangfor ’Yuanwang’ Research Fund.\\nReferences\\n[1] [n. d.]. AUTOSAR - Wikipedia Page. Retrieved Nov. 1, 2024 from https://en.wikipedia.org/wiki/AUTOSAR\\n[2] [n. d.]. Django - The web framework for perfectionists with deadlines.\\nRetrieved Nov. 1, 2024 from https://www.\\ndjangoproject.com/\\n[3] [n. d.]. OCFL - Specifications. Retrieved Nov. 1, 2024 from https://ocfl.io/1.1/spec/#storage-root\\n[4] Ayush Agrawal, Mirac Suzgun, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language Models Know When\\nThey’re Hallucinating References? arXiv preprint arXiv:2305.18248 (2023).\\n[5] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas\\nMuennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian\\nZi, Joel Lamy-Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert,\\nFrancesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu,\\nPaulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li,\\nDzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra.\\n2023. SantaCoder: don’t reach for the stars! CoRR abs/2301.03988 (2023). https://doi.org/10.48550/ARXIV.2301.03988\\narXiv:2301.03988\\n[6] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\\nCarrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models.\\nCoRR abs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732\\n[7] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with\\ncode-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85–111.\\n[8] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,\\nKyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow,\\nBen Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. CoRR\\nabs/2204.06745 (2022). https://doi.org/10.48550/ARXIV.2204.06745 arXiv:2204.06745\\n[9] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language\\nModeling with Mesh-Tensorflow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please cite it using\\nthese metadata..\\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison\\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,\\nHeidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,\\nLukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas\\nTezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira\\nMurati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\\nZaremba. 2021. Evaluating Large Language Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374\\nhttps://arxiv.org/abs/2107.03374\\n[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374 (2021).\\n[12] Kaustubh D Dhole, Ramraj Chandradevan, and Eugene Agichtein. 2023. An interactive query generation assistant\\nusing LLM-based prompt modification and user feedback. arXiv preprint arXiv:2311.11226 (2023).\\n[13] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng,\\nand Yiling Lou. 2023. Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation.\\narXiv preprint arXiv:2308.01861 (2023).\\n[14] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke\\nZettlemoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh\\nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\nhttps://openreview.net/pdf?id=hQwb-lbM6EL\\n[15] Mark Grechanik, Collin McMillan, Luca DeFerrari, Marco Comi, Stefano Crespi, Denys Poshyvanyk, Chen Fu, Qing Xie,\\nand Carlo Ghezzi. 2010. An empirical investigation into a large-scale Java open source code repository. In Proceedings\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 20}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:21\\nof the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement. 1–10.\\n[16] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al.\\n2024. DeepSeek-Coder: When the Large Language Model Meets Programming–The Rise of Code Intelligence. arXiv\\npreprint arXiv:2401.14196 (2024).\\n[17] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang.\\n2024. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680\\n(2024).\\n[18] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration.\\narXiv preprint arXiv:1904.09751 (2019).\\n[19] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng,\\nXiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy,\\nchallenges, and open questions. arXiv preprint arXiv:2311.05232 (2023).\\n[20] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and\\nPascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1–38.\\n[21] Shahedul Huq Khandkar. 2009. Open coding. University of Calgary 23, 2009 (2009).\\n[22] Jan H Klemmer, Stefan Albert Horstmann, Nikhil Patnaik, Cordelia Ludden, Cordell Burton Jr, Carson Powers, Fabio\\nMassacci, Akond Rahman, Daniel Votipka, Heather Richter Lipford, et al. 2024. Using AI Assistants in Software\\nDevelopment: A Qualitative Study on Security Practices and Concerns. arXiv preprint arXiv:2405.06371 (2024).\\n[23] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida\\nWang, and Tao Yu. 2023. DS-1000: A natural and reliable benchmark for data science code generation. In International\\nConference on Machine Learning. PMLR, 18319–18345.\\n[24] Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. EvoCodeBench: An Evolving Code Generation\\nBenchmark Aligned with Real-World Code Repositories. arXiv preprint arXiv:2404.00599 (2024).\\n[25] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier\\nDehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,\\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo\\nWang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan\\nZhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas,\\nMaxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey\\nSchoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-\\nGavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean\\nHughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with\\nyou! CoRR abs/2305.06161 (2023). https://doi.org/10.48550/ARXIV.2305.06161 arXiv:2305.06161\\n[26] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James\\nKeeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,\\nXinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,\\nEsme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-\\nLevel Code Generation with AlphaCode. CoRR abs/2203.07814 (2022). https://doi.org/10.48550/ARXIV.2203.07814\\narXiv:2203.07814\\n[27] Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and Li Zhang. 2024. Exploring and Evaluating\\nHallucinations in LLM-Powered Code Generation. arXiv preprint arXiv:2404.00971 (2024).\\n[28] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint\\narXiv:2402.19173 (2024).\\n[29] Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination\\ndetection for generative large language models. arXiv preprint arXiv:2303.08896 (2023).\\n[30] Fangwen Mu, Lin Shi, Song Wang, Zhuohao Yu, Binquan Zhang, Chenxue Wang, Shichao Liu, and Qing Wang. 2023.\\nClarifyGPT: Empowering LLM-based Code Generation with Intention Clarification. arXiv preprint arXiv:2310.10996\\n(2023).\\n[31] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.\\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).\\n[32] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\\n2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In The Eleventh\\nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\nhttps://openreview.net/pdf?id=iaYcJKpY2B_\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 21}, page_content='ISSTA022:22\\nZ. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\n[33] OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt.\\n[34] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\\nRemez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron\\nGrattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,\\nThomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. CoRR abs/2308.12950\\n(2023). https://doi.org/10.48550/ARXIV.2308.12950 arXiv:2308.12950\\n[35] Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md Rafiqul Islam Rabin, Susmit Jha, Prem Devanbu, and\\nToufique Ahmed. 2024. Quality and Trust in LLM-generated Code. arXiv preprint arXiv:2402.02047 (2024).\\n[36] Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022. On the importance of building high-quality training datasets\\nfor neural code search. In Proceedings of the 44th International Conference on Software Engineering. 1609–1620.\\n[37] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A comprehensive\\nsurvey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313 (2024).\\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\\nIllia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual\\nConference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon,\\nUlrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.).\\n5998–6008. https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\\n[39] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:\\n//github.com/kingoflolz/mesh-transformer-jax.\\n[40] Chong Wang, Xin Peng, Zhenchang Xing, and Xiujie Meng. 2023. Beyond literal meaning: Uncover and explain implicit\\nknowledge in code through wikipedia-based concept linking. IEEE Transactions on Software Engineering 49, 5 (2023),\\n3226–3240.\\n[41] Chong Wang, Xin Peng, Zhenchang Xing, Yue Zhang, Mingwei Liu, Rong Luo, and Xiujie Meng. 2023. Xcos: Explainable\\ncode search based on query scoping and knowledge graph. ACM Transactions on Software Engineering and Methodology\\n32, 6 (2023), 1–28.\\n[42] Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, and Xin Peng. 2024. Teaching Code LLMs to\\nUse Autocompletion Tools in Repository-Level Code Generation. arXiv preprint arXiv:2401.06391 (2024).\\n[43] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+: Open Code\\nLarge Language Models for Code Understanding and Generation. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and\\nKalika Bali (Eds.). Association for Computational Linguistics, 1069–1088. https://aclanthology.org/2023.emnlp-main.68\\n[44] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained\\nEncoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Em-\\npirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11\\nNovember, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for\\nComputational Linguistics, 8696–8708. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.685\\n[45] Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023. Cognitive mirage: A review of hallucinations in\\nlarge language models. arXiv preprint arXiv:2309.06794 (2023).\\n[46] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao\\nXie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings\\nof the 46th IEEE/ACM International Conference on Software Engineering. 1–12.\\n[47] Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022. When language model meets\\nprivate library. arXiv preprint arXiv:2210.17236 (2022).\\n[48] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda\\nZhang, et al. 2021. PanGu-𝛼: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel\\nComputation. arXiv preprint arXiv:2104.12369 (2021).\\n[49] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu\\nChen. 2023. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint\\narXiv:2303.12570 (2023).\\n[50] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\\nChen, et al. 2023. Siren’s song in the AI ocean: a survey on hallucination in large language models. arXiv preprint\\narXiv:2309.01219 (2023).\\n[51] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the\\nperspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989\\n(2023).\\n[52] Li Zhong and Zilong Wang. 2024. Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large\\nLanguage Model Code Generation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 21841–21849.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': '灤晔敘Ⱐ噥牳楯渠㌮ㄴㄵ㤲㘵㌭㈮㘭ㄮ㐰⸲㘠⡔敘⁌楶攠㈰㈴⤠歰慴桳敡⁶敲獩潮‶⸴⸰㬠䍯湦偵戠ⴠ楳獴愲㕭慩渭瀱㤴⵰\\u2072敶ⴷ昳㘷㔵㡣昭ㄲ㤲㤰⁰㐸ㄠ潮′〲㔭〶ⴲ㉔ㄵ㨰㠺㐳⬰〺〰㬠浯摩晩敤⁵獩湧\\u2069呥硴‴⸲⸰\\u2062礠ㅔ㍘', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'file_path': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'format': 'PDF 1.5', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '', 'modDate': \"D:20250917005408-07'00'\", 'creationDate': 'D:20250618063054Z', 'page': 22}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation\\nISSTA022:23\\n[53] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan\\nZhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse function calls\\nand complex instructions. arXiv preprint arXiv:2406.15877 (2024).\\nReceived 2024-10-31; accepted 2025-03-31\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader( \n",
    "    \"../data/text_files/pdf\",\n",
    "    glob=\"*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    show_progress = False)\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: neural networks.pdf\n",
      "  ✓ Loaded 27 pages\n",
      "\n",
      "Processing: llm_hallucinations.pdf\n",
      "  ✓ Loaded 23 pages\n",
      "\n",
      "Total documents loaded: 50\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 50 documents into 358 chunks\n",
      "\n",
      " Example chunk\n",
      "Content : Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2025. All\n",
      "rights reserved. Draft of August 24, 2025.\n",
      "CHAPTER\n",
      "6\n",
      "Neural Networks\n",
      "“[M]achines of this character can behave i...\n",
      "Metadata : {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}\n",
      "Total chunks created: 358\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\" Split documents into smaller chunks\n",
    "    Args:\n",
    "        documents: List of Document objects or raw strings.\n",
    "        chunk_size: Max characters per chunk.\n",
    "        chunk_overlap: Overlap between chunks.\n",
    "    Returns:\n",
    "        List of Document chunks\n",
    "    \"\"\"\n",
    "    # Ensure all inputs are Document objects\n",
    "    if isinstance(documents[0], str):\n",
    "        documents = [Document(page_content=doc, metadata={}) for doc in documents]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    \n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # show example of chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\n Example chunk\")\n",
    "        print(f\"Content : {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata : {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "\n",
    "chunks = split_documents(all_pdf_documents)\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding and vectorstoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x169ae1e80>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"Handles document embedding generation using SenetenceTransformer\"\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"Load the SentenceTransformer model\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(\"Model loaded successfully. Embedding dimension:\", self.model.get_sentence_embedding_dimension())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def embed_documents(self, documents: List[str]) -> np.ndarray:\n",
    "        \"Generate embeddings for a list of documents\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        print(f\"Generating embeddings for {len(documents)} documents:\")\n",
    "        embeddings = self.model.encode(documents, convert_to_numpy=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def get_embedding_dimesnion(self) -> int:\n",
    "        \"Get the dimension of the embeddings\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Embedding model is not loaded.\")\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "##initialize embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we store these embeddings in the VectorStore\n",
    "# VectorStore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x16a9c6120>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"Manages storage and retrieval of document embeddings using ChromaDB\"\n",
    "    def __init__(self, collection_name: str = 'pdf_documents', persist_directory: str = '../data/vector_store'):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"Initialize ChromaDB client and collection\"\n",
    "        try:\n",
    "            #create persistent chromadb client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            #create or get collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata = {\"description\":\"Document embeddings collection\"}\n",
    "            )\n",
    "            print(f\"VectorDB store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            documents (List[Document]): List of LangChain documents\n",
    "            embeddings: corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "\n",
    "        #prepare data for chromadb:\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # create a unique id for each document\n",
    "            doc_id = str(uuid.uuid4())\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # prepare metadata \n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(\"Documents added successfully. Total documents in collection:\", self.collection.count())\n",
    "            print(f\"Successfully added {len(documents)} documents.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to Vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2025. All\\nrights reserved. Draft of August 24, 2025.\\nCHAPTER\\n6\\nNeural Networks\\n“[M]achines of this character can behave in a very complicated manner when\\nthe number of units is large.”\\nAlan Turing (1948) “Intelligent Machines”, page 6\\nNeural networks are a fundamental computational tool for language process-\\ning, and a very old one. They are called neural because their origins lie in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpliﬁed model of the\\nbiological neuron as a kind of computing element that could be described in terms\\nof propositional logic. But the modern use in language processing no longer draws\\non these early biological inspirations.\\nInstead, a modern neural network is a network of small computing units, each\\nof which takes a vector of input values and produces a single output value. In this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='chapter we introduce the neural net applied to classiﬁcation. The architecture we\\nintroduce is called a feedforward network because the computation proceeds iter-feedforward\\natively from one layer of units to the next. The use of modern neural nets is often\\ncalled deep learning, because modern networks are often deep (have many layers).deep learning\\nNeural networks share much of the same mathematics as logistic regression. But'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='neural networks are a more powerful classiﬁer than logistic regression, and indeed a\\nminimal neural network (technically one with a single ‘hidden layer’) can be shown\\nto learn any function.\\nNeural net classiﬁers are different from logistic regression in another way. With\\nlogistic regression, we applied the regression classiﬁer to many different tasks by\\ndeveloping many rich kinds of feature templates based on domain knowledge. When'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='working with neural networks, it is more common to avoid most uses of rich hand-\\nderived features, instead building neural networks that take raw tokens as inputs\\nand learn to induce features as part of the process of learning to classify. We saw\\nexamples of this kind of representation learning for embeddings in Chapter 5, and\\nwe’ll see lots of examples once we start studying deep transformers networks. Nets\\nthat are very deep are particularly good at representation learning. For that reason'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='deep neural nets are the right tool for tasks that offer sufﬁcient data to learn features\\nautomatically.\\nIn this chapter we’ll introduce feedforward networks as classiﬁers, ﬁrst with\\nhand-built features, and then using the embeddings that we studied in Chapter 5.\\nIn subsequent chapters we’ll introduce many other kinds of neural models, most\\nimportantly the transformer and attention, (Chapter 8), but also recurrent neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='networks (Chapter 13) and convolutional neural networks (Chapter 15). And in\\nthe next chapter we’ll introduce the paradigm of neural large language models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='2 CHAPTER 6 • N EURAL NETWORKS\\n6.1 Units\\nThe building block of a neural network is a single computational unit. A unit takes\\na set of real valued numbers as input, performs some computation on them, and\\nproduces an output.\\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one addi-\\ntional term in the sum called a bias term. Given a set of inputs x1...xn, a unit hasbias term\\na set of corresponding weights w1...wn and a bias b, so the weighted sum z can be\\nrepresented as:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='represented as:\\nz = b +\\n∑\\ni\\nwixi (6.1)\\nOften it’s more convenient to express this weighted sum using vector notation; recall\\nfrom linear algebra that a vector is, at heart, just a list or array of numbers. Thusvector\\nwe’ll talk aboutz in terms of a weight vector w, a scalar bias b, and an input vector\\nx, and we’ll replace the sum with the convenientdot product:\\nz = w ·x+b (6.2)\\nAs deﬁned in Eq. 6.2, z is just a real valued number.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='Finally, instead of using z, a linear function of x, as the output, neural units\\napply a non-linear function f to z. We will refer to the output of this function as\\nthe activation value for the unit, a. Since we are just modeling a single unit, theactivation\\nactivation for the node is in fact the ﬁnal output of the network, which we’ll generally\\ncall y. So the value y is deﬁned as:\\ny = a = f (z)\\nWe’ll discuss three popular non-linear functionsf below (the sigmoid, the tanh, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='the rectiﬁed linear unit or ReLU) but it’s pedagogically convenient to start with the\\nsigmoid function since we saw it in Chapter 4:sigmoid\\ny = σ(z) = 1\\n1 +e−z (6.3)\\nThe sigmoid (shown in Fig. 6.1) has a number of advantages; it maps the output\\ninto the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s\\ndifferentiable, which as we saw in Section ?? will be handy for learning.\\nFigure 6.1 The sigmoid function takes a real value and maps it to the range (0,1). It is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='nearly linear around 0 but outlier values get squashed toward 0 or 1.\\nSubstituting Eq. 6.2 into Eq. 6.3 gives us the output of a neural unit:\\ny = σ(w ·x+b) = 1\\n1 +exp(−(w ·x+b)) (6.4)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.1 • U NITS 3\\nFig. 6.2 shows a ﬁnal schematic of a basic neural unit. In this example the unit\\ntakes 3 input values x1,x2, and x3, and computes a weighted sum, multiplying each\\nvalue by a weight (w1, w2, and w3, respectively), adds them to a bias termb, and then\\npasses the resulting sum through a sigmoid function to result in a number between 0\\nand 1.\\nx1\\nx2\\nx3\\ny\\nw1\\nw2\\nw3\\n∑\\nb\\nσ\\n+1\\nz a\\nFigure 6.2 A neural unit, taking 3 inputs x1, x2, and x3 (and a bias b that we represent as a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='weight for an input clamped at +1) and producing an output y. We include some convenient\\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\\nthis case the output of the unit y is the same as a, but in deeper networks we’ll reserve y to\\nmean the ﬁnal output of the entire network, leaving a as the activation of an individual node.\\nLet’s walk through an example just to get an intuition. Let’s suppose we have a\\nunit with the following weight vector and bias:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='unit with the following weight vector and bias:\\nw = [0.2,0.3,0.9]\\nb = 0.5\\nWhat would this unit do with the following input vector:\\nx = [0.5,0.6,0.1]\\nThe resulting output y would be:\\ny = σ(w ·x+b) = 1\\n1 +e−(w·x+b) = 1\\n1 +e−(.5∗.2+.6∗.3+.1∗.9+.5) = 1\\n1 +e−0.87 = .70\\nIn practice, the sigmoid is not commonly used as an activation function. A function\\nthat is very similar but almost always better is the tanh function shown in Fig. 6.3a;tanh\\ntanh is a variant of the sigmoid that ranges from -1 to +1:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='y = tanh(z) =ez −e−z\\nez +e−z (6.5)\\nThe simplest activation function, and perhaps the most commonly used, is the rec-\\ntiﬁed linear unit, also called the ReLU, shown in Fig. 6.3b. It’s just the same as zReLU\\nwhen z is positive, and 0 otherwise:\\ny = ReLU(z) = max(z,0) (6.6)\\nThese activation functions have different properties that make them useful for differ-\\nent language applications or network architectures. For example, the tanh function'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='has the nice properties of being smoothly differentiable and mapping outlier values\\ntoward the mean. The rectiﬁer function, on the other hand, has nice properties that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='4 CHAPTER 6 • N EURAL NETWORKS\\n(a) (b)\\nFigure 6.3 The tanh and ReLU activation functions.\\nresult from it being very close to linear. In the sigmoid or tanh functions, very high\\nvalues of z result in values ofy that are saturated, i.e., extremely close to 1, and havesaturated\\nderivatives very close to 0. Zero derivatives cause problems for learning, because as\\nwe’ll see in Section 6.6, we’ll train networks by propagating an error signal back-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='wards, multiplying gradients (partial derivatives) from each layer of the network;\\ngradients that are almost 0 cause the error signal to get smaller and smaller until it is\\ntoo small to be used for training, a problem called the vanishing gradient problem.vanishing\\ngradient\\nRectiﬁers don’t have this problem, since the derivative of ReLU for high values ofz\\nis 1 rather than very close to 0.\\n6.2 The XOR problem\\nEarly in the history of neural networks it was realized that the power of neural net-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='works, as with the real neurons that inspired them, comes from combining these\\nunits into larger networks.\\nOne of the most clever demonstrations of the need for multi-layer networks was\\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\\nsome very simple functions of its input. Consider the task of computing elementary\\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\\nthe truth tables for those functions:\\nAND OR XOR'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='the truth tables for those functions:\\nAND OR XOR\\nx1 x2 y x1 x2 y x1 x2 y\\n0 0 0 0 0 0 0 0 0\\n0 1 0 0 1 1 0 1 1\\n1 0 0 1 0 1 1 0 1\\n1 1 1 1 1 1 1 1 0\\nThis example was ﬁrst shown for the perceptron, which is a very simple neuralperceptron\\nunit that has a binary output and has a very simple step function as its non-linear\\nactivation function. The output y of a perceptron is 0 or 1, and is computed as\\nfollows (using the same weight w, input x, and bias b as in Eq. 6.2):\\ny =\\n{0, if w ·x+b ≤0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='y =\\n{0, if w ·x+b ≤0\\n1, if w ·x+b > 0 (6.7)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.2 • T HE XOR PROBLEM 5\\nIt’s very easy to build a perceptron that can compute the logical AND and OR\\nfunctions of its binary inputs; Fig. 6.4 shows the necessary weights.\\nx1\\nx2\\n+1\\n-1\\n1\\n1\\nx1\\nx2\\n+1\\n0\\n1\\n1\\n(a) (b)\\nFigure 6.4 The weights w and bias b for perceptrons for computing logical functions. The\\ninputs are shown asx1 and x2 and the bias as a special node with value+1 which is multiplied\\nwith the bias weight b. (a) logical AND, with weights w1 = 1 and w2 = 1 and bias weight'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='b = −1. (b) logical OR, with weights w1 = 1 and w2 = 1 and bias weight b = 0. These\\nweights/biases are just one from an inﬁnite number of possible sets of weights and biases that\\nwould implement the functions.\\nIt turns out, however, that it’s not possible to build a perceptron to compute\\nlogical XOR! (It’s worth spending a moment to give it a try!)\\nThe intuition behind this important result relies on understanding that a percep-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='tron is a linear classiﬁer. For a two-dimensional input x1 and x2, the perceptron\\nequation, w1x1 +w2x2 +b = 0 is the equation of a line. (We can see this by putting\\nit in the standard linear format: x2 = (−w1/w2)x1 + (−b/w2).) This line acts as a\\ndecision boundary in two-dimensional space in which the output 0 is assigned to alldecision\\nboundary\\ninputs lying on one side of the line, and the output 1 to all input points lying on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='other side of the line. If we had more than 2 inputs, the decision boundary becomes\\na hyperplane instead of a line, but the idea is the same, separating the space into two\\ncategories.\\nFig. 6.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn\\nby one possible set of parameters for an AND and an OR classiﬁer. Notice that there\\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='from the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly\\nseparable\\nfunction. Of course we could draw a boundary with a curve, or some other function,\\nbut not a single line.\\n6.2.1 The solution: neural networks\\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\\nculated by a layered network of perceptron units. Rather than see this with networks\\nof simple perceptrons, however, let’s see how to compute XOR using two layers of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='ReLU-based units following Goodfellow et al. (2016). Fig. 6.6 shows a ﬁgure with\\nthe input being processed by two layers of neural units. The middle layer (called\\nh) has two units, and the output layer (called y) has one unit. A set of weights and\\nbiases are shown that allows the network to correctly compute the XOR function.\\nLet’s walk through what happens with the input x = [0, 0]. If we multiply each\\ninput value by the appropriate weight, sum, and then add the biasb, we get the vector'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='[0, -1], and we then apply the rectiﬁed linear transformation to give the output of the\\nh layer as [0, 0]. Now we once again multiply by the weights, sum, and add the\\nbias (0 in this case) resulting in the value 0. The reader should work through the\\ncomputation of the remaining 3 possible input pairs to see that the resultingy values\\nare 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6 CHAPTER 6 • N EURAL NETWORKS\\n0\\n0 1\\n1\\nx1\\nx2\\n0\\n0 1\\n1\\nx1\\nx2\\n0\\n0 1\\n1\\nx1\\nx2\\na)  x1 AND x2 b)  x1 OR x2 c)  x1 XOR x2\\n?\\nFigure 6.5 The functions AND, OR, and XOR, represented with input x1 on the x-axis and input x2 on the\\ny-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\\n(2002).\\nx1\\nx2\\nh1\\nh2\\ny1\\n+1\\n1\\n-1\\n1\\n1\\n1\\n-2\\n0\\n1\\n+1\\n0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='(2002).\\nx1\\nx2\\nh1\\nh2\\ny1\\n+1\\n1\\n-1\\n1\\n1\\n1\\n-2\\n0\\n1\\n+1\\n0\\nFigure 6.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in\\ntwo layers; we’ve called them h1, h2 (h for “hidden layer”) and y1. As before, the numbers\\non the arrows represent the weights w for each unit, and we represent the bias b as a weight\\non a unit clamped to +1, with the bias weights/units in gray.\\nIt’s also instructive to look at the intermediate results, the outputs of the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='hidden nodes h1 and h2. We showed in the previous paragraph that the h vector for\\nthe inputs x = [0, 0] was [0, 0]. Fig. 6.7b shows the values of the h layer for all\\n4 inputs. Notice that hidden representations of the two input points x = [0, 1] and\\nx = [1, 0] (the two cases with XOR output = 1) are merged to the single point h =\\n[1, 0]. The merger makes it easy to linearly separate the positive and negative cases\\nof XOR. In other words, we can view the hidden layer of the network as forming a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='representation of the input.\\nIn this example we just stipulated the weights in Fig. 6.6. But for real examples\\nthe weights for neural networks are learned automatically using the error backprop-\\nagation algorithm to be introduced in Section 6.6. That means the hidden layers will\\nlearn to form useful representations. This intuition, that neural networks can auto-\\nmatically learn useful representations of the input, is one of their key advantages,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='and one that we will return to again and again in later chapters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.3 • F EEDFORWARD NEURAL NETWORKS 7\\n0\\n0 1\\n1\\nx1\\nx2\\na) The original x space\\n0\\n0 1\\n1\\nh1\\nh2\\n2\\nb) The new (linearly separable) h space\\nFigure 6.7 The hidden layer forming a new representation of the input. (b) shows the\\nrepresentation of the hidden layer, h, compared to the original input representation x in (a).\\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\\npossible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='(2016).\\n6.3 Feedforward Neural Networks\\nLet’s now walk through a slightly more formal presentation of the simplest kind of\\nneural network, the feedforward network. A feedforward network is a multilayerfeedforward\\nnetwork\\nnetwork in which the units are connected with no cycles; the outputs from units in\\neach layer are passed to units in the next higher layer, and no outputs are passed\\nback to lower layers. (In Chapter 13 we’ll introduce networks with cycles, called\\nrecurrent neural networks.)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='recurrent neural networks.)\\nFor historical reasons multilayer networks, especially feedforward networks, are\\nsometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer,multi-layer\\nperceptrons\\nMLP since the units in modern multilayer networks aren’t perceptrons (perceptrons have a\\nsimple step-function as their activation function, but modern networks are made up\\nof units with many kinds of non-linearities like ReLUs and sigmoids), but at some\\npoint the name stuck.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='point the name stuck.\\nSimple feedforward networks have three kinds of nodes: input units, hidden\\nunits, and output units.\\nFig. 6.8 shows a picture. The input layerx is a vector of simple scalar values just\\nas we saw in Fig. 6.2.\\nThe core of the neural network is thehidden layer h formed of hidden units hi,hidden layer\\neach of which is a neural unit as described in Section 6.1, taking a weighted sum of\\nits inputs and then applying a non-linearity. In the standard architecture, each layer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='is fully-connected, meaning that each unit in each layer takes as input the outputsfully-connected\\nfrom all the units in the previous layer, and there is a link between every pair of units\\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.\\nRecall that a single hidden unit has as parameters a weight vector and a bias. We\\nrepresent the parameters for the entire hidden layer by combining the weight vector'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='and bias for each unit i into a single weight matrix W and a single bias vector b for\\nthe whole layer (see Fig. 6.8). Each element Wji of the weight matrix W represents\\nthe weight of the connection from the ith input unit xi to the jth hidden unit hj.\\nThe advantage of using a single matrix W for the weights of the entire layer is\\nthat now the hidden layer computation for a feedforward network can be done very'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='8 CHAPTER 6 • N EURAL NETWORKS\\nx1\\nx2\\nxn0\\n…\\n…\\n+1 b\\n…\\nUW\\ninput layer hidden layer output layer\\nh1\\ny1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\nFigure 6.8 A simple 2-layer feedforward network, with one hidden layer, one output layer,\\nand one input layer (the input layer is usually not counted when enumerating layers).\\nefﬁciently with simple matrix operations. In fact, the computation only has three\\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='and applying the activation functiong (such as the sigmoid, tanh, or ReLU activation\\nfunction deﬁned above).\\nThe output of the hidden layer, the vectorh, is thus the following (for this exam-\\nple we’ll use the sigmoid functionσ as our activation function):\\nh = σ(Wx+b) (6.8)\\nNotice that we’re applying the σ function here to a vector, while in Eq. 6.3 it was\\napplied to a scalar. We’re thus allowing σ(·), and indeed any activation function'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='g(·), to apply to a vector element-wise, so g[z1,z2,z3] = [g(z1),g(z2),g(z3)].\\nLet’s introduce some constants to represent the dimensionalities of these vectors\\nand matrices. We’ll refer to the input layer as layer 0 of the network, and have\\nn0 represent the number of inputs, so x is a vector of real numbers of dimension\\nn0, or more formally x ∈Rn0 , a column vector of dimensionality [n0 ×1]. Let’s\\ncall the hidden layer layer 1 and the output layer layer 2. The hidden layer has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='dimensionality n1, so h ∈Rn1 and also b ∈Rn1 (since each hidden unit can take a\\ndifferent bias value). And the weight matrix W has dimensionality W ∈Rn1×n0 , i.e.\\n[n1 ×n0].\\nTake a moment to convince yourself that the matrix multiplication in Eq. 6.8 will\\ncompute the value of each hj as σ\\n(∑n0\\ni=1 Wjixi +bj\\n)\\n.\\nAs we saw in Section 6.2, the resulting value h (for hidden but also for hypoth-\\nesis) forms a representation of the input. The role of the output layer is to take'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='this new representation h and compute a ﬁnal output. This output could be a real-\\nvalued number, but in many cases the goal of the network is to make some sort of\\nclassiﬁcation decision, and so we will focus on the case of classiﬁcation.\\nIf we are doing a binary task like sentiment classiﬁcation, we might have a sin-\\ngle output node, and its scalar value y is the probability of positive versus negative\\nsentiment. If we are doing multinomial classiﬁcation, such as assigning a part-of-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='speech tag, we might have one output node for each potential part-of-speech, whose\\noutput value is the probability of that part-of-speech, and the values of all the output\\nnodes must sum to one. The output layer is thus a vector y that gives a probability\\ndistribution across the output nodes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.3 • F EEDFORWARD NEURAL NETWORKS 9\\nLet’s see how this happens. Like the hidden layer, the output layer has a weight\\nmatrix (let’s call it U), but some models don’t include a bias vector b in the output\\nlayer, so we’ll simplify by eliminating the bias vector in this example. The weight\\nmatrix is multiplied by its input vector (h) to produce the intermediate output z:\\nz = Uh\\nThere are n2 output nodes, so z ∈Rn2 , weight matrix U has dimensionality U ∈'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='Rn2×n1 , and element Ui j is the weight from unit j in the hidden layer to unit i in the\\noutput layer.\\nHowever, z can’t be the output of the classiﬁer, since it’s a vector of real-valued\\nnumbers, while what we need for classiﬁcation is a vector of probabilities. There is\\na convenient function for normalizing a vector of real values, by which we meannormalizing\\nconverting it to a vector that encodes a probability distribution (all the numbers lie'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='between 0 and 1 and sum to 1): the softmax function that we saw on page ?? ofsoftmax\\nChapter 4. More generally for any vector z of dimensionality d, the softmax is\\ndeﬁned as:\\nsoftmax(zi) = exp(zi)∑d\\nj=1 exp(zj)\\n1 ≤i ≤d (6.9)\\nThus for example given a vector\\nz = [0.6,1.1,−1.5,1.2,3.2,−1.1], (6.10)\\nthe softmax function will normalize it to a probability distribution (shown rounded):\\nsoftmax(z) = [0.055,0.090,0.0067,0.10,0.74,0.010] (6.11)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='You may recall that we used softmax to create a probability distribution from a\\nvector of real-valued numbers (computed from summing weights times features) in\\nthe multinomial version of logistic regression in Chapter 4.\\nThat means we can think of a neural network classiﬁer with one hidden layer\\nas building a vector h which is a hidden layer representation of the input, and then\\nrunning standard multinomial logistic regression on the features that the network'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='develops in h. By contrast, in Chapter 4 the features were mainly designed by hand\\nvia feature templates. So a neural network is like multinomial logistic regression,\\nbut (a) with many layers, since a deep neural network is like layer after layer of lo-\\ngistic regression classiﬁers; (b) with those intermediate layers having many possible\\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='continue to use σ for convenience to mean any activation function); (c) rather than\\nforming the features by feature templates, the prior layers of the network induce the\\nfeature representations themselves.\\nHere are the ﬁnal equations for a feedforward network with a single hidden layer,\\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-\\nized by weight matrices W and U and a bias vector b:\\nh = σ(Wx+b)\\nz = Uh\\ny = softmax(z) (6.12)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='h = σ(Wx+b)\\nz = Uh\\ny = softmax(z) (6.12)\\nAnd just to remember the shapes of all our variables, x ∈Rn0 , h ∈Rn1 , b ∈Rn1 ,\\nW ∈Rn1×n0 , U ∈Rn2×n1 , and the output vectory ∈Rn2 . We’ll call this network a 2-\\nlayer network (we traditionally don’t count the input layer when numbering layers,\\nbut do count the output layer). So by this terminology logistic regression is a 1-layer\\nnetwork.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='10 CHAPTER 6 • N EURAL NETWORKS\\n6.3.1 More details on feedforward networks\\nLet’s now set up some notation to make it easier to talk about deeper networks of\\ndepth more than 2. We’ll use superscripts in square brackets to mean layer num-\\nbers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the\\n(ﬁrst) hidden layer, and b[1] will mean the bias vector for the (ﬁrst) hidden layer. nj\\nwill mean the number of units at layer j. We’ll use g(·) to stand for the activation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='function, which will tend to be ReLU or tanh for intermediate layers and softmax\\nfor output layers. We’ll usea[i] to mean the output from layer i, and z[i] to mean the\\ncombination of previous layer output, weights and biases W[i]a[i−1] + b[i]. The 0th\\nlayer is for inputs, so we’ll refer to the inputsx more generally as a[0].\\nThus we can re-represent our 2-layer net from Eq. 6.12 as follows:\\nz[1] = W[1]a[0] +b[1]\\na[1] = g[1](z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = g[2](z[2])\\nˆy = a[2] (6.13)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='a[2] = g[2](z[2])\\nˆy = a[2] (6.13)\\nNote that with this notation, the equations for the computation done at each layer are\\nthe same. The algorithm for computing the forward step in an n-layer feedforward\\nnetwork, given the input vector a[0] is thus simply:\\nfor i in 1,...,n\\nz[i] = W[i] a[i−1] + b[i]\\na[i] = g[i](z[i])\\nˆy = a[n]\\nIt’s often useful to have a name for the ﬁnal set of activations right before the ﬁnal\\nsoftmax. So however many layers we have, we’ll generally call the unnormalized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='values in the ﬁnal vector z[n], the vector of scores right before the ﬁnal softmax, the\\nlogits (see Eq. ??).logits\\nThe need for non-linear activation functions One of the reasons we use non-\\nlinear activation functions for each layer in a neural network is that if we did not, the\\nresulting network is exactly equivalent to a single-layer network. Let’s see why this\\nis true. Imagine the ﬁrst two layers of such a network of purely linear layers:\\nz[1] = W[1]x+b[1]\\nz[2] = W[2]z[1] +b[2]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='z[1] = W[1]x+b[1]\\nz[2] = W[2]z[1] +b[2]\\nWe can rewrite the function that the network is computing as:\\nz[2] = W[2]z[1] +b[2]\\n= W[2](W[1]x+b[1])+ b[2]\\n= W[2]W[1]x+W[2]b[1] +b[2]\\n= W′x+b′ (6.14)\\nThis generalizes to any number of layers. So without non-linear activation functions,\\na multilayer network is just a notational variant of a single layer network with a\\ndifferent set of weights, and we lose all the representational power of multilayer\\nnetworks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.4 • F EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION 11\\nReplacing the bias unit In describing networks, we will sometimes use a slightly\\nsimpliﬁed notation that represents exactly the same function without referring to an\\nexplicit bias node b. Instead, we add a dummy node a0 to each layer whose value\\nwill always be 1. Thus layer 0, the input layer, will have a dummy node a[0]\\n0 = 1,\\nlayer 1 will havea[1]\\n0 = 1, and so on. This dummy node still has an associated weight,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='and that weight represents the bias value b. For example instead of an equation like\\nh = σ(Wx+b) (6.15)\\nwe’ll use:\\nh = σ(Wx) (6.16)\\nBut now instead of our vector x having n0 values: x = x1,..., xn0 , it will have n0 +\\n1 values, with a new 0th dummy value x0 = 1: x = x0,..., xn0 . And instead of\\ncomputing each hj as follows:\\nhj = σ\\n(n0∑\\ni=1\\nWji xi +bj\\n)\\n, (6.17)\\nwe’ll instead use:\\nhj = σ\\n(n0∑\\ni=0\\nWji xi\\n)\\n, (6.18)\\nwhere the value Wj0 replaces what had been bj. Fig. 6.9 shows a visualization.\\nx1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='x1\\nx2\\nxn0\\n…\\n…\\n+1 b\\n…\\nUW h1 y1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\nx1\\nx2\\nxn0\\n…\\n…\\nx0=1\\n…\\nUW\\nh1 y1\\ny2\\nyn2\\nh2\\nh3\\nhn1\\n(a) (b)\\nFigure 6.9 Replacing the bias node (shown in a) with x0 (b).\\nWe’ll continue showing the bias as b when we go over the learning algorithm\\nin Section 6.6, but going forward in the book, for most ﬁgures and some equations\\nwe’ll use this simpliﬁed notation without explicit bias terms.\\n6.4 Feedforward networks for NLP: Classiﬁcation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.4 Feedforward networks for NLP: Classiﬁcation\\nLet’s see how to apply feedforward networks to NLP classiﬁcation tasks. In practice,\\nsimple feedforward networks aren’t the way we do text classiﬁcation; for real appli-\\ncations we would use more sophisticated architectures like the BERT transformers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='12 CHAPTER 6 • N EURAL NETWORKS\\nof Chapter 10. Nonetheless seeing a feedforward network text classiﬁer will let us\\nintroduce key ideas that will play a role throughout the rest of the book, includ-\\ning the ideas of theembedding matrix, representation pooling, and representation\\nlearning.\\nBut before introducing any of these ideas, let’s start with a classiﬁer by making\\nonly minimal change from the sentiment classiﬁers we saw in Chapter 4. Like them,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='we’ll take hand-built features, pass them through a classiﬁer, and produce a class\\nprobability. The only difference is that we’ll use a neural network instead of logistic\\nregression as the classiﬁer.\\n6.4.1 Neural net classiﬁers with hand-built features\\nLet’s begin with a simple 2-layer sentiment classiﬁer by taking our logistic regres-\\nsion classiﬁer from Chapter 4, which corresponds to a 1-layer network, and just\\nadding a hidden layer. The input element xi can be scalar features like those in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='Fig. ??, e.g., x1 = count(words ∈doc), x2 = count(positive lexicon words ∈doc),\\nx3 = 1 if “no” ∈doc, and so on, for a total of d features. And the output layer\\nˆy could have two nodes (one each for positive and negative), or 3 nodes (positive,\\nnegative, neutral), in which case ˆy1 would be the estimated probability of positive\\nsentiment, ˆy2 the probability of negative and ˆy3 the probability of neutral. The re-\\nsulting equations would be just what we saw above for a 2-layer network (as always,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='we’ll continue to use the σ to stand for any non-linearity, whether sigmoid, ReLU\\nor other).\\nx = [x1,x2,...xd] (each xi is a hand-designed feature)\\nh = σ(Wx+b)\\nz = Uh\\nˆy = softmax(z) (6.19)\\nFig. 6.10 shows a sketch of this architecture. As we mentioned earlier, adding this\\nhidden layer to our logistic regression classiﬁer allows the network to represent the\\nnon-linear interactions between features. This alone might give us a better sentiment\\nclassiﬁer.\\nUW\\n[d⨉1]\\nHidden layer Output layer\\nsoftmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='UW\\n[d⨉1]\\nHidden layer Output layer\\nsoftmax\\n[dh⨉d] [dh⨉1] [3⨉dh]\\nInput words\\np(+)\\nh1\\nh2\\nh3\\nhdh …\\ny1^\\ny2^\\ny3^\\nx h y\\nInput layer \\nd=3 features\\n[3⨉1]\\nx1\\nx2\\nx3\\ndessert\\nwas\\ngreat\\npositive lexicon\\nwords = 1\\ncount of “no” \\n= 0\\nwordcount\\n=3\\np(-)\\np(neut)\\nFigure 6.10 Feedforward network sentiment analysis using traditional hand-built features\\nof the input text.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 13\\n6.4.2 Vectorizing for parallelizing inference\\nWhile Eq. 6.19 shows how to classify a single example x, in practice we want to\\nefﬁciently classify an entire test set of m examples. We do this by vectorizing the\\nprocess, just as we saw with logistic regression; instead of using for-loops to go\\nthrough each example, we’ll use matrix multiplication to do the entire computation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='of an entire test set at once. First, we pack all the input feature vectors for each input\\nx into a single input matrixX, with each rowi a row vector consisting of the features\\nfor input example x(i) (i.e., the vector x(i)). If the dimensionality of our input feature\\nvector is d, X will be a matrix of shape [m ×d].\\nBecause we are now modeling each input as a row vector rather than a column\\nvector, we also need to slightly modify Eq. 6.19. X is of shape [m ×d] and W is of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='shape [dh ×d], so we’ll reorder how we multiplyX and W and transpose W so they\\ncorrectly multiply to yield a matrix H of shape [m ×dh]. 1\\nThe bias vector b from Eq. 6.19 of shape [1 ×dh] will now have to be replicated\\ninto a matrix of shape [m ×dh]. We’ll need to similarly reorder the next step and\\ntranspose U. Finally, our output matrix ˆY will be of shape [m ×3] (or more gen-\\nerally [m ×do], where do is the number of output classes), with each row i of our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='output matrix ˆY consisting of the output vector ˆy(i). Here are the ﬁnal equations for\\ncomputing the output class distribution for an entire test set:\\nH = σ(XW⊺ +b)\\nZ = HU⊺\\nˆY = softmax(Z) (6.20)\\nIn this book, we’ll sometimes see orderings like WX + b and sometimes XW + b.\\nThat’s why it’s always important to be very aware of the shapes of your weight\\nmatrices participating in any given equation.\\n6.5 Embeddings as the input to neural net classiﬁers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='While hand-built features are a traditional way to design classiﬁers, most applica-\\ntions of neural networks for NLP don’t use hand-built human-engineered features as\\ninputs. Instead, we draw on deep learning’s ability to learn features from the data by\\nrepresenting tokens as embeddings. For this section we’ll represent each token by\\nits static word2vec or GloVe embeddings that we saw how to compute in Chapter 5.\\nBy static embedding, we mean that each token is represented by a ﬁxed vector that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='we train once, and then just put into a big dictionary. When we want to refer to that\\ntoken, we grab its embedding out of the dictionary.\\nHowever when we apply neural models to the task of language modeling (as\\nwe’ll see in Chapter 8) the situation is more complex, and we’ll use a more power-\\nful kind of embedding called a contextual embedding. Contextual embeddings are\\ndifferent for each time a word occurs in a different context. Furthermore, we’ll have'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='the network learn these embeddings as part of the task of word prediction.\\nSo let’s explore the text classiﬁcation domain above, but using static embeddings\\nas features instead of the hand-designed features. Let’s focus on the inference stage,\\n1 Note that we could have kept the original order of our products if we had instead made our input\\nmatrix X represent each input as a column vector instead of a row vector, making it of shape[d ×m]. But'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='representing inputs as row vectors is convenient and common in neural network models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='14 CHAPTER 6 • N EURAL NETWORKS\\nin which we have already learned embeddings for all the input tokens. An embed-\\nding is a vector of dimension d that represents the input token. The dictionary of\\nstatic embeddings in which we store these embeddings is the embedding matrixembedding\\nmatrix\\nE. Each row of the embedding matrix represents each token of the vocabulary V\\nas a (row) vector of dimensionality d. Since E has a row for each of the |V |to-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='kens in the vocabulary, E has shape [|V |×d]. This embedding matrix E plays a role\\nwhenever we are using embeddings as input to neural NLP systems, including in the\\ntransformer-based large language models we will introduce over the next chapters.\\nGiven an input token string likedessert was great we ﬁrst convert the tokens\\ninto vocabulary indices (these were created when we ﬁrst tokenized the input using\\nBPE or SentencePiece). So the representation of dessert was great might be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='w = [3,9824,226]. Next we use indexing to select the corresponding rows from E\\n(row 3, row 4000, row 10532).\\nAnother way to think about selecting token embeddings from the embedding\\nmatrix is to represent input tokens as one-hot vectors of shape [1 ×|V |], i.e., with\\none dimension for each word in the vocabulary. Recall that in a one-hot vector allone-hot vector\\nthe elements are 0 except one, the element whose dimension is the word’s index'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='in the vocabulary, which has value 1. So if the word “dessert” has index 3 in the\\nvocabulary, x3 = 1, and xi = 0 ∀i ̸= 3, as shown here:\\n[0 0 1 0 0 0 0 ... 0 0 0 0]\\n1 2 3 4 5 6 7 ... ... |V|\\nMultiplying by a one-hot vector that has only one non-zero elementxi = 1 simply\\nselects out the relevant row vector for wordi, resulting in the embedding for word i,\\nas depicted in Fig. 6.11.\\nE\\n|V|\\nd\\n1\\n|V| d\\n=✕\\n33\\n0 0 1 0 0 0 0 … 0 0 0 0 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='E\\n|V|\\nd\\n1\\n|V| d\\n=✕\\n33\\n0 0 1 0 0 0 0 … 0 0 0 0 1\\nFigure 6.11 Selecting the embedding vector for word V3 by multiplying the embedding\\nmatrix E with a one-hot vector with a 1 in index 3.\\nWe can extend this idea to represent the entire input token sequence as a matrix\\nof one-hot vectors, one for each of the N input positions as shown in Fig. 6.12.\\nE\\n|V|\\nd\\nd\\nN\\n=✕\\n|V|\\nN\\n0 0 0 0 0 0 0 … 0 0 1 0 \\n0 0 1 0 0 0 0 … 0 0 0 0 \\n1 0 0 0 0 0 0 … 0 0 0 0 \\n0 0 0 0 1 0 0 … 0 0 0 0 \\n…'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='0 0 0 0 1 0 0 … 0 0 0 0 \\n…\\nFigure 6.12 Selecting the embedding matrix for the input sequence of token idsW by mul-\\ntiplying a one-hot matrix corresponding to W by the embedding matrix E.\\nWe now need to classify this input ofN [1 ×d] embeddings, representing a win-\\ndow of N tokens, into a single class (like positive or negative).\\nThere are two common ways to to pass embeddings to a classiﬁer: concate-\\nnation and pooling. First, we can take this input of shape [N ×d] and reshape it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 15\\nby concatenating all the input vectors into one very long vector of shape [1 ×dN].\\nThen we pass this input to our classiﬁer and let it make its decision. This gives\\nus lots of information, at the cost of using a pretty large network. Second, we can\\npool the N embeddings into a single embedding and then pass that single pooledpool\\nembedding to the classiﬁer. Pooling gives us less information than would have been'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='present in all the original embeddings, but has the advantage of being small and ef-\\nﬁcient and is especially useful in tasks for which we don’t care as much about the\\noriginal word order. Let’s give an example of each: pooling for the sentiment task,\\nand concatenation for the language modeling task.\\nPooling input embeddings for sentiment So let’s begin with seeing how pooling\\ncan work for the sentiment classiﬁcation task. The intuition of pooling is that for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='sentiment, the exact position of the input (is some word like great the ﬁrst word?\\nthe second word?) is less important than the identity of the word itself.\\nA pooling function is a way to turn a set of embeddings into a single embedding.\\nFor example, for a text with N input words/tokens w1,..., wN , we want to turn\\nthe N row embeddings e(w1),..., e(wN ) (each of dimensionality d) into a single\\nembedding also of dimensionality d.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='embedding also of dimensionality d.\\nThere are various ways to pool. The simplest is mean-pooling: taking the meanmean-pooling\\nby summing the embeddings and then dividing by N:\\nxmean = 1\\nN\\nN∑\\ni=1\\ne(wi) (6.21)\\nHere are the equations for this classiﬁer assuming mean pooling:\\nx = mean(e(w1),e(w2),..., e(wn))\\nh = σ(xW +b)\\nz = hU\\nˆy = softmax(z) (6.22)\\nThe architecture is sketched in Fig. 6.13, where we also give the shapes for all the\\nrelevant matrices.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='relevant matrices.\\nThere are many other options for pooling, like max-pooling, in which case formax-pooling\\neach dimension we take the element-wise max over all the inputs. The element-wise\\nmax of a set of N vectors is a new vector whose kth element is the max of the kth\\nelements of all the N vectors.\\nConcatenating input embeddings for language modeling For sentiment analy-\\nsis we saw how to generate an output vector with probabilities over three classes:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='positive, negative, or neutral, given as input a window of N input tokens, by ﬁrst\\npooling those token embeddings into a single embedding vector.\\nNow let’s considerlanguage modeling: predicting upcoming words from prior\\nwords. In this task we are given the same window of N input tokens, but our task\\nnow is to predict the next token that should follow the window. We’ll sketch a\\nsimple feedforward neural language model, drawing on an algorithm ﬁrst introduced'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='by Bengio et al. (2003). The feedforward language model introduces many of the\\nimportant concepts of large language modeling that we will return to in Chapter 7\\nand Chapter 8.\\nNeural language models have many advantages over the n-gram language mod-\\nels of Chapter 3. Neural language models can handle much longer histories, can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='16 CHAPTER 6 • N EURAL NETWORKS\\n“dessert” = V3 “was” = V524 “great” = V902\\nembedding for “dessert”\\nembedding for “was”\\nembedding for “great”\\nU\\nW\\n[1⨉d]\\nHidden layer\\nOutput layer\\n[d⨉dh]\\n[1⨉dh]\\n[dh⨉3]\\nInput words\\np(+)\\nh1 h2 h3 hdh\\n…\\ny1\\n^ y2^ y3^\\nx\\nh\\ny\\nInput layer \\n[1⨉3]\\npooling+\\np(-) p(neut)\\nembeddings\\none-hot vectors\\ndessert was great\\nN⨉d\\n0 0 1 00\\n1 |V|3\\n0 0 1 00\\n1 |V|902\\n0 0 1 00\\n1 |V|524\\n0\\n0\\nE\\nN⨉|V|\\n|V|⨉dE E E matrix\\nshared across words\\nOutput probabilities\\nweights\\nweights\\nsoftmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='Output probabilities\\nweights\\nweights\\nsoftmax\\npooled embedding\\nFigure 6.13 Feedforward network sentiment analysis using a pooled embedding of the input words. At each\\ntimestep the network computes a d-dimensional embedding for each context word (by multiplying a one-hot\\nvector by the embedding matrix E), and pools the resulting N embeddings to get a single embedding that\\nrepresents the context window as the layer e.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='represents the context window as the layer e.\\ngeneralize better over contexts of similar words, and are far more accurate at word-\\nprediction. On the other hand, neural net language models are slower, more com-\\nplex, need vast amounts of energy to train, and are less interpretable than n-gram\\nmodels, so for some smaller tasks an n-gram language model is still the right tool.\\nA feedforward neural language model is a feedforward network that takes as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='input at time t a representation of some number of previous words (wt−1,wt−2, etc.)\\nand outputs a probability distribution over possible next words. Thus—like the n-\\ngram LM—the feedforward neural LM approximates the probability of a word given\\nthe entire prior context P(wt |w1:t−1) by approximating based on the N −1 previous\\nwords:\\nP(wt |w1,..., wt−1) ≈P(wt |wt−N+1,..., wt−1) (6.23)\\nIn the following examples we’ll use a 4-gram example, so we’ll show a neural net to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='estimate the probability P(wt = i|wt−3,wt−2,wt−1).\\nNeural language models represent words in this prior context by their embed-\\ndings, rather than just by their word identity as used in n-gram language models.\\nUsing embeddings allows neural language models to generalize better to unseen\\ndata. For example, suppose we’ve seen this sentence in training:\\nI have to make sure that the cat gets fed.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.5 • E MBEDDINGS AS THE INPUT TO NEURAL NET CLASSIFIERS 17\\nbut have never seen the words “gets fed” after the word “dog”. Our test set has the\\npreﬁx “I forgot to make sure that the dog gets”. What’s the next word? An n-gram\\nlanguage model will predict “fed” after “that the cat gets”, but not after “that the dog\\ngets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will\\nbe able to generalize from the “cat” context to assign a high enough probability to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='“fed” even after seeing “dog”.\\nh1 h2\\ny1\\nh3 hdh…\\n…\\nU\\nW\\ny34 y|V|\\nembedding layer e 1⨉Nd\\nhidden layer h\\noutput layer y\\nsoftmax\\n…\\n...\\nwt-1wt-2 wtwt-3\\nNd⨉dh\\n1⨉dh\\ndh⨉|V|\\n1⨉|V|\\nInput layer\\none-hot \\nvectors “for” = V35\\n0 0 1 00\\n1 |V|35\\n0 0 1 00\\n1 |V|451\\n0 0 1 00\\n1 |V|992\\n0 0\\n“all” = V992 “the” = V451\\nE\\nN⨉|V|\\nE is shared\\nacross words |V|⨉d\\n…\\np(wt=do|…)p(wt=aardvark|wt-3,wt-2,wt-1) p(wt=zebra|…)p(wt=fish|…)\\n… y42 y35102^^^ ^ ^\\nE E\\nfor all the ?thanksand… …'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='… y42 y35102^^^ ^ ^\\nE E\\nfor all the ?thanksand… …\\nFigure 6.14 Forward inference in a feedforward neural language model. At each timestep\\nt the network computes a d-dimensional embedding for each of the N = 3 context tokens (by\\nmultiplying a one-hot vector by the embedding matrix E), and concatenates the three to get\\nthe embedding e. This embedding e is multiplied by weight matrix W and then an activation\\nfunction is applied element-wise to produce the hidden layer h, which is then multiplied by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='another weight matrix U. A softmax layer predicts at each output node i the probability that\\nthe next word wt will be vocabulary wordVi. We show the context window sizeN as 3 just to\\nﬁt on the page, but in practice language modeling requires a much longer context.\\nThis prediction task requires an output vector that expresses |V |probabilities:\\none probability value for each possible next token. We might have a vocabulary'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='between 60,000 and 300,000 tokens, so the output vector for the task of language\\nmodeling is much longer than 3. Another difference for language modeling is that\\ninstead of pooling the embeddings of the N input tokens to create a single embed-\\nding, we concatenate the inputs into one very long input vector. To predict the next\\ntoken, it helps to know each of the preceding tokens and what order they were in.\\nFig. 6.14 shows the language modeling task, sketched with a very short context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='window of N = 3 just to ﬁt on the page. These 3 embedding vectors are concatenated\\nto produce e, the embedding layer. This is multiplied by a weight matrix W to pro-\\nduce a hidden layer, and another weight matrix U to produce an output layer whose\\nsoftmax gives a probability distribution over words. For example y42, the value of\\noutput node 42, is the probability of the next wordwt being V42, the vocabulary word\\nwith index 42 (which is the word ‘ﬁsh’ in our example).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='The equations for a simple feedforward neural language model with a window'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='18 CHAPTER 6 • N EURAL NETWORKS\\nsize of 3, given one-hot input vectors for each input context word, are:\\ne = [Ext−3;Ext−2;Ext−1]\\nh = σ(We+b)\\nz = Uh\\nˆy = softmax(z) (6.24)\\nNote that we we use semicolons to mean concatenation of vectors, so we form the\\nembedding layer e by concatenating the 3 embeddings for the three context vectors.\\nWe’ll return to this idea of using neural networks to do language modeling in\\nChapter 7 and Chapter 8 when we introduce transformer language models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.6 Training Neural Nets\\nA feedforward neural net is an instance of supervised machine learning in which we\\nknow the correct output y for each observation x. What the system produces, via\\nEq. 6.13, is ˆy, the system’s estimate of the truey. The goal of the training procedure\\nis to learn parameters W[i] and b[i] for each layer i that make ˆy for each training\\nobservation as close as possible to the true y.\\nIn general, we do all this by drawing on the methods we introduced in Chapter 4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='for logistic regression, so the reader should be comfortable with that chapter before\\nproceeding. We’ll explore the algorithm on simple generic networks rather than\\nnetworks designed for sentiment or language modeling.\\nFirst, we’ll need a loss function that models the distance between the system\\noutput and the gold output, and it’s common to use the loss function used for logistic\\nregression, the cross-entropy loss.\\nSecond, to ﬁnd the parameters that minimize this loss function, we’ll use the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='gradient descent optimization algorithm introduced in Chapter 4.\\nThird, gradient descent requires knowing the gradient of the loss function, the\\nvector that contains the partial derivative of the loss function with respect to each\\nof the parameters. In logistic regression, for each observation we could directly\\ncompute the derivative of the loss function with respect to an individual w or b. But\\nfor neural networks, with millions of parameters in many layers, it’s much harder to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='see how to compute the partial derivative of some weight in layer 1 when the loss\\nis attached to some much later layer. How do we partial out the loss over all those\\nintermediate layers? The answer is the algorithm called error backpropagation or\\nbackward differentiation.\\n6.6.1 Loss function\\nThe cross-entropy loss that is used in neural networks is the same one we saw forcross-entropy\\nloss\\nlogistic regression. If the neural network is being used as a binary classiﬁer, with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='the sigmoid at the ﬁnal layer, the loss function is the same logistic regression loss\\nwe saw in Eq. ??:\\nLCE (ˆy,y) =−log p(y|x) = −[ylog ˆy +(1 −y)log(1 −ˆy)] (6.25)\\nIf we are using the network to classify into 3 or more classes, the loss function is\\nexactly the same as the loss for multinomial regression that we saw in Chapter 4 on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 19\\npage ??. Let’s brieﬂy summarize the explanation here for convenience. First, when\\nwe have more than 2 classes we’ll need to represent both y and ˆy as vectors. Let’s\\nassume we’re doing hard classiﬁcation , where only one class is the correct one.\\nThe true label y is then a vector with K elements, each corresponding to a class,\\nwith yc = 1 if the correct class is c, with all other elements of y being 0. Recall that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='a vector like this, with one value equal to 1 and the rest 0, is called aone-hot vector.\\nAnd our classiﬁer will produce an estimate vector with K elements ˆy, each element\\nˆyk of which represents the estimated probability p(yk = 1|x).\\nThe loss function for a single example x is the negative sum of the logs of the K\\noutput classes, each weighted by their probability yk:\\nLCE (ˆy,y) =−\\nK∑\\nk=1\\nyk log ˆyk (6.26)\\nWe can simplify this equation further; let’s ﬁrst rewrite the equation using the func-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='tion 1 {}which evaluates to 1 if the condition in the brackets is true and to 0 oth-\\nerwise. This makes it more obvious that the terms in the sum in Eq. 6.26 will be 0\\nexcept for the term corresponding to the true class for which yk = 1:\\nLCE (ˆy,y) = −\\nK∑\\nk=1\\n1 {yk = 1}log ˆyk\\nIn other words, the cross-entropy loss is simply the negative log of the output proba-\\nbility corresponding to the correct class, and we therefore also call this the negative\\nlog likelihood loss:negative log'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='log likelihood loss:negative log\\nlikelihood loss\\nLCE (ˆy,y) = −log ˆyc (where c is the correct class) (6.27)\\nPlugging in the softmax formula from Eq. 6.9, and with K the number of classes:\\nLCE (ˆy,y) = −log exp(zc)∑K\\nj=1 exp(zj)\\n(where c is the correct class) (6.28)\\nLet’s think about the negative log probability as a loss function. A perfect clas-\\nsiﬁer would assign the correct class i probability 1 and all the incorrect classes prob-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='ability 0. That means the higher p(ˆyi) (the closer it is to 1), the better the classiﬁer;\\np(ˆyi) is (the closer it is to 0), the worse the classiﬁer. The negative log of this prob-\\nability is a beautiful loss metric since it goes from 0 (negative log of 1, no loss)\\nto inﬁnity (negative log of 0, inﬁnite loss). This loss function also insures that as\\nprobability of the correct answer is maximized, the probability of all the incorrect'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='answers is minimized; since they all sum to one, any increase in the probability of\\nthe correct answer is coming at the expense of the incorrect answers.\\nThe number K of classes of the output vector ˆy can be small or large. Perhaps\\nour task is 3-way sentiment, and then the classes might be positive, negative, and\\nneutral. Or if our task is deciding the part of speech of a word (i.e., whether it is a\\nnoun or verb or adjective, etc.), thenK is set of possible parts of speech in our tagset'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='(of which there are 17 in the tagset we will deﬁne in Chapter 17). And if our task\\nis language modeling, and our classiﬁer is trying to predict which word is next, then\\nour set of classes is the set of words, which might be 50,000 or 100,000.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='20 CHAPTER 6 • N EURAL NETWORKS\\n6.6.2 Computing the Gradient\\nHow do we compute the gradient of this loss function? Computing the gradient\\nrequires the partial derivative of the loss function with respect to each parameter.\\nFor a network with one weight layer and sigmoid output (which is what logistic\\nregression is), we could simply use the derivative of the loss that we used for logistic\\nregression in Eq. 6.29 (and derived in Section ??):\\n∂LCE (ˆy,y)\\n∂wj\\n= ( ˆy −y)xj\\n= (σ(w ·x+b)−y)xj (6.29)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='∂wj\\n= ( ˆy −y)xj\\n= (σ(w ·x+b)−y)xj (6.29)\\nOr for a network with one weight layer and softmax output (=multinomial logistic\\nregression), we could use the derivative of the softmax loss from Eq. ??, shown for\\na particular weight wk and input xi\\n∂LCE(ˆy,y)\\n∂wk,i\\n= −(yk −ˆyk)xi\\n= −(yk −p(yk = 1|x))xi\\n= −\\n(\\nyk − exp(wk ·x+bk)∑K\\nj=1 exp(wj ·x+bj)\\n)\\nxi (6.30)\\nBut these derivatives only give correct updates for one weight layer: the last one!'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='For deep networks, computing the gradients for each weight is much more complex,\\nsince we are computing the derivative with respect to weight parameters that appear\\nall the way back in the very early layers of the network, even though the loss is\\ncomputed only at the very end of the network.\\nThe solution to computing this gradient is an algorithm called error backprop-\\nagation or backprop (Rumelhart et al., 1986). While backprop was invented spe-error back-\\npropagation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='propagation\\ncially for neural networks, it turns out to be the same as a more general procedure\\ncalled backward differentiation , which depends on the notion of computation\\ngraphs. Let’s see how that works in the next subsection.\\n6.6.3 Computation Graphs\\nA computation graph is a representation of the process of computing a mathematical\\nexpression, in which the computation is broken down into separate operations, each\\nof which is modeled as a node in a graph.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='of which is modeled as a node in a graph.\\nConsider computing the function L(a,b,c) =c(a +2b). If we make each of the\\ncomponent addition and multiplication operations explicit, and add names (d and e)\\nfor the intermediate outputs, the resulting series of computations is:\\nd = 2 ∗b\\ne = a +d\\nL = c ∗e\\nWe can now represent this as a graph, with nodes for each operation, and di-\\nrected edges showing the outputs from each operation as the inputs to the next, as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='in Fig. 6.15. The simplest use of computation graphs is to compute the value of\\nthe function with some given inputs. In the ﬁgure, we’ve assumed the inputs a = 3,\\nb = 1, c = −2, and we’ve shown the result of the forward pass to compute the re-\\nsult L(3,1,−2) =−10. In the forward pass of a computation graph, we apply each'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 21\\noperation left to right, passing the outputs of each computation as the input to the\\nnext node.\\ne=a+d\\nd = 2b L=ce\\na=3\\nb=1\\nc=-2\\ne=5d=2\\nL=-10\\nforward pass\\na\\nb\\nc\\nFigure 6.15 Computation graph for the functionL(a,b,c) =c(a+2b), with values for input\\nnodes a = 3, b = 1, c = −2, showing the forward pass computation of L.\\n6.6.4 Backward differentiation on computation graphs\\nThe importance of the computation graph comes from the backward pass, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='is used to compute the derivatives that we’ll need for the weight update. In this\\nexample our goal is to compute the derivative of the output function L with respect\\nto each of the input variables, i.e., ∂L\\n∂a , ∂L\\n∂b , and ∂L\\n∂c . The derivative ∂L\\n∂a tells us how\\nmuch a small change in a affects L.\\nBackwards differentiation makes use of the chain rule in calculus, so let’s re-chain rule\\nmind ourselves of that. Suppose we are computing the derivative of a composite'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='function f (x) =u(v(x)). The derivative of f (x) is the derivative ofu(x) with respect\\nto v(x) times the derivative of v(x) with respect to x:\\nd f\\ndx = du\\ndv ·dv\\ndx (6.31)\\nThe chain rule extends to more than two functions. If computing the derivative of a\\ncomposite function f (x) =u(v(w(x))), the derivative of f (x) is:\\nd f\\ndx = du\\ndv ·dv\\ndw ·dw\\ndx (6.32)\\nThe intuition of backward differentiation is to pass gradients back from the ﬁnal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='node to all the nodes in the graph. Fig. 6.16 shows part of the backward computation\\nat one node e. Each node takes an upstream gradient that is passed in from its parent\\nnode to the right, and for each of its inputs computes a local gradient (the gradient\\nof its output with respect to its input), and uses the chain rule to multiply these two\\nto compute a downstream gradient to be passed on to the next earlier node.\\nLet’s now compute the 3 derivatives we need. Since in the computation graph'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='L = ce, we can directly compute the derivative ∂L\\n∂c :\\n∂L\\n∂c = e (6.33)\\nFor the other two, we’ll need to use the chain rule:\\n∂L\\n∂a = ∂L\\n∂e\\n∂e\\n∂a\\n∂L\\n∂b = ∂L\\n∂e\\n∂e\\n∂d\\n∂d\\n∂b (6.34)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='22 CHAPTER 6 • N EURAL NETWORKS\\ned L\\ned\\n∂L\\n∂d\\n∂L\\n∂e= ∂e\\n∂d\\n∂L\\n∂e\\n∂e\\n∂d\\nupstream\\n gradientdownstream\\n gradient\\nlocal\\n gradient\\nFigure 6.16 Each node (like e here) takes an upstream gradient, multiplies it by the local\\ngradient (the gradient of its output with respect to its input), and uses the chain rule to compute\\na downstream gradient to be passed on to a prior node. A node may have multiple local\\ngradients if it has multiple inputs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='gradients if it has multiple inputs.\\nEq. 6.34 and Eq. 6.33 thus require ﬁve intermediate derivatives: ∂L\\n∂e , ∂L\\n∂c , ∂e\\n∂a , ∂e\\n∂d , and\\n∂d\\n∂b , which are as follows (making use of the fact that the derivative of a sum is the\\nsum of the derivatives):\\nL = ce : ∂L\\n∂e = c, ∂L\\n∂c = e\\ne = a +d : ∂e\\n∂a = 1, ∂e\\n∂d = 1\\nd = 2b : ∂d\\n∂b = 2\\nIn the backward pass, we compute each of these partials along each edge of the\\ngraph from right to left, using the chain rule just as we did above. Thus we begin by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='computing the downstream gradients from nodeL, which are ∂L\\n∂e and ∂L\\n∂c . For node e,\\nwe then multiply this upstream gradient ∂L\\n∂e by the local gradient (the gradient of the\\noutput with respect to the input), ∂e\\n∂d to get the output we send back to node d: ∂L\\n∂d .\\nAnd so on, until we have annotated the graph all the way to all the input variables.\\nThe forward pass conveniently already will have computed the values of the forward'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='intermediate variables we need (liked and e) to compute these derivatives. Fig. 6.17\\nshows the backward pass.\\ne=d+a\\nd = 2b L=ce\\na=3\\nb=1\\ne=5d=2\\nL=-10\\n \\na\\nb\\nc ∂L=5∂c\\n∂L =-2∂e\\n∂e =1∂d\\n∂d =2∂b\\n∂e =1∂a\\nbackward pass\\nc=-2\\n∂L =-2∂e\\n∂L =5∂c\\n∂L\\n∂d =-2∂e\\n∂d\\n∂L\\n∂e=\\n∂L\\n∂a =-2∂e\\n∂a\\n∂L\\n∂e=\\n∂L\\n∂b =-4∂d\\n∂b\\n∂L\\n∂d=\\nFigure 6.17 Computation graph for the function L(a,b,c) =c(a +2b), showing the backward pass computa-\\ntion of ∂L\\n∂a , ∂L\\n∂b , and ∂L\\n∂c .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.6 • T RAINING NEURAL NETS 23\\nBackward differentiation for a neural network\\nOf course computation graphs for real neural networks are much more complex.\\nFig. 6.18 shows a sample computation graph for a 2-layer neural network with n0 =\\n2, n1 = 2, and n2 = 1, assuming binary classiﬁcation and hence using a sigmoid\\noutput unit for simplicity. The function that the computation graph is computing is:\\nz[1] = W[1]x+b[1]\\na[1] = ReLU(z[1])\\nz[2] = W[2]a[1] +b[2]\\na[2] = σ(z[2])\\nˆy = a[2] (6.35)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='a[2] = σ(z[2])\\nˆy = a[2] (6.35)\\nFor the backward pass we’ll also need to compute the loss L. The loss function\\nfor binary sigmoid output from Eq. 6.25 is\\nLCE (ˆy,y) = −[ylog ˆy +(1 −y)log(1 −ˆy)] (6.36)\\nOur output ˆy = a[2], so we can rephrase this as\\nLCE (a[2],y) = −\\n[\\nyloga[2] +(1 −y)log(1 −a[2])\\n]\\n(6.37)\\nz[2] = \\n+ a[2] = σ\\n \\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1] *\\n*\\n*\\n*\\nx1\\nx2\\na[1] = \\nReLU\\nz[1] = \\n+\\nb[1]\\n*\\n*\\nw[2]\\n11\\nw[1]\\n11\\nw[1]\\n12\\nw[1]\\n21\\nw[1]\\n22 b[2]\\nw[2]\\n12\\nL (a[2],y)1\\n2\\n1\\n1 1\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='21\\nw[1]\\n22 b[2]\\nw[2]\\n12\\nL (a[2],y)1\\n2\\n1\\n1 1\\n22\\nFigure 6.18 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units\\nand 2 hidden units. We’ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning\\nthe function that is being computed, and the resulting variable name. Thus the * to the right of node w[1]\\n11 means\\nthat w[1]\\n11 is to be multiplied by x1, and the node z[1] = +means that the value of z[1] is computed by summing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='the three nodes that feed into it (the two products, and the bias term b[1]\\ni ).\\nThe weights that need updating (those for which we need to know the partial\\nderivative of the loss function) are shown in teal. In order to do the backward pass,\\nwe’ll need to know the derivatives of all the functions in the graph. We already saw\\nin Section ?? the derivative of the sigmoid σ:\\ndσ(z)\\ndz = σ(z)(1 −σ(z)) (6.38)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='24 CHAPTER 6 • N EURAL NETWORKS\\nWe’ll also need the derivatives of each of the other activation functions. The\\nderivative of tanh is:\\nd tanh(z)\\ndz = 1 −tanh2(z) (6.39)\\nThe derivative of the ReLU is2\\nd ReLU(z)\\ndz =\\n{0 f or z < 0\\n1 f or z ≥0 (6.40)\\nWe’ll give the start of the computation, computing the derivative of the loss function\\nL with respect to z, or ∂L\\n∂z (and leaving the rest of the computation as an exercise for\\nthe reader). By the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z (6.41)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z (6.41)\\nSo let’s ﬁrst compute ∂L\\n∂a[2] , taking the derivative of Eq. 6.37, repeated here:\\nLCE (a[2],y) = −\\n[\\nyloga[2] +(1 −y)log(1 −a[2])\\n]\\n∂L\\n∂a[2] = −\\n((\\ny∂ log(a[2])\\n∂a[2]\\n)\\n+(1 −y)∂ log(1 −a[2])\\n∂a[2]\\n)\\n= −\\n((\\ny 1\\na[2]\\n)\\n+(1 −y) 1\\n1 −a[2] (−1)\\n)\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\n(6.42)\\nNext, by the derivative of the sigmoid:\\n∂a[2]\\n∂z = a[2](1 −a[2])\\nFinally, we can use the chain rule:\\n∂L\\n∂z = ∂L\\n∂a[2]\\n∂a[2]\\n∂z\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\na[2](1 −a[2])'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='∂z\\n= −\\n( y\\na[2] + y −1\\n1 −a[2]\\n)\\na[2](1 −a[2])\\n= a[2] −y (6.43)\\nContinuing the backward computation of the gradients (next by passing the gra-\\ndients over b[2]\\n1 and the two product nodes, and so on, back to all the teal nodes), is\\nleft as an exercise for the reader.\\n6.6.5 More details on learning\\nOptimization in neural networks is a non-convex optimization problem, more com-\\nplex than for logistic regression, and for that and other reasons there are many best\\npractices for successful learning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='practices for successful learning.\\n2 The derivative is actually undeﬁned at the point z = 0, but by convention we treat it as 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='6.7 • S UMMARY 25\\nFor logistic regression we can initialize gradient descent with all the weights and\\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\\nweights with small random numbers. It’s also helpful to normalize the input values\\nto have 0 mean and unit variance.\\nVarious forms of regularization are used to prevent overﬁtting. One of the most\\nimportant is dropout: randomly dropping some units and their connections fromdropout'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='the network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\\niteration of training (whenever we update parameters, i.e. each mini-batch if we are\\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\\neach unit we replace its output with zero with probability p (and renormalize the\\nrest of the outputs from that layer).\\nTuning of hyperparameters is also important. The parameters of a neural net-hyperparameter'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='work are the weights W and biases b; those are learned by gradient descent. The\\nhyperparameters are things that are chosen by the algorithm designer; optimal val-\\nues are tuned on a devset rather than by gradient descent learning on the training\\nset. Hyperparameters include the learning rate η, the mini-batch size, the model\\narchitecture (the number of layers, the number of hidden nodes per layer, the choice\\nof activation functions), how to regularize, and so on. Gradient descent itself also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='has many architectural variants such as Adam (Kingma and Ba, 2015).\\nFinally, most modern neural networks are built using computation graph for-\\nmalisms that make it easy and natural to do gradient computation and parallelization\\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)\\nand TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\\nreader should consult a neural network textbook for further details; some sugges-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='tions are at the end of the chapter.\\n6.7 Summary\\n• Neural networks are built out ofneural units, originally inspired by biological\\nneurons but now simply an abstract computational device.\\n• Each neural unit multiplies input values by a weight vector, adds a bias, and\\nthen applies a non-linear activation function like sigmoid, tanh, or rectiﬁed\\nlinear unit.\\n• In a fully-connected, feedforward network, each unit in layer i is connected\\nto each unit in layer i +1, and there are no cycles.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='• The power of neural networks comes from the ability of early layers to learn\\nrepresentations that can be utilized by later layers in the network.\\n• Neural networks are trained by optimization algorithms like gradient de-\\nscent.\\n• Error backpropagation, backward differentiation on a computation graph,\\nis used to compute the gradients of the loss function for a network.\\n• Neural language models use a neural network as a probabilistic classiﬁer, to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='compute the probability of the next word given the previous n words.\\n• Neural language models can use pretrained embeddings, or can learn embed-\\ndings from scratch in the process of language modeling.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='26 CHAPTER 6 • N EURAL NETWORKS\\nHistorical Notes\\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-\\nloch and Pitts, 1943), a simpliﬁed model of the biological neuron as a kind of com-\\nputing element that could be described in terms of propositional logic. By the late\\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\\nBernard Widrow at Stanford) developed research into neural networks; this phase'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='saw the development of the perceptron (Rosenblatt, 1958), and the transformation\\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\\nThe ﬁeld of neural networks declined after it was shown that a single perceptron\\nunit was unable to model functions as simple as XOR (Minsky and Papert, 1969).\\nWhile some small amount of work continued during the next two decades, a major\\nrevival for the ﬁeld didn’t come until the 1980s, when practical tools for building'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='deeper networks like error backpropagation became widespread (Rumelhart et al.,\\n1986). During the 1980s a wide variety of neural network and related architec-\\ntures were developed, particularly for applications in psychology and cognitive sci-\\nence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart\\nand McClelland 1986a, Elman 1990), for which the term connectionist or paral-connectionist\\nlel distributed processing was often used (Feldman and Ballard 1982, Smolensky'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='1988). Many of the principles and techniques developed in this period are foun-\\ndational to modern work, including the ideas of distributed representations (Hinton,\\n1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\\n(Smolensky, 1990).\\nBy the 1990s larger neural networks began to be applied to many practical lan-\\nguage processing tasks as well, like handwriting recognition (LeCun et al. 1989) and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements\\nin computer hardware and advances in optimization and training techniques made it\\npossible to train even larger and deeper networks, leading to the modern term deep\\nlearning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in\\nChapter 13 and Chapter 15.\\nThere are a number of excellent books on neural networks, including Goodfellow\\net al. (2016) and Nielsen (2015).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='Historical Notes 27\\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-\\nard, Y . Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-\\nberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah,\\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-\\nwar, P. Tucker, V . Vanhoucke, V . Vasudevan, F. Vi´egas,\\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y . Yu,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='and X. Zheng. 2015. TensorFlow: Large-scale machine\\nlearning on heterogeneous systems. Software available\\nfrom tensorﬂow.org.\\nBengio, Y ., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\\nA neural probabilistic language model. JMLR, 3:1137–\\n1155.\\nBengio, Y ., P. Lamblin, D. Popovici, and H. Larochelle.\\n2007. Greedy layer-wise training of deep networks.\\nNeurIPS.\\nElman, J. L. 1990. Finding structure in time. Cognitive sci-\\nence, 14(2):179–211.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='ence, 14(2):179–211.\\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist mod-\\nels and their properties. Cognitive Science, 6:205–254.\\nGoodfellow, I., Y . Bengio, and A. Courville. 2016. Deep\\nLearning. MIT Press.\\nHinton, G. E. 1986. Learning distributed representations of\\nconcepts. COGSCI.\\nHinton, G. E., S. Osindero, and Y .-W. Teh. 2006. A fast\\nlearning algorithm for deep belief nets. Neural computa-\\ntion, 18(7):1527–1554.\\nHinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='R. R. Salakhutdinov. 2012. Improving neural networks\\nby preventing co-adaptation of feature detectors. ArXiv\\npreprint arXiv:1207.0580.\\nKingma, D. and J. Ba. 2015. Adam: A method for stochastic\\noptimization. ICLR 2015.\\nLeCun, Y ., B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backprop-\\nagation applied to handwritten zip code recognition.Neu-\\nral computation, 1(4):541–551.\\nMcClelland, J. L. and J. L. Elman. 1986. The TRACE model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='of speech perception. Cognitive Psychology, 18:1–86.\\nMcCulloch, W. S. and W. Pitts. 1943. A logical calculus of\\nideas immanent in nervous activity. Bulletin of Mathe-\\nmatical Biophysics, 5:115–133.\\nMinsky, M. and S. Papert. 1969. Perceptrons. MIT Press.\\nMorgan, N. and H. Bourlard. 1990. Continuous speech\\nrecognition using multilayer perceptrons with hidden\\nmarkov models. ICASSP.\\nNielsen, M. A. 2015. Neural networks and Deep learning .\\nDetermination Press USA.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='Determination Press USA.\\nPaszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\\n2017. Automatic differentiation in pytorch. NIPS-W.\\nRosenblatt, F. 1958. The perceptron: A probabilistic model\\nfor information storage and organization in the brain.Psy-\\nchological review, 65(6):386–408.\\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\\nLearning internal representations by error propagation. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='D. E. Rumelhart and J. L. McClelland, eds, Parallel Dis-\\ntributed Processing, volume 2, 318–362. MIT Press.\\nRumelhart, D. E. and J. L. McClelland. 1986a. On learning\\nthe past tense of English verbs. In D. E. Rumelhart and\\nJ. L. McClelland, eds, Parallel Distributed Processing,\\nvolume 2, 216–271. MIT Press.\\nRumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel\\nDistributed Processing. MIT Press.\\nRussell, S. and P. Norvig. 2002. Artiﬁcial Intelligence: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='Modern Approach, 2nd edition. Prentice Hall.\\nSmolensky, P. 1988. On the proper treatment of connection-\\nism. Behavioral and brain sciences, 11(1):1–23.\\nSmolensky, P. 1990. Tensor product variable binding and\\nthe representation of symbolic structures in connectionist\\nsystems. Artiﬁcial intelligence, 46(1-2):159–216.\\nSrivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever,\\nand R. R. Salakhutdinov. 2014. Dropout: a simple\\nway to prevent neural networks from overﬁtting. JMLR,\\n15(1):1929–1958.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-24T11:50:53-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2025-08-24T11:50:53-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'source': '../data/text_files/pdf/neural networks.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'neural networks.pdf', 'file_type': 'pdf'}, page_content='15(1):1929–1958.\\nWidrow, B. and M. E. Hoff. 1960. Adaptive switching cir-\\ncuits. IRE WESCON Convention Record, volume 4.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation:\\nPhenomena, Mechanism, and Mitigation\\nZIYAO ZHANG, Sun Yat-sen University, China\\nCHONG WANG,Nanyang Technological University, Singapore\\nYANLIN WANG∗, Sun Yat-sen University, China\\nENSHENG SHI, Huawei Cloud Computing Technologies Co., Ltd, China\\nYUCHI MA, Huawei Cloud Computing Technologies Co., Ltd, China\\nWANJUN ZHONG,Sun Yat-sen University, China\\nJIACHI CHEN, Sun Yat-sen University, China\\nMINGZHI MAO, Sun Yat-sen University, China'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='MINGZHI MAO, Sun Yat-sen University, China\\nZIBIN ZHENG, Sun Yat-sen University, China\\nCode generation aims to automatically generate code from input requirements, significantly enhancing\\ndevelopment efficiency. Recent large language models (LLMs) based approaches have shown promising results\\nand revolutionized code generation task. Despite the promising performance, LLMs often generate contents'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='with hallucinations, especially for the code generation scenario requiring the handling of complex contextual\\ndependencies in practical development process. Although previous study has analyzed hallucinations in\\nLLM-powered code generation, the study is limited to standalone function generation. In this paper, we\\nconduct an empirical study to study the phenomena, mechanism, and mitigation of LLM hallucinations within'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='more practical and complex development contexts in repository-level generation scenario. First, we manually\\nexamine the code generation results from six mainstream LLMs to establish a hallucination taxonomy of LLM-\\ngenerated code. Next, we elaborate on the phenomenon of hallucinations, analyze their distribution across\\ndifferent models. We then analyze causes of hallucinations and identify four potential factors contributing'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='to hallucinations. Finally, we propose an RAG-based mitigation method, which demonstrates consistent\\neffectiveness in all studied LLMs.\\nCCS Concepts: • Software and its engineering →Automatic programming.\\nAdditional Key Words and Phrases: Repository-Level Code Generation, Hallucination, Large Language Models\\nACM Reference Format:\\nZiyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Mao, and Zibin Zheng. 2025. LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and\\n∗Yanlin Wang is the corresponding author.\\nAuthors’ Contact Information: Ziyao Zhang, zhangzy373@mail2.sysu.edu.cn, Zhuhai Key Laboratory of Trusted Large\\nLanguage Models, Sun Yat-sen University, Zhuhai, China; Chong Wang, chong.wang@ntu.edu.sg, Nanyang Technological\\nUniversity, Nanyang Avenue, Singapore; Yanlin Wang, wangylin36@mail.sysu.edu.cn, Zhuhai Key Laboratory of Trusted'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Large Language Models, Sun Yat-sen University, Zhuhai, China; Ensheng Shi, shiensheng@huawei.com, Huawei Cloud\\nComputing Technologies Co., Ltd, Beijing, China; Yuchi Ma, mayuchi1@huawei.com, Huawei Cloud Computing Technologies\\nCo., Ltd, Shenzhen, China; Wanjun Zhong, zhongwj25@mail2.sysu.edu.cn, Sun Yat-sen University, Guangzhou, China; Jiachi\\nChen, chenjch86@mail.sysu.edu.cn, Zhuhai Key Laboratory of Trusted Large Language Models, Sun Yat-sen University,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Zhuhai, China; Mingzhi Mao, mcsmmz@mail.sysu.edu.cn, Zhuhai Key Laboratory of Trusted Large Language Models,\\nSun Yat-sen University, Zhuhai, China; Zibin Zheng, zhzibin@mail.sysu.edu.cn, Zhuhai Key Laboratory of Trusted Large\\nLanguage Models, Sun Yat-sen University, Zhuhai, China.\\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\\n© 2025 Copyright held by the owner/author(s).\\nACM 2994-970X/2025/7-ARTISSTA022\\nhttps://doi.org/10.1145/3728894'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 0, 'page_label': '1', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='https://doi.org/10.1145/3728894\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:2 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\nMitigation. Proc. ACM Softw. Eng. 2, ISSTA, Article ISSTA022 (July 2025), 23 pages. https://doi.org/10.1145/\\n3728894\\n1 Introduction\\nCode generation aims at efficiently producing code from specifications described in natural language.\\nThis process significantly reduces the manual coding workload for developers [7, 11, 51], allowing'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='them to focus more on solving advanced technical challenges and engaging in innovative tasks.\\nRecent developments have introduced a variety of large language models (LLMs) [5, 8–10, 14, 16,\\n25, 26, 32, 34, 39, 43, 44, 48] built upon the Transformer architecture [38]. These models, trained on\\nextensive code corpora, can automatically generate code from natural language inputs and have\\nshown high efficacy in code generation. For example, GPT-4 has achieved state-of-the-art results'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='on evaluation benchmarks such as HumanEval [10] and MBPP [6], demonstrating high functional\\ncorrectness, particularly in generating standalone functions based on detailed specifications.\\nHowever, in practical development scenarios, the requirements for code generation are more com-\\nplex than simply generating standalone functions [46]. To address this complexity, new benchmarks\\nsuch as CoderEval [46] and EvoCodeBench [24] have been proposed to better reflect real-world'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='repository-level development scenarios. Evaluations based on these benchmarks have revealed that\\nLLMs face challenges in generatingnon-standalone functions with contextual dependencies, such as\\ncalls to user-defined functions and project-defined data protocol. While these benchmarks provide\\nvaluable insights into the effectiveness of LLMs in practical code generation, they mainly emphasize\\nfunctional correctness of the LLM-generated code—measured by test case pass rates—without a'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='comprehensive examination of the underlying mechanism of failure.\\nTo bridge this gap, this work aims to systematically investigate issues in practical LLM-based code\\ngeneration from the perspective of hallucinations. Hallucination is one of the most significant issues\\nfor state-of-the-art generative LLMs [19]. For general natural language tasks, LLM hallucinations\\nhave been explored to a certain extent [ 19, 37, 45, 50] and are typically categorized into three'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='types: Input-Conflicting Hallucination, Fact-Conflicting Hallucination, and Context-Conflicting\\nHallucination [50]. In the domain of code generation, Liu et al. [27] conducted a study to analyze\\nhallucinations in LLM-generated code and established a taxonomy that includes Intent Conflicting ,\\nContext Deviation, and Knowledge Conflicting . While Liu et al. ’s study provided insightful findings,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='it still faces two potential limitations. First, as this study relies on benchmarks (i.e., HumanEval [10]\\nand DS-1000 [23]) focusing on standalone function or script generation, the resulting taxonomy may\\nnot fully align with practical repository-level code generation scenarios involving non-standalone\\ndependencies. For instance, project contexts such as user-defined dependencies and non-code\\nresources (e.g., configuration files) are not taken into account. Second, their study primarily catego-'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='rized fine-grained hallucinations in LLM-generated code and attempted to detect them, without\\nthoroughly analyzing potential causes or exploring mitigation feasibility. In contrast, we investigate\\nhallucinations in repository-level code generationwithin more practical development contexts,\\nadopting a holistic perspective that encompasses phenomena, mechanism, and mitigation.\\nIn this work, we conduct an empirical study to uncover the status quo and root causes of halluci-'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 1, 'page_label': '2', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='nations in LLM-based code generation within real-world projects. The study aims at answering the\\nfollowing research questions (RQs):\\n•RQ1 (Hallucination Taxonomy): What are the specific manifestations of hallucinations in\\npractical code generation, and how are they distributed?\\n•RQ2 (LLM Comparison): How do different LLMs compare in terms of hallucination occur-\\nrences and patterns?\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:3\\n•RQ3 (Potential Cause Discussion): What are the potential causes of hallucinations in practical\\nLLM-based code generation?\\nTo answer the questions, we experiment on six mainstream LLMs (ChatGPT [33], CodeGen [32],\\nPanGu-𝛼 [48], StarCoder2 [28], DeepSeekCoder [16], and CodeLlama [34]) with the CoderEval\\ndataset [46]. To obtain the hallucination taxonomy of practical LLM-based code generation, we'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='manually perform open coding [21] on the LLM-generated code. Specifically, we first extract 10% of\\nthe coding tasks from the CoderEval dataset in the initial stage. Then, from the initial annotation and\\ndiscussion, we obtain preliminary taxonomy. Finally, we obtain the fully hallucination taxonomy\\nwith iterative labelling the remaining 90% coding tasks and continuously refining the taxonomy in\\nthe process. After obtaining the taxonomy, we conduct extensive analysis based on the research'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='questions aforementioned.\\nFindings. Our study reveals the following findings. 1○LLM hallucinations in code generation can\\nbe divided into three major categories (Task Requirement Conflicts, Factual Knowledge Conflicts,\\nand Project Context Conflicts) with eight subcategories: Functional Requirement Violation, Non-\\nFunctional Requirement Violation, Background Knowledge Conflicts, Library knowledge Conflicts,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='API Knowledge Conflicts, Environment Conflicts, Dependency Conflicts, and Non-code Resource\\nConflicts. 2○We analyze the hallucination distribution in different LLMs and find that Task Require-\\nment Conflicts are the most prevalent type of hallucination across all models. 3○ We identify four\\npotential factors that cause hallucinations: training data quality, intention understanding capacity,\\nknowledge acquisition capacity, and repository-level context awareness.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Mitigation. Based on the findings, we explore a lightweight mitigation approach based on\\nretrieval augmented generation (RAG) and evaluate its effectiveness.\\nIn this approach, we take two main steps. First, we construct a retrieval library. This library is\\nbuilt based on the code repository relevant to the development scenario of each code generation task.\\nSecond, we identify useful code snippets by detecting the similarity between the task description of'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='the current generation task and the code snippets stored in the retrieval library. The code snippet\\nthat has the highest relevance to the current task, as determined by this similarity check, is then used\\nas a prompt to guide the code generation process. Experimental results show that this lightweight\\nmitigation can consistently improve the performance of all studied LLMs.\\nIn summary, this paper makes the following contributions:'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='•We conduct an empirical study to analyze the hallucinations in LLM-based code generation\\nwithin real development scenarios and establish a hallucination taxonomy.\\n•We elaborate on the phenomenon of hallucinations, analyze the distribution of hallucinations\\non different models.\\n•We further analyze causes of hallucinations and identify four possible factors.\\n•We propose a RAG-based mitigation approach based on the causes of hallucinations and'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='experiment on various LLMs to study its effectiveness.\\n2 Background & Related Work\\n2.1 LLM-based Code Generation\\nFor developers, a realistic scenario is to use a code repository to write code, which is very common\\nin practice [15]. For example, due to security and functionality considerations, companies often\\nonly build code warehouses internally. The code repository provides many private APIs that are'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 2, 'page_label': '3', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='not seen by the language model and are not public on any code hosting platform. Therefore, it is\\nworth exploring whether pre-trained language models can adapt to real development needs and\\ngenerate correct and efficient code. Previous studies such as MBPP [6] and HumanEval [10] have\\nevaluated LLMs for standalone functions, which do not depend on functions in other files. However,\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:4 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\nin real-world development scenarios, the development of a function not only relies on the text\\ndescription and function signature of the function, but also requires calling a custom API in the\\ncode repository. Such non-independent functions are commonly found in real-world generation\\nscenarios. By analyzing the 100 most popular projects written in Java and Python on GitHub [46],'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='previous work found that dependent functions account for more than 70% of the functions in open\\nsource projects. In order to better simulate real development scenarios and to check the correctness\\nof LLMs, CoderEval [46], ClassEval [13], and EvoCodeBench [24] collected code snippets and text\\ndescriptions from real code repositories and used test cases to check the correctness of the code\\nrepositories in their corresponding environments. However, the performance of the model on these'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='benchmarks is extremely poor. LLMs cannot generate correct code based on the problem description,\\nand the model prefers to generate independent code segments rather than using existing functions\\nin the current development scenario.\\n2.2 Hallucinations in LLMs\\nIn the field of natural language processing (NLP), hallucination refers specifically to situations\\nwhere the content produced by a language model in the process of generating text is inconsistent'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='with the given input or expected output environment, lacks meaning, or violates the facts [20]. This\\nkind of phenomenon is particularly prominent in text generation models, especially in tasks such\\nas text completion, summary generation, and machine translation. The output of the model must\\nmaintain a high degree of consistency and authenticity to ensure its practicality and reliability.\\nHallucination phenomena can be divided into the following categories according to their nature [50]:'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='(1) Input-Conflicting Hallucinations: When the text generated by the model deviates from the\\noriginal input source, input-conflicting hallucinations will occur. This hallucination may result\\nfrom the model’s incorrect parsing or inaccurate internal representation of the input information,\\ncausing the output content to deviate from the intent and context of the source input. (2) Context-\\nConflicting Hallucinations: This type of hallucination occurs when the text generated by the'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='model is contradictory or inconsistent with its previously generated content. Contextual conflict\\nhallucinations reflect the model’s challenges in maintaining textual coherence and consistency,\\nwhich may be due to the model’s insufficient processing of contextual information or limitations of\\nits memory mechanism. (3) Fact-Conflicting Hallucinations: When the content generated by LLM\\nis inconsistent with established knowledge or facts in the real world, fact-conflicting hallucinations'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='will occur. This hallucination reveals the model’s inadequacy in understanding and applying\\nknowledge about the external world, and may be caused by limitations in model training data, lags\\nin knowledge updates, or limitations in the model’s reasoning capabilities.\\nHowever, there is a lack of research on hallucination phenomena in the field of code generation.\\nAlthough there have been a large number of LLM-based methods to optimize code generation tasks,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='these works do not have a clear definition of the code generation hallucination. The presence of\\nhallucination problems can be detrimental to the overall quality of the generated code. This may not\\nonly affect the performance and maintainability of the code, but may also lead to unexpected errors\\nand security vulnerabilities, thus posing a threat to the stability and security of the software. In\\norder to make up for the gaps in the definition of hallucination problems, Liu et al. [27] conducted'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='a study based on a data set of standalone code generation to define hallucinations for LLMs in code\\ngeneration tasks. They divided hallucinations in LLM-based code generation into five main types,\\nbut they ignored that LLMs in real-world code generation tasks will involve complex repository-\\nlevel contexts such as development environment, system resources, external constraints, code\\nwarehouses, etc. These factors often cause LLMs to fail in actual development, leading to problems'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 3, 'page_label': '4', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='such as low usability and low accuracy. We believe that focusing solely on generating standalone\\nfunctions is inadequate for developers in real-world development scenarios, often requiring the\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:5\\nintegration of code into a comprehensive code repository by using LLMs. In order to better explore\\nhallucinations that exist in LLMs in practical development scenarios, our work obtained data sets\\nin real development scenarios for empirical study, and defined new types of hallucinations, which\\nopened up new ideas for subsequent research on hallucinations.\\n3 Evaluation Setup\\n3.1 Dataset'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='3 Evaluation Setup\\n3.1 Dataset\\nExisting work only discusses the hallucination in the function-level development scenario, but does\\nnot discuss hallucinations of LLMs in practical development scenarios. To better simulate practical\\ndevelopment scenarios, we use a set of coding tasks from real-world Python repositories based\\non CoderEval [46]. It comprises 230 Python code generation tasks. Each task consists of a natural'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='language description, a ground-truth code snippet, and a set of test cases, along with the project\\ncontext.\\n3.2 Studied LLMs\\nWe utilize several mainstream LLMs to perform code generation for the studied programming\\ntasks. The LLMs being used cover both open-source and closed-source models and span various\\nparameter sizes, listed as follows.\\n•ChatGPT [33]: ChatGPT is a versatile text generation model for multilingualism with powerful'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='code generation capabilities, we use the GPT-3.5-Turbo in our experiments.\\n•CodeGen [31]: CodeGen is a family of auto-regressive language models for program synthesis\\nwith several different versions. To better accomplish the generation task, we use the CodeGen-\\n350M-Mono model.\\n•PanGu-𝛼 [48]: PanGu-𝛼 can perform code generation tasks in multiple languages. We use the\\nPanGu-𝛼-2.6B model.\\n•DeepSeekCoder [16]: DeepSeekCoder performs well in open source models across multiple'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='programming languages and various benchmarks. We use the DeepSeekCoder-6.7B base model.\\n•CodeLlama [34]: CodeLlama is a set of pre-trained and fine-tuned generative text models\\nranging in size from 7 to 34 billion parameters. We use the CodeLlama-7b-Python-hf model.\\n•StarCoder2 [25]: StarCoder2 is a family of open code-oriented models for large languages,\\nproviding three scales of models, we use the StarCoder2-7B model.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='For each task, we use the LLMs to generate 10 code snippets by employing the nucleus sampling\\nstrategy [18] and setting temperature to 0.6, following the same setting as CoderEval.\\n3.3 Taxonomy Annotation\\nIn order to analyze the hallucination types in the LLM-generated code, we manually perform open\\ncoding [21] on the generated code to obtain the hallucination taxonomy.\\n(1) Initial Open Coding. Firstly, in the initial open-coding stage, we select 10% of the 230'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='coding tasks in CoderEval Python dataset for preliminary manual analysis. We randomly collect\\n23 generative tasks from CoderEval, we employ CodeGen, Pangu-𝛼, ChatGPT, DeepSeekCoder,\\nCodeLlama, and StarCoder2, with each model generating ten code snippets for each code generation\\ntask, culminating in a total of 1,380 code snippets to be analysed for hallucination taxonomy frame-\\nwork. For each code snippet, we further run its test cases in the actual development environment'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 4, 'page_label': '5', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='corresponding to the task to determine its correctness. Next, two annotators manually review the\\nLLM-generated code snippets that fail the tests, identifying specific code issues by referring to the\\nground truth and execution results. Note that an LLM-generated snippet may fail test cases due to\\nmultiple code issues. Ultimately, this stage identifies a total of seventeen types of issues, such as\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:6 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\nCode Generation Hallucination\\nTask Requirement Conflicts (§4.1.1)Functional Requirement ViolationExample: Wrong Functionality, Missing Functionality\\nNon-functional Requirement ViolationExample: Security, Performance, Style, Code Smell\\nFactual Knowledge Conflicts (§4.1.2)\\nBackground Knowledge Conflicts\\nLibrary Knowledge Conflicts'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Library Knowledge Conflicts\\nAPI Knowledge Conflicts Example: Parameters, Guard ConditionsSimilar-but-wrong APIs, API Call Exceptions\\nProject Context Conflicts (§4.1.3)\\nEnvironment Conflicts\\nDependency Conflicts Example: Undefined Methods, API Version Conflict\\nNon-code Resource ConflictsExample: Data, Configs, Assets, Connections\\nFig. 1. Taxonomy of Hallucinations in LLM-based Code Generation.\\nMissing Functionality , Code Smell , Similar-but-Wrong APIs, Data Resource Conflicts and Undefined'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Methods, after deduplication.\\n(2) Preliminary Taxonomy Construction. Secondly, based on the specific issues identified,\\nwe conduct a manual analysis to group related code issues into broader hallucination types. For\\nexample, code issues involving API Parameters , API Guard Conditions , Similar-but-Wrong APIs, and\\nAPI Call Exceptions are grouped under the type API Knowledge Conflicts , as they all represent API'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='misuses and conflicts with factual API knowledge. This grouping process, conducted by two authors\\nin consultation with the original annotators, results in eight hallucination types. Building on this\\ngrouping, we further categorize the types into three higher-level categories that generally align\\nwith definitions in the NLP domain. Finally, we establish a preliminary hallucination taxonomy\\ncomprising three major categories divided into eight subcategories, each including some code'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='issues identified in the previous stage.\\n(3) Full Taxonomy Construction. Finally, after obtaining the taxonomy categorization, the\\nremaining code snippets will be independently annotated by three newly invited volunteers with\\nextensive Python programming experience, two with more than ten years of experience and one\\nwith four years of programming experience. The goal of the annotation is to identify issues in the'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM-generated code snippets and assess whether these issues fit within the established preliminary\\ntaxonomy. If any previously unknown code issues are identified, we will discuss whether existing\\nhallucination (sub)categories can accommodate them or if a new type should be added to the\\ntaxonomy. Ultimately, we identify two new code issuesAsset Conflicts and Connection Conflicts , but\\nno new hallucination categories or subcategories.\\n3.4 Hallucination Statistics'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='3.4 Hallucination Statistics\\nDuring the process of taxonomy annotation, we record the specific code issues identified in\\neach LLM-generated code snippet and count the frequency of each hallucination based on the\\nrelationships between hallucination (sub)categories and specific code issues. It is important to note\\nthat for the code snippets that contain multiple code issues, all hallucinations present will be included\\nin the statistics.\\n4 Evaluation Results'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='in the statistics.\\n4 Evaluation Results\\nIn this section, we present the evaluation results and answer the three aforementioned research\\nquestions.\\n4.1 RQ1: Hallucination Taxonomy\\nThe overall LLM coding hallucination taxonomy we obtained from Section 3.3 is presented in\\nFigure 1. Through manual annotation, we identify three primary hallucination categories: Task\\nRequirement Conflicts , Factual Knowledge Conflicts , and Project Context Conflicts , which can be'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 5, 'page_label': '6', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:7\\nFig. 2. Hallucination Distribution.\\nfurther divided into eight specific types. Note that our three primary categories align well with\\nthe hallucination types in the general domain [ 20]. Task requirement conflicts correspond to\\ninput-conflicting hallucinations in the general domain, indicating that the generated code does'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='not meet the functional or non-functional requirements of the coding tasks. Factual knowledge\\nconflicts correspond to knowledge-conflicting hallucinations in the general domain, indicating that\\nthe generated code does not comply with background knowledge, library/framework knowledge,\\nor API knowledge. Project context conflicts correspond to context-conflicting hallucinations in the\\ngeneral domain, indicating that the generated code incorrectly uses project contexts, including'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='environments, dependencies, and resources.\\nFigure 2 shows the distribution of the hallucination types, where the denominator is the sum of\\nall annotated hallucination instances across the six LLMs. We present the detailed hallucination\\ntypes in our taxonomy as follows.\\n4.1.1 Task Requirement Conflicts (43.53%). In the general domain, input-conflicting hallucinations\\noccur when the answers generated by LLMs deviate from the original intentions of user inputs [19].'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='In the context of code generation tasks, the primary intentions of inputs typically revolve around the\\nfunctional and non-functional requirements of the coding tasks. When the code generated by LLMs\\ndoes not align with these requirements, hallucinations related to Task Requirement Conflicts occur.\\nSpecifically, these conflicts can be categorized into two types: Functional Requirement Violation and\\nNon-functional Requirement Violation .'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Non-functional Requirement Violation .\\nFunctional Requirement Violation (36.66%). Functional requirements are typically expressed in\\nnatural language and describe the desired functionality of the generated code. When these re-\\nquirements are not correctly and comprehensively understood, the resulting code may fail to\\nmeet expected functionality, leading to logic bugs (such as unexpected execution behaviors) or'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 6, 'page_label': '7', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='runtime errors (such as the KeyError during dictionary access). More specifically, the functional\\nrequirement mismatch can be subdivided into two typical types: Wrong Functionality and Missing\\nFunctionality. For example, as illustrated in Figure 3, the functional requirement involves handling\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:8 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\ndefhydrate_time(nanoseconds,tz=None):frompytzimportFixedOffsetseconds,nanoseconds=map(int,divmod(nanoseconds,1000000000))minutes,seconds=map(int,divmod(seconds,60))hours,minutes=map(int,divmod(minutes,60))t=Time(hours,minutes,seconds,nanoseconds)iftzisNone:returnttz_offset_minutes,tz_offset_seconds=divmod(tz,60)zone=FixedOffset(tz_offset_minutes)returnzone.localize(t)\\nGround-truth'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Ground-truth\\nDocstring # similarity_filter.pyHydratorfor`Time`and`LocalTime`values. :paramnanoseconds: :paramtz: :return:Time\\ndefhydrate_time(nanoseconds,tz=None):from.timeimportTimereturnTime.from_nanoseconds(nanoseconds,tz)\\nLLMGeneration\\nHandle `LocalTime` in functional requirement\\nOverlook `LocalTime`\\nFig. 3. Example: Functional Requirement Violation.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='defvalidate_from_content(cls,file_content=None):iffile_contentisNone:raiseIRValidatorException( \"RegistryYAMLcontentismissing\")registry_dict=yaml.safe_load(file_content)ifnotisinstance(registry_dict,dict):raiseIRValidatorException(  \"Registryfileisemptyorcorrupted:{}\".format(file_content))try:jsonschema.validate(registry_dict,cls.SCHEMA_REGISTRY)exceptjsonschema.exceptions.ValidationErroraserror:raiseIRValidatorException(\"{}infile:\\\\n{}\".format(error.message,file_content))returnregistry_dict'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content=\"Ground-truth\\nDocstring validatesthatRegistryYAMLcontenthasallrequiredfields:paramfile_content:contentoftheRegistryYAMLfile:raiseIRValidatorException:whenmandatorydataismissinginRegistry:return:DictionarywithdataloadedfromaRegistryYAMLfile\\nAvoiding safety hazards\\ndefvalidate_from_content(cls,file_content=None):if file _contentisNone:raiseIRValidatorException('filecontentismissing’)file_data=yaml.load(spec_content)validate_data(cls, file_data)return file_data\\nLLMGeneration\"),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLMGeneration\\nLeading to system security risks\\nFig. 4. Example: Non-functional Requirement Violation.\\nLocalTime based on the specific timezone tz. In the ground-truth code, this requirement is ad-\\ndressed by the lines highlighted in the green rectangle. However, the code generated by PanGu-𝛼\\noverlooks this requirement, resulting in a hallucination of Functional Requirement Violation.\\nNon-functional Requirement Violation (6.86%). Besides functional requirements, developers of-'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 7, 'page_label': '8', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ten have non-functional requirements for the generated code, such as security concerns or perfor-\\nmance considerations. Although these non-functional requirements are often more implicit than\\nfunctional ones and are not explicitly outlined in the user descriptions [41], they reflect common\\ndefault requirements in real-world software development.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:9\\nOur open coding annotation reveals that non-functional requirements in coding tasks can be\\nmainly divided into the following aspects: Security, Performance, Style, and Code Smell . Generated\\ncode that violates these non-functional requirements may introduce safety risks or increase the\\nmaintenance complexity to the project.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='maintenance complexity to the project.\\nSpecifically, on the security side, the generated code may introduce vulnerabilities such as\\nunsanitized inputs, which can lead to insecure deserialization or SQL injection attacks. As shown in\\nFigure 4, the ground-truth code uses the safe_load function to safely read YAML files. In contrast,\\nthe LLM-generated code utilizes the load function, thereby introducing a potential security risk.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Regarding performance, the generated code may lack optimization for execution efficiency, for\\nexample, by using inefficient loop structures that lead to unnecessary overhead in computing and\\nmemory resources. Style violations often occur when the generated code fails to follow established\\nprogramming conventions or style guides, such as inconsistent naming conventions or inappropriate\\ncode layout, which can negatively affect code readability and maintainability. Code smell violations'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='include issues such as overly complex functions or excessive use of global variables, which increase\\nthe complexity and potential risks associated with future maintenance.\\n4.1.2 Factual Knowledge Conflicts (31.91%). In the field of NLP, the term “factual conflicts” refers\\nto content generated by LLMs that does not align with established knowledge or facts about the\\nreal world. Practical software development similarly relies on various types and levels of factual'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='knowledge to produce correct code. Consequently, when LLMs fail to accurately understand and\\napply background knowledge [40], library/framework knowledge, or API knowledge, hallucinations\\non Factual Knowledge Conflicts arise. We further divide this hallucination category into three types:\\nBackground Knowledge Conflicts , Library Knowledge Conflicts , and API Knowledge Conflicts .\\nBackground Knowledge Conflicts (8.82%). Background Knowledge Conflicts are a common issue'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='when using large language models. These conflicts refer to the situation that the generated code is\\ninconsistent with existing domain-specific knowledge, potentially rendering the code invalid or\\nintroducing logic bugs and risks. For instance, in automotive software development, if the generated\\ncode fails to adhere to certain industry standards (e.g., AUTOSAR [1]), it can result in significant\\ncompliance issues or safety risks.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='compliance issues or safety risks.\\nBackground knowledge typically includes Domain Concepts (e.g., specific data formats or pro-\\ntocols) and related Standards and Specifications (e.g., standard parameters or configurations). For\\nexample, Figure 5 shows an example about OCFL (Oxford Common File Layout), a specification for\\ndata storage and transformation. According to the official description [3], an OCFL storage root'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='must contain a “Root Conformance Declaration” following the “NAMASTE” specification and may\\ninclude a file named ocfl_layout.json to describe the root layout arrangement. However, the\\ngenerated code might incorrectly focus on other OCFL aspects that are irrelevant to storage root.\\nLibrary Knowledge Conflicts (2.68%). In modern software development, developers frequently\\nemploy frameworks or third-party libraries (e.g., Django [2] for web applications) to expedite the'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='development process by reusing the features or functionalities that these frameworks or libraries\\nprovide. When utilizing these frameworks or libraries, LLMs may encounter factual errors that\\nlead to unexpected behaviors or even security risks. For example, As depicted in Figure 6, the\\ntask requires the model to generate a decorator that caches the return value of the function upon\\neach invocation. In the code generated by the DeepSeekCoder model, the APIs from the asyncio'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='framework are utilized. This framework is designed for asynchronous processing, and the model’s\\nmisuse of this framework poses unexpected behaviors to the developed application.\\nAPI Knowledge Conflicts (20.41%). API Knowledge Conflicts are a common hallucination in\\nLLM-generated code caused by various types of API misuses, such as parameter errors, improper\\nguard conditions, similar-but-incorrect/deprecated API usage, and improper exception handling.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 8, 'page_label': '9', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:10 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\ndef initialize(self):\\n    self.root = File(self .path, “ro”, \"ocfl_storage.json\")\\n    self.root.open()\\n    self.root.create_child(\"ocfl_storage\", \"ocfl_storage\")\\n    self.root.create_child(\"ocfl_storage \", \"ocfl_storage.json\")\\n    ... //omitted\\n    self.root.create_child(\"ocfl_storage\", \"ocfl_storage.tar.bz2\")\\ndef initialize(self):\\n (parent, root_dir) = fs.path.split(self .root)\\nparent_fs = open_fs(parent)'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='parent_fs = open_fs(parent)\\nif parent_fs.exists(root_dir):\\nraise StoreException(\"OCFL storage root %s already exists, aborting!\" % (self.root))\\nself.root_fs = parent_fs.makedir(root_dir)\\nNamaste(d=0, content=self.declaration_tvalue).write(pyfs=self.root_fs)\\n if self.disposition is not None:\\nwith self.root_fs.open(self.layout_file,\\'w\\') as fh:\\nlayout = {\\'extension\\': self.disposition,\\n\\'description’: \"...\"}\\njson.dump(layout, fh, sort_keys=True, indent=2)\\nGround-truth\\nDocstring'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Ground-truth\\nDocstring \\nCreate and initialize a new OCFL storage root.\\n:param self: an OCFL storage root\\n:return: OCFL storage root\\nLLM Generation\\nCreate an OCFL \\nstorage root\\nMisunderstood \\nOCFL and \\ngenerated wrong \\nfile structure\\nFig. 5. Example: Background Knowledge Conflicts.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Fig. 5. Example: Background Knowledge Conflicts.\\ndefttl_cache(maxsize=128,ttl=600,timer=time.monotonic,typed=False):    timer=timeror(lambda:perf_counter())    is_async=asyncio.iscoroutinefunction    is_async_gen=asyncio.iscoroutine    get_loop=asyncio.get_event_loop   ifis_async(func):        return_cache(TTLCache(maxsize,ttl,timer),typed)(maxsize)'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='defttl_cache(maxsize=128,ttl=600,timer=time.monotonic,typed=False): ifmaxsizeisNone:return_cache(_UnboundTTLCache(ttl,timer),typed)elifcallable(maxsize):return_cache(TTLCache(128,ttl,timer),typed)(maxsize)else:return_cache(TTLCache(maxsize,ttl,timer),typed)\\nGround-truth'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Ground-truth\\nDocstring Decorator to wrap a function with a memoizing callable that saves up to `maxsize` results based on a Least Recently Used (LRU) algorithm with a per-item time-to-live (TTL) value.:parammaxsize ttl timer typed:typeint int object boolean:return Decorator\\nLLMGeneration\\nCorrectly handle the decorator\\nIncorrectlyadopt the asyncio framework\\nFig. 6. Example: Library Knowledge Conflicts.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Fig. 6. Example: Library Knowledge Conflicts.\\nFor example, parameter errors can occur when inappropriate parameter types or values are used in\\nthe generated code, causing API calls to fail or return unexpected results. This case is especially\\ncommon in dynamically typed programming language such as Python. Improper guard conditions\\nmean that the generated code does not correctly implement pre-condition checks. If the validity of'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='the pre-conditions of certain APIs is not verified before calling them (e.g., file existence), runtime\\nerrors may occur. In terms of similar-but-wrong/deprecated API usage, LLMs may mistakenly\\nchoose APIs with similar functions but different applicable scenarios. Although this choice is\\nsyntactically correct, it cannot meet actual application needs. Improper exception handling involves\\ngenerating code that fails to properly handle potential exceptions throwed by certain APIs, which'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 9, 'page_label': '10', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='can cause the program to crash or behave abnormally when faced with an error condition. This\\nkind of API knowledge conflict will not only directly lead to program functional errors, but may\\nalso affect the stability of the system and the usability of the code.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:11\\ndef parse_frequency(frequency):\\nif not frequency:\\n… … //omitted\\nif not time_unit.endswith(\\'s\\'):\\ntime_unit += \\'s\\'\\nif time_unit == \\'months\\':\\nnumber *= 30\\ntime_unit = \\'days’\\nelif time_unit == \\'years\\':\\nnumber *= 365\\ntime_unit = \\'days\\'\\ntry:\\nreturn datetime.timedelta(**{time_unit: number})\\nexcept TypeError:\\nraise ValueError(f\"Could not parse consistency check frequency \\'{frequency}\\'\")'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='def parse_frequency(frequency):\\nif frequency == \"always\":\\nreturn None\\nelif frequency == \"years\":\\nreturn datetime.timedelta(year=1)\\n... //omitted\\nelse:\\nraise ValueError(\"Unknown frequency: \\'%s\\'\", frequency)\\nGround-truth\\nDocstring \\nGiven a frequency string with a number and a unit of time, return a corresponding datetime.timedelta instance or None if the frequency \\nis None or “always”. For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content=\"Raise ValueError if the given frequency cannot be parsed.\\nLLM Generation\\nCorrect use of parameter ‘days' \\nin datetime.timedelta()\\nIncorrect use of  a non-existent \\nparameter 'years' in \\ndatetime.timedelta()\\nFig. 7. Example: API Knowledge Conflicts.\\nWe present an example in Figure 7. In this generation task, CodeGen correctly identifies the task\\nintent and utilizes the datetime.timedelta() function. However, the code snippet generated by\\nCodeGen uses a non-existing parameter year.\"),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='CodeGen uses a non-existing parameter year.\\n4.1.3 Project Context Conflicts (24.56%). Project Context Conflict hallucination refers to the phenom-\\nenon where the code generated by LLMs is inconsistent with the specific context of a given project.\\nIn a sense, this type of hallucination is also a type of factual conflict, where facts within the current\\nproject context are violated. The key difference is thatFactual Knowledge Conflicts involve common'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='facts (e.g., libraries and APIs) that are publicly accessible, while Project Context Conflicts pertain\\nto facts that are specific to the corresponding project, which are generally unavailable for public\\naccess. Project Context Conflicts are often caused by LLMs not aware of such project-specific facts\\nwhen generating code. This hallucination can be divided into Environment Conflicts, Dependency\\nConflicts, and Non-code Resource Conflicts .'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Conflicts, and Non-code Resource Conflicts .\\nEnvironment Conflicts (0.94%). In the process of software development, conflicts between the\\ngenerated code and the development environment are common, especially regarding version\\ndifferences in platforms, operating systems, drivers, languages, compilers/interpreters, frameworks,\\nand libraries. When generating code, such environmental concerns are often not considered, leading'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='to problematic code if there are environment-sensitive operations. For example, if the generated\\ncode uses language features (e.g., f-string expressions) from higher Python versions that are not\\nsupported by the current development environment, a conflict arises. For example, Figure 8 shows\\na code snippet generated by CodeGen that attempts to use the package _lfu_cache, which does\\nnot exist in the current environment.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='not exist in the current environment.\\nDependency Conflicts (11.26%). Dependency Conflicts arise when the generated code relies on\\nundefined or unimported dependencies, such as user-defined attributes and functions. This often\\nresults in errors such as undefined variables or no-member errors. In practical software development,\\n70% of functions are non-standalone and depend on entities defined elsewhere in the project or'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 10, 'page_label': '11', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='imported from third-party libraries [46]. Due to the inability of LLMs to access the entire project\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:12 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\ndef lfu_cache(maxsize=128, typed=False):\\n       import _lfu_cache\\n  return _lfu_cache.lfu_cache(maxsize, typed)\\ndef lfu_cache(maxsize=128, typed=False):\\n  if maxsize is None:\\n    return _cache(_UnboundCache(), typed)\\n  elif callable(maxsize):\\n    return _cache(LFUCache(128), typed)(maxsize)\\n  else:\\n       return _cache(LFUCache(maxsize), typed)\\nGround-truth\\nDocstring'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Ground-truth\\nDocstring \\nDecorator to wrap a function with a memoizing callable that saves up to `maxsize` results based on a Least \\nFrequently Used (LFU) algorithm.\\nLLM Generation\\nUsing a non-existent \\npackage  in the current \\nenvironment\\nUse the current \\ndevelopment \\nenvironment’s classes \\nFig. 8. Example: Environment Conflicts.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Fig. 8. Example: Environment Conflicts.\\ndefgenerate_default_observer_schema(app):app.status.mangled_observer_schema=deepcopy(app.observer_schema)   observer_schema={}forresource_manifestinapp.spec.manifest:try:ifresource_manifestnotinapp.observer_schemas:    observer_schema[resource]=generate_default_observer_schema_dict()exceptIndexError:app.status.mangled_observer_schema.append(generate_default_observer_schema_dict(resource_manifest,first_level=True))   returnobserver_schema'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='defgenerate_default_observer_schema(app):observer_schema={}resources=app.spec.manifestforresourceinresources:ifresourcenotinapp.observer_schemas:observer_schema[resource]=generate_default_schema()returnobserver_schema \\nGround-truth'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content=\"Ground-truth\\nDocstring Generate the default observer schema for each Kubernetes resource present in``spec.manifest`` for which a custom observer schema hasn't been specified.:param app (krake.data.kubernetes.Application): The application for which to generate a default observer schema:return \\nLLMGeneration\\nProper use of  thegenerate_default_schema function\\nError using non-existing generate_default_observer_schema_dict function.\\nFig. 9. Example: Dependency Conflicts.\"),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Fig. 9. Example: Dependency Conflicts.\\ncontext, they often resort to using non-existent APIs, functions, attributes, and variables when\\ndealing with non-standalone functions.\\nFor example, Figure 9 illustrates a scenario involving a user-defined functiongenerate_default\\n_observer_schema_dict(). In this case, the PanGu-𝛼 erroneously uses a function with a similar\\nbut incorrect name, generate_default_schema(), which does not exist in the project. This leads'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='to a Dependency Conflict, as the code fails to execute correctly due to the missing definition.\\nNon-code Resource Conflicts (12.36%). Non-code Resource Conflicts can be further categorized\\ninto four main types:Data, Configs, Assets, and Connections. Each type of conflict can undermine the\\ncorrectness and reliability of the system. Data conflicts often involve mishandling of data formats,\\nfields, or content. For example, if the generated code incorrectly parses a data file or attempts to'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 11, 'page_label': '12', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='access a non-existent data field, it can lead to runtime errors or data inconsistencies. Config conflicts\\narise from incorrect settings or options in configuration files. This might include using undefined\\nconfiguration fields or options, which can prevent the generated code from properly applying\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:13'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='def get_repo_archive(self, url: str, destination_path: Path) -> Path:  archive_name = url.split(\\'/\\')[-1]  archive_path = os.path.join(os.getcwd(), \"archive\", archive_name)  if not os.path.exists(archive_path):    os.makedirs(archive_path)    tar = tarfile.open(archive_path, \"w:gz\")  for i in range(len(url.split(\\'/\\'))-1):    tar.add(url.split(\\'/\\')[i+1], arcname=url.split(\\'/\\')[i+1])    tar.close()  return archive_path'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='def get_repo_archive(self, url: str, destination_path: Path) -> Path:  res = self.http_request(url)  destination_path.parent.mkdir(parents=True, exist_ok=True)  destination_path.write_bytes(res.content)  extract_to = Path(str(destination_path).split(\".tar.gz\")[0])  tar = tarfile.open(destination_path)  tar.extractall(path=extract_to)  tar.close()  return extract_to\\nGround-truth'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content=\"Ground-truth\\nDocstring Given an url and a destination path, retrieve and extract.tar.gz archive which contains 'desc' file for each package    Args:      url: url of the.tar.gz archive to download      destination_path: the path on disk where to extract archive    Returns:        path where the archive is extracted to\\nLLMGeneration\\nSuccessfully fetched the file with the correct path and filename\\nFail to fetch the file due to the use of  a non-existent file path\"),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Fig. 10. Example: Non-code Resource Conflicts.\\nthe configuration and affect system behavior. Asset conflicts here involve improper handling of\\nasset files and their properties. For instance, if the generated code fails to set the correct size and\\nresolution for images or videos, it can result in display issues or severe bugs, such as application\\ncrashes. Connection conflicts relate to wrong settings of various connection resources, such as'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='incorrect IP addresses, port numbers, or database tables. These issues often lead to failed connections\\nor operations being performed on the wrong server or database, potentially causing data leaks or\\nsecurity incidents.\\nFor example, Figure 10 illustrates the generation task hopes that LLM can generate a function\\nfor a given URL and target path to retrieve and extract the tar.gz compressed package containing'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='each package’s “description” file. However, in the code snippet generated by the model, the model\\nadds “archive” as a path in the target path, which causes the code snippet to point to a non-existent\\nfile path. This will not allow the tar.gz compressed package to be correctly obtained, resulting in\\na program error.\\nRQ1 Summary: We have established a hallucination taxonomy in LLM-based code generation,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='comprising three main categories (i.e., Task Requirement Conflicts , Factual Knowledge Conflicts ,\\nand Project Context Conflicts ) with eight subtypes. Among these, Task Requirement Conflicts are\\nthe most frequently occurring category.\\n4.2 RQ2: LLM Comparison\\nBased on the obtained hallucination taxonomy for LLM-based code generation, we further analyze\\nthe hallucination distribution comparison across different models. Figure 11 shows the distribution'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 12, 'page_label': '13', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='of the number of hallucinations of different models based on the breakdown analysis of the three\\nhallucination types. We find that Task Requirement Conflicts are the most common hallucination\\ntype for all models, while Factual Knowledge Conflicts and Project Context Conflicts remain at\\napproximately the same frequency.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:14 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n1400\\n1600\\nTask Requirement Conflicts Factual Knowledge Conflicts Project Context Conflicts\\nCodeGen PanGu-α\\nChatGPT DeepSeekCoder\\nCodeLlama StarCoder2\\nCodegen\\nPanGu-α\\nChatGPT\\nDeepSeekCoder\\nCodeLlama\\nStarCoder2\\nFig. 11. Hallucination distribution of different models\\nAdditionally, we find that Code-\\nGen and StarCoder2 exhibit a notably\\nhigher frequency of hallucinations re-'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='higher frequency of hallucinations re-\\nlated to Task Requirement Conflicts ,\\nwhereas DeepSeekCoder and CodeL-\\nlama demonstrates the lowest occur-\\nrence. This variation may be related\\nto the models’ ability to understand\\ntask requirements, potentially influ-\\nenced by factors such as the model\\nsize or the training corpora. For in-\\nstance, DeepSeekCoder and CodeL-\\nlama are trained on diverse corpora\\nincluding both extensive code and'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='including both extensive code and\\ntext data, while CodeGen and StarCoder2 are primarily trained on code-related data. In terms of\\nFactual Knowledge Conflicts , PanGu-𝛼 demonstrates the highest frequency of factual hallucinations.\\nThis can be attributed to its extensive training on Chinese corpora, which may have led to a\\nrelatively limited exposure to factual knowledge, such as specific domain concepts, expressed in\\nEnglish.\\nRQ2 Summary:'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='English.\\nRQ2 Summary:\\nTask Requirement Conflicts are the most prevalent type of hallucination across all models, with\\nCodeGen and StarCoder2 showing a notably higher frequency of this type compared to others.\\n4.3 RQ3: Potential Cause Discussion\\nIn this research question, we conduct further analysis on the possible root causes of the hallucina-\\ntions in practical LLM-based code generation.\\n4.3.1 Training Data Quality. The quality of the training data is a crucial factor in the development'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='of LLMs, as it significantly affects models’ inference capabilities. Recent LLMs are often trained on\\nlarge-scale code corpora typically collected from open-source repositories. However, the quality\\nof these repositories is not always assured, leading to the inclusion of low-quality data in the\\ntraining corpora. Such issues include mismatches between docstrings and code [ 36], inefficient\\nor insecure code implementations [22], misused API calls, outdated library documentation and'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='usage [52], and a lack of domain diversity. When LLMs are trained on such corpora, they may\\nunintentionally incorporate these flaws into their knowledge base, leading to hallucinations in\\ncode generation. As shown in Figure 4, LLMs may generate code that uses unsafe APIs, reflecting\\nproblematic patterns commonly found in the training data (e.g., there are 256k lines of Python code\\nin the GitHub repository that use yaml.load() instead of the safer yaml.safe_load() API). This'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='indicates that the model may have been affected by low-quality data during the training phase.\\nMost hallucinations associated with Task Requirement Conflicts and Factual Knowledge Conflicts\\ncan be, to a certain extent, attributed to data quality issues in the training corpora. This highlights\\nthe importance of building a high-quality code-related training data to reduce hallucinations in\\ncode generation.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 13, 'page_label': '14', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='code generation.\\n4.3.2 Intention Understanding Capacity. Although LLMs have shown great potential in code gener-\\nation, they still face challenges in accurately capturing and interpreting specific user intentions\\nand needs [30]. This limitation can result in generated code that is functionally or non-functionally\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:15\\ninaccurate, thereby affecting the overall effectiveness and trustworthiness of LLM-based code gen-\\neration [35]. The core advantage of LLMs lies in their excellent pattern recognition capabilities, but\\nthis is also the source of their limitations. LLMs tend to generate code based on common patterns\\nobserved in the training data rather than from a deep understanding of the specific requirements'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='context. As shown in Figure 3, the task description requires the LLM to handle LocalTime, which\\nis ignored in the LLM-generated code. This example highlights LLMs’ inadequacy in comprehen-\\nsively interpreting the intentions behind requirements. Furthermore, LLMs also show limitations in\\nhandling subtle requirements involving complex logic or multi-step operations [47]. Due to a poor\\nunderstanding of the overall scope and potential limitations of the task, LLM-generated code may'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='only address part of the requirements or perform poorly in handling edge cases. This can result in\\ngenerated code snippets that seem correct on the surface but fail to meet specific business logic or\\nfunctional requirements in practice. As shown in Figure 4, although the code generated by LLMs\\nfunctionally matches the description of the problem, such code will be more vulnerable to attacks\\nin real development scenarios.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='in real development scenarios.\\n4.3.3 Knowledge Acquisition Capacity. LLMs may learn incorrect knowledge and miss certain\\ndomain-specific knowledge due to the aforementioned training data quality issues. There is a\\ndisparity in the distribution of data across different domains within the training dataset. The\\ncomputation domain constitutes a substantial 63% of the total data, while the network domain\\nis markedly less represented, comprising only 8% [ 53]. For example, as shown in Figure 5, the'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='task description needs a piece of code for generating a data format that satisfies the OCFL storage\\nspecification, but LLM generates incorrect code, possibly due to its lack of the OCFL-related\\nknowledge. Moreover, as software development techniques evolve, such as library updates, relevant\\nknowledge developed after model training period cannot be acquired by LLMs. Unlike human\\ndevelopers who can continuously learn and integrate latest information during development, LLMs'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='are limited to the knowledge available at the time of training. This limitation in LLMs’ knowledge\\nacquisition capacity leads to hallucinations related to incorrect or outdated factual information\\nin the generated code. This highlights the need for a knowledge acquisition mechanism, such\\nas retrieval augmented generation (RAG), to allow LLMs to update, correct, and supplement the\\nknowledge they have learned.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='knowledge they have learned.\\n4.3.4 Repository-level Context Awareness. Feeding all project contexts, including code, documents,\\nand non-code resources, into an LLM for repository-level code generation is challenging and\\nimpractical. This is because LLMs, typically based on the Transformer architecture [ 38], have\\ntoken number limits (e.g. 8k or 12k tokens) and experience quadratic computation growth as the'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='number of tokens increases. Additionally, including all project contexts can introduce a significant\\namount of irrelevant information, hindering LLMs’ ability to focus on the most relevant context for\\ncode generation. Therefore, it is crucial to develop methods that make LLMs aware of the project\\ncontexts (project-specific memory) that are precisely related to the current coding task. Recent\\nworks attempt to integrate static analysis tools [42] or apply retrieval-augmented generation (RAG)'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='based on repository-level retrieval corpora [49] to address such context awareness issues.\\nRQ3 Summary: By further analyzing the causes of hallucinations, we identify four possible\\ncontributing factors: training data quality, intention understanding capacity, knowledge acquisi-\\ntion capacity, and repository-level context awareness. Deficiencies in any of these factors can\\nlead to hallucinations in practical development scenarios.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 14, 'page_label': '15', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Proc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:16 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\n5 Mitigation Approach\\n5.1 Motivation\\nThe aforementioned root causes of hallucinations in code generated by LLMs can be traced back\\nto three main factors at the inference stage: incorrect or insufficient understanding for task re-\\nquirements, the lack of factual knowledge pertinent to the generation tasks, and the inability to'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='access the necessary code and non-code resources from the repository. These limitations create\\nsubstantial challenges for LLMs in code generation in practical development settings. Drawing\\ninspiration from existing work [49] on repository-level code generation, we explore the feasibility\\nof applying retrieval-augmented generation (RAG) to mitigate hallucinations. The idea is that by\\nproviding LLMs with code snippets relevant to the current task, they can better understand the'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='requirements and gain awareness of specific factual knowledge and project contexts.\\n5.2 RAG-based Mitigation\\nTo implement the RAG method, we first collect all code repositories from the CoderEval dataset\\nand follow RepoCoder’s method [ 49] to construct the retrieval corpora. Specifically, for each\\nrepository, we apply a sliding window to scan all the source files in it. This scanning process\\nextracts consecutive lines of code based on a predefined window size. The sliding window moves'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='by a fixed number of lines (slicing step) at each iteration to ensure complete coverage of the code.\\nWe adhere to RepoCoder’s parameter settings, with a window size of 20 lines and a sliding step of\\n2 lines. To prevent answer leakage, code lines containing or following the ground-truth code are\\nexcluded from the scanning process. Once all files are processed, a retrieval corpus of code snippets\\nis generated for the repository.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='is generated for the repository.\\nWe employ a sparse bag-of-words (BOW) model for our retrieval mechanism, which simplifies\\ngauging similarity between textual data. This model transmutes both the query and the candidate\\ncode snippets into sets of tokens, which are compared using the Jaccard index. The Jaccard index\\nmeasures the similarity between two sets by dividing the size of their intersection by the size of'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='their union, we choose the code snippet that retrieves the top ten scores each time to return as the\\nprompt for the LLMs.\\n5.3 Evaluation\\nTable 1. Experimental results of mitigation method under Pass@1.\\nModel Raw Method RAG-based Mitigation\\nCodeGen 1.30% 2.61% (↑1.31%)\\nPanGu-𝛼 0.04% 1.74% (↑1.70%)\\nDeepSeekCoder 3.04% 3.91% (↑0.87%)\\nCodeLlama 2.17% 5.22% (↑3.05%)\\nStarCoder2 0.04% 2.61% (↑2.57%)\\nChatGPT 10.40% 14.78% (↑4.38%)\\nWe evaluate the effectiveness of the\\nRAG-based mitigation method with'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='RAG-based mitigation method with\\nthe six LLMs: CodeGen, PanGu- 𝛼,\\nChatGPT, DeepSeekCoder, CodeL-\\nlama, and StarCoder2 on the CodeE-\\nval dataset. We compared our RAG-\\nbased mitigation method with the\\nRaw method. In the Raw method, we\\nonly provide LLMs basic docstrings\\nand function signatures. In the RAG-\\nbased mitigation, when providing\\ndocstrings and function signatures, we will obtain ten related code snippets from the above-'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='constructed retrieval library through a similarity algorithm as prompts and provide them to LLMs.\\nWe use the Pass@1 metric to assess the functionality correctness of the generated code snippets\\naccording to test cases. As shown in Table 1, the Pass@1 scores of all six models are slightly\\nimproved with the RAG-based mitigation method. Note that the performance improvement in our\\nexperiments is modest, as the mitigation method we explored is preliminary. We consider this'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 15, 'page_label': '16', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='experiment as an pilot study to explore the potential effectiveness of RAG-based mitigation. In\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:17\\nTask Requirement Conflicts Factual Knowledge Conflicts Project Context Conflicts Correct\\n79\\n37\\n15\\n8\\n6\\n35\\n5\\n2\\n25\\n4\\n52\\n7\\n4\\n1\\n2\\n17\\n139\\n48\\n24\\n88\\nFig. 12. Hallucination mitigation of ChatGPT.\\nfuture work, there are more methods worth studying, such as model fine-tuning and multi-agent\\nframework with tool using, etc.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='framework with tool using, etc.\\nWe also investigate how hallucinations evolve before and after employing RAG-based mitigation.\\nWe manually track the hallucinations in the code snippets generated by ChatGPT in both the\\nRAW Method and RAG-based Mitigation. Figure 12 presents the analysis results in four sub-\\nfigures, each representing the evolution of a specific category of hallucination. The results reveal'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='several interesting findings. First, among the three categories of hallucinations, the RAG-based\\nmitigation shows the least impact on Factual Knowledge Conflicts (see the second sub-figure). This\\nis intuitive, as the RAG in our experiments is utilized to retrieve code contexts from the current\\nworking repositories rather than from external knowledge sources such as API documentation or\\nonline knowledge bases like Wikipedia. Consequently, it does not introduce substantial factual'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='knowledge regarding background, libraries, or APIs. Second, employing the RAG-based method\\ncan mitigate hallucinations related to Task Requirement Conflicts and Project Context Conflicts\\nto a greater extent than those related to Factual Knowledge Conflicts (see the first and third sub-\\nfigures). Specifically, RAG has successfully resolved approximately 8% of Project Context Conflicts\\nand about 6% of Task Requirement Conflicts . This improvement can be attributed to the code'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='snippets retrieved using the current task descriptions, which enrich the information regarding the\\nrequirements and provide project-specific context. Third, while the RAG-based approach mitigates\\nsome hallucinations, it can also introduce new ones or alter existing hallucination types. For instance,\\napproximately 30% of code generation tasks that were accurately handled by the RAW Method\\nare now mishandled by the RAG-based Mitigation (see the last sub-figure) due to the introduction'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='of new hallucinations. This may occur because the retrieved code snippets can sometimes act as\\nnoise rather than providing informative context, highlighting the importance of effective retrieval\\nalgorithms when implementing RAG-based mitigation.\\nTo further illustrate the effectiveness of the hallucination mitigation, we conduct two case studies.\\nAs shown in Figure 13, in the Raw method, which only provides a docstring and a function signature,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='CodeGen incorrectly uses the replace function and fails to convert scripts to one-line commands.\\nIn contrast, with the RAG-based method, CodeGen correctly uses thesplitlines function, aligning\\nwith the ground-truth and successfully addressing the requirement. In addition, the RAG-based\\nmethod can also effectively mitigate Project Context Conflicts . As shown in Figure 14, in the Raw\\nmethod, ChatGPT attempts to use the self.items.popitem() API, which does not exist in the'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 16, 'page_label': '17', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='repository, leading to hallucinated generation. In contrast, with the RAG-based mitigation, ChatGPT\\ncorrectly implements the requirement using the self.pop() function.\\n6 Discussion\\nWe provide implications for future research on the hallucinations in practical LLM-based code\\ngeneration.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content=\"ISSTA022:18 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\ndefoneline(script,separator=’&&’):returnseperator.join(script.splitlines())\\ndefoneline(script,separator=’&&’):returnseperator.join(textwrap.dedent(script).strip().splitlines())\\ndefoneline(script,separator=’&&'):returnscript.replace(’\\\\t',seperator)\\nGround-truth\\nDocstring convertsascripttoonelinecommand.THisisusefultorunasinglesshcommandandpassaonelinescript.:paramscript::return:\\nRawMethod\"),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content=\"RawMethod\\nCorrectly convert scripts to one-line commands\\nTask Requirement Conflicts\\nRAG-basedMitigation\\nFig. 13. Example: hallucination mitigation in Task Requirement Conflicts.\\ndefpopitem(self):try:key=next(iter(self.__order))exceptStopIteration:raiseKeyError('%sisempty'%type(self).__name__)fromNoneelse:   return(key,self.pop(key))\\ndefpopitem(self):try:key=next(iter(self.__order))exceptStopIteration:raiseKeyError('%sisempty'%type(self).__name__)fromNoneelse:   return(key,self.pop(key))\"),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content=\"defpopitem(self):returnself.items.popitem( )\\nGround-truth\\nDocstring Removeandreturnthe`(key,value)`pairfirstinserted.\\nRawMethod\\nProper use of  the repository's internal API\\nAttributeError: 'self  ' object has no attribute 'items'\\nRAG-basedMitigation\\nFig. 14. Example: hallucination mitigation in Project Context Conflicts.\\nDeveloping hallucination identification techniques : Through our study, we find three major\"),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='categories of hallucinations in the LLM-based code generation. Some hallucinations likeDependency\\nConflicts, Environment Conflicts and API Knowledge Conflicts can be detected by using static analysis\\n(e.g., undefined variables or wrong API methods) or dynamic test execution (runtime errors or test\\nfailures), making it relatively easy for developers to recognize and locate the relevant code issues.\\nHowever, certain hallucinations, such as incomplete functionality and security issues, are very'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='difficult for developers to detect and correct, as they can likely pass static checks and all test cases. As\\na result, LLM-generated code containing these hallucinations may be introduced into development\\nprojects and even real production environments, leading to unreliable software systems and severe\\nsecurity risks. Existing hallucination localization approaches [4, 29] based on LLM self-feedback\\nmethods can detect hallucinations to a certain extent. However, these approaches heavily rely on'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 17, 'page_label': '18', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='the current model’s capabilities and cannot address the fundamental limitations imposed by the\\ntraining corpora. Therefore, in future work, researchers may consider developing more effective\\ntechniques to quickly and precisely identify and localize hallucinations in LLM-generated code.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:19\\nDeveloping more effective hallucination mitigation techniques : In Section 5, we explore\\nthe feasibility of applying a lightweight RAG-based method to mitigate hallucinations in LLM-based\\ncode generation. While the method demonstrates effectiveness in mitigating hallucinations such\\nas undefined attributes, the potentials of RAG need to be further explored. For example, we only'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='construct retrieval corpus using current code repository, leading to the augmented information\\nis insufficient to mitigate many hallucinations such as background knowledge conflicts. In the\\nfuture, we can integrate more comprehensive knowledge sources like online search engines, API\\ndocuments, and StackOverflow discussions. In addition to RAG techniques, other methods such as\\ninput query refinement [12, 30] and multi-agent systems [17] can also be leveraged to achieve an'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='iterative process of (i) clarifying task requirements, (ii) generating code, (iii) running test cases,\\nand (iv) mitigating hallucinations. To achieve this, we need to design the appropriate interaction\\nprotocols between agents and relevant tools (e.g., search engines and static analysis tools) and\\napply suitable prompting strategies.\\n7 Threats to Validity\\nExternal Validity. Threats to external validity mainly concern the generalizability of our findings.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='We focused on Python when exploring the taxonomy and root causes of hallucinations in LLM-\\nbased code generation due to its simplicity and ease of use. Constructing hallucination taxonomies\\nfor other programming languages and comparing them with our current taxonomy is a valuable\\nfuture direction. Another potential threat is the limited scale of the adopted CoderEval dataset,\\nwhich contains only 230 coding tasks. To mitigate this, we selected six LLMs and had each generate'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='10 code snippets for each task to ensure a sufficient number of annotations.\\nInternal Validity. Threats to internal validity primarily concern the manual annotation process\\nin taxonomy construction. A key issue is the absence of formal inter-rater reliability measure for\\nannotating hallucinations. To address this, discrepancies were discussed and resolved in annotator\\nmeetings to ensure a consistent annotation protocol, with each identified hallucination receiving'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='a mutually agreed-upon label. Additionally, to ensure consistency in our findings, one author\\nreviewed all labeled data. Another potential threat is model bias during the annotation process. To\\nmitigate this, we mixed the generation results of the six models before annotation.\\nConstruct Validity. Threats to construct validity are related to evaluating our hallucination\\nmitigation approach. To alleviate these threats, we conducted experiments on six models using'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='test cases available in the CoderEval dataset, a standard method for evaluating the correctness of\\ngenerated code.\\n8 Conclusion\\nIn this paper, we conduct an empirical study on code-generated hallucinations of large models in\\nthe practical development scenarios and through a full manual analysis, we construct a taxonomy of\\nhallucinations and follow up with further hallucination classifications. Based on the hallucinations'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 18, 'page_label': '19', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='found, we provide a deeper discussion of the causes of hallucinations and the distribution of\\nhallucinations in different LLMs. At last, we implement a RAG-based approach for hallucination\\nmitigation and further discuss potential hallucination mitigation approaches.\\n9 Data Availability\\nWe provide the replication package for this study at https://github.com/DeepSoftwareAnalytics/\\nLLMCodingHallucination.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:20 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\nAcknowledgments\\nThis work is supported by CCF-Huawei Populus Grove Fund CCF-HuaweiSE202403. This work is\\nsupported by the Guangdong Basic and Applied Basic Research Foundation (2023A1515012292)\\nand CCF - Sangfor ’Yuanwang’ Research Fund.\\nReferences\\n[1] [n. d.]. AUTOSAR - Wikipedia Page . Retrieved Nov. 1, 2024 from https://en.wikipedia.org/wiki/AUTOSAR'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='[2] [n. d.]. Django - The web framework for perfectionists with deadlines. Retrieved Nov. 1, 2024 from https://www.\\ndjangoproject.com/\\n[3] [n. d.]. OCFL - Specifications . Retrieved Nov. 1, 2024 from https://ocfl.io/1.1/spec/#storage-root\\n[4] Ayush Agrawal, Mirac Suzgun, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language Models Know When\\nThey’re Hallucinating References? arXiv preprint arXiv:2305.18248 (2023).'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='[5] Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas\\nMuennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian\\nZi, Joel Lamy-Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert,\\nFrancesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li,\\nDzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra.\\n2023. SantaCoder: don’t reach for the stars! CoRR abs/2301.03988 (2023). https://doi.org/10.48550/ARXIV.2301.03988\\narXiv:2301.03988\\n[6] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models.\\nCoRR abs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732\\n[7] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with\\ncode-generating models. Proceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85–111.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='[8] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,\\nKyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow,\\nBen Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. CoRR\\nabs/2204.06745 (2022). https://doi.org/10.48550/ARXIV.2204.06745 arXiv:2204.06745'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='[9] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021.GPT-Neo: Large Scale Autoregressive Language\\nModeling with Mesh-Tensorflow . https://doi.org/10.5281/zenodo.5297715 If you use this software, please cite it using\\nthese metadata..\\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison\\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,\\nLukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias\\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas\\nTezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira\\nMurati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\\nZaremba. 2021. Evaluating Large Language Models Trained on Code. CoRR abs/2107.03374 (2021). arXiv:2107.03374\\nhttps://arxiv.org/abs/2107.03374\\n[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374 (2021).\\n[12] Kaustubh D Dhole, Ramraj Chandradevan, and Eugene Agichtein. 2023. An interactive query generation assistant\\nusing LLM-based prompt modification and user feedback. arXiv preprint arXiv:2311.11226 (2023).\\n[13] Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='and Yiling Lou. 2023. Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation.\\narXiv preprint arXiv:2308.01861 (2023).\\n[14] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke\\nZettlemoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh\\nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 19, 'page_label': '20', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='https://openreview.net/pdf?id=hQwb-lbM6EL\\n[15] Mark Grechanik, Collin McMillan, Luca DeFerrari, Marco Comi, Stefano Crespi, Denys Poshyvanyk, Chen Fu, Qing Xie,\\nand Carlo Ghezzi. 2010. An empirical investigation into a large-scale Java open source code repository. In Proceedings\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:21\\nof the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement . 1–10.\\n[16] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al.\\n2024. DeepSeek-Coder: When the Large Language Model Meets Programming–The Rise of Code Intelligence. arXiv\\npreprint arXiv:2401.14196 (2024).'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2401.14196 (2024).\\n[17] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang.\\n2024. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680\\n(2024).\\n[18] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration.\\narXiv preprint arXiv:1904.09751 (2019).'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='arXiv preprint arXiv:1904.09751 (2019).\\n[19] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng,\\nXiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy,\\nchallenges, and open questions. arXiv preprint arXiv:2311.05232 (2023).\\n[20] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Pascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1–38.\\n[21] Shahedul Huq Khandkar. 2009. Open coding. University of Calgary 23, 2009 (2009).\\n[22] Jan H Klemmer, Stefan Albert Horstmann, Nikhil Patnaik, Cordelia Ludden, Cordell Burton Jr, Carson Powers, Fabio\\nMassacci, Akond Rahman, Daniel Votipka, Heather Richter Lipford, et al . 2024. Using AI Assistants in Software'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Development: A Qualitative Study on Security Practices and Concerns. arXiv preprint arXiv:2405.06371 (2024).\\n[23] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida\\nWang, and Tao Yu. 2023. DS-1000: A natural and reliable benchmark for data science code generation. In International\\nConference on Machine Learning . PMLR, 18319–18345.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='[24] Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. 2024. EvoCodeBench: An Evolving Code Generation\\nBenchmark Aligned with Real-World Code Repositories. arXiv preprint arXiv:2404.00599 (2024).\\n[25] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\\nChristopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,\\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo\\nWang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan\\nZhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey\\nSchoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-\\nGavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean\\nHughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='you! CoRR abs/2305.06161 (2023). https://doi.org/10.48550/ARXIV.2305.06161 arXiv:2305.06161\\n[26] Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James\\nKeeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,\\nXinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-\\nLevel Code Generation with AlphaCode. CoRR abs/2203.07814 (2022). https://doi.org/10.48550/ARXIV.2203.07814\\narXiv:2203.07814\\n[27] Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and Li Zhang. 2024. Exploring and Evaluating\\nHallucinations in LLM-Powered Code Generation. arXiv preprint arXiv:2404.00971 (2024).'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='[28] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint\\narXiv:2402.19173 (2024).\\n[29] Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination\\ndetection for generative large language models. arXiv preprint arXiv:2303.08896 (2023).'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='[30] Fangwen Mu, Lin Shi, Song Wang, Zhuohao Yu, Binquan Zhang, Chenxue Wang, Shichao Liu, and Qing Wang. 2023.\\nClarifyGPT: Empowering LLM-based Code Generation with Intention Clarification. arXiv preprint arXiv:2310.10996\\n(2023).\\n[31] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.\\nCodegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474\\n(2022).'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 20, 'page_label': '21', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='(2022).\\n[32] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\\n2023. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In The Eleventh\\nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net.\\nhttps://openreview.net/pdf?id=iaYcJKpY2B_\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='ISSTA022:22 Z. Zhang, Y. Wang, C. Wang, E. Shi, Y. Ma, W. Zhong, J. Chen, M. Mao, Z. Zheng\\n[33] OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt.\\n[34] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\\nRemez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,\\nThomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. CoRR abs/2308.12950\\n(2023). https://doi.org/10.48550/ARXIV.2308.12950 arXiv:2308.12950\\n[35] Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md Rafiqul Islam Rabin, Susmit Jha, Prem Devanbu, and'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Toufique Ahmed. 2024. Quality and Trust in LLM-generated Code. arXiv preprint arXiv:2402.02047 (2024).\\n[36] Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022. On the importance of building high-quality training datasets\\nfor neural code search. In Proceedings of the 44th International Conference on Software Engineering . 1609–1620.\\n[37] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A comprehensive'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313 (2024).\\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\\nIllia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual\\nConference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , Isabelle Guyon,'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.).\\n5998–6008. https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\\n[39] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:\\n//github.com/kingoflolz/mesh-transformer-jax.\\n[40] Chong Wang, Xin Peng, Zhenchang Xing, and Xiujie Meng. 2023. Beyond literal meaning: Uncover and explain implicit'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='knowledge in code through wikipedia-based concept linking. IEEE Transactions on Software Engineering 49, 5 (2023),\\n3226–3240.\\n[41] Chong Wang, Xin Peng, Zhenchang Xing, Yue Zhang, Mingwei Liu, Rong Luo, and Xiujie Meng. 2023. Xcos: Explainable\\ncode search based on query scoping and knowledge graph. ACM Transactions on Software Engineering and Methodology\\n32, 6 (2023), 1–28.\\n[42] Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, and Xin Peng. 2024. Teaching Code LLMs to'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Use Autocompletion Tools in Repository-Level Code Generation. arXiv preprint arXiv:2401.06391 (2024).\\n[43] Yue Wang, Hung Le, Akhilesh Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. 2023. CodeT5+: Open Code\\nLarge Language Models for Code Understanding and Generation. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023 , Houda Bouamor, Juan Pino, and'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Kalika Bali (Eds.). Association for Computational Linguistics, 1069–1088. https://aclanthology.org/2023.emnlp-main.68\\n[44] Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained\\nEncoder-Decoder Models for Code Understanding and Generation. In Proceedings of the 2021 Conference on Em-\\npirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for\\nComputational Linguistics, 8696–8708. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.685\\n[45] Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023. Cognitive mirage: A review of hallucinations in\\nlarge language models. arXiv preprint arXiv:2309.06794 (2023).\\n[46] Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Xie. 2024. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings\\nof the 46th IEEE/ACM International Conference on Software Engineering . 1–12.\\n[47] Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022. When language model meets\\nprivate library. arXiv preprint arXiv:2210.17236 (2022).\\n[48] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Zhang, et al. 2021. PanGu- 𝛼: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel\\nComputation. arXiv preprint arXiv:2104.12369 (2021).\\n[49] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu\\nChen. 2023. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint\\narXiv:2303.12570 (2023).'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='arXiv:2303.12570 (2023).\\n[50] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\\nChen, et al. 2023. Siren’s song in the AI ocean: a survey on hallucination in large language models. arXiv preprint\\narXiv:2309.01219 (2023).\\n[51] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 21, 'page_label': '22', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='perspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989\\n(2023).\\n[52] Li Zhong and Zilong Wang. 2024. Can LLM Replace Stack Overflow? A Study on Robustness and Reliability of Large\\nLanguage Model Code Generation. InProceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 21841–21849.\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation ISSTA022:23\\n[53] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan\\nZhan, Junda He, Indraneil Paul, et al. 2024. Bigcodebench: Benchmarking code generation with diverse function calls\\nand complex instructions. arXiv preprint arXiv:2406.15877 (2024).\\nReceived 2024-10-31; accepted 2025-03-31'),\n",
       " Document(metadata={'producer': 'þÿpdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0; ConfPub - issta25main-p194-p rev-7f367558cf-129290 p481 on 2025-06-22T15:08:43+00:00; modified using iText 4.2.0 by 1T3XT', 'creator': 'LaTeX with acmart 2024/12/28 v2.12 Typesetting articles for the Association for Computing Machinery and hyperref 2024-01-20 v7.01h Hypertext links for LaTeX; Conference Publishing Consulting', 'creationdate': '2025-06-18T06:30:54+00:00', 'keywords': 'Repository-Level Code Generation; Hallucination; Large Language Models', 'moddate': '2025-09-17T00:54:08-07:00', 'trapped': '/False', 'subject': 'Proc. ACM Softw. Eng. 2025.2:481-503', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'author': 'Ziyao Zhang, Chong Wang, Yanlin Wang, Ensheng Shi, Yuchi Ma, Wanjun Zhong, Jiachi Chen, Mingzhi Mao, and Zibin Zheng', 'title': 'LLM Hallucinations in Practical Code Generation: Phenomena, Mechanism, and Mitigation', 'source': '../data/text_files/pdf/llm_hallucinations.pdf', 'total_pages': 23, 'page': 22, 'page_label': '23', 'source_file': 'llm_hallucinations.pdf', 'file_type': 'pdf'}, page_content='Received 2024-10-31; accepted 2025-03-31\\nProc. ACM Softw. Eng., Vol. 2, No. ISSTA, Article ISSTA022. Publication date: July 2025.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 358 documents:\n",
      "Adding 358 documents to the vector store...\n",
      "Documents added successfully. Total documents in collection: 358\n",
      "Successfully added 358 documents.\n"
     ]
    }
   ],
   "source": [
    "# convert the text to enbeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "# generate embeddings\n",
    "embeddings = embedding_manager.embed_documents(texts)\n",
    "\n",
    "#store in the vectorDB\n",
    "vectorstore._add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG retriever pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGretriever at 0x1766b2a50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGretriever:\n",
    "    \"Handling query based retrieval from the vector store\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        initialize the retriever\n",
    "\n",
    "        Args:\n",
    "            vector_store: vector store containing document embeddings\n",
    "            embedding_manager: embedding manager to generate query embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a given query\n",
    "\n",
    "        Args:\n",
    "            query (str): The input query string.\n",
    "            top_k (int): Number of top documents to retrieve.\n",
    "            score_threshold (float): Minimum similarity score threshold.\n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved info and similarity scores.\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top k:{top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate embedding for the query\n",
    "        query_embedding = self.embedding_manager.embed_documents([query])[0]\n",
    "        print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
    "\n",
    "        # search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGretriever(vectorstore,embedding_manager)\n",
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the XOR problem in Neural Networks?'\n",
      "Top k:5, Score threshold: 0.0\n",
      "Generating embeddings for 1 documents:\n",
      "Query embedding dimension: 384\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '4937d1ec-2311-4297-bc19-e75c79322dcb',\n",
       "  'content': 'from the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly\\nseparable\\nfunction. Of course we could draw a boundary with a curve, or some other function,\\nbut not a single line.\\n6.2.1 The solution: neural networks\\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\\nculated by a layered network of perceptron units. Rather than see this with networks\\nof simple perceptrons, however, let’s see how to compute XOR using two layers of',\n",
       "  'metadata': {'keywords': '',\n",
       "   'page_label': '5',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'subject': '',\n",
       "   'page': 4,\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'author': '',\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'doc_index': 26,\n",
       "   'content_length': 489,\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 27,\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'creationdate': '2025-08-24T11:50:53-07:00'},\n",
       "  'similarity_score': 0.36060798168182373,\n",
       "  'distance': 0.6393920183181763,\n",
       "  'rank': 1},\n",
       " {'id': '2eaed373-9652-4bd5-92d5-5f059e4d3d92',\n",
       "  'content': '6.3 • F EEDFORWARD NEURAL NETWORKS 7\\n0\\n0 1\\n1\\nx1\\nx2\\na) The original x space\\n0\\n0 1\\n1\\nh1\\nh2\\n2\\nb) The new (linearly separable) h space\\nFigure 6.7 The hidden layer forming a new representation of the input. (b) shows the\\nrepresentation of the hidden layer, h, compared to the original input representation x in (a).\\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\\npossible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.',\n",
       "  'metadata': {'doc_index': 34,\n",
       "   'page_label': '7',\n",
       "   'author': '',\n",
       "   'subject': '',\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'trapped': '/False',\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'total_pages': 27,\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'page': 6,\n",
       "   'content_length': 497,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'title': ''},\n",
       "  'similarity_score': 0.246690034866333,\n",
       "  'distance': 0.753309965133667,\n",
       "  'rank': 2},\n",
       " {'id': '36970316-76c1-47e3-b924-a04e63fe1695',\n",
       "  'content': 'ReLU-based units following Goodfellow et al. (2016). Fig. 6.6 shows a ﬁgure with\\nthe input being processed by two layers of neural units. The middle layer (called\\nh) has two units, and the output layer (called y) has one unit. A set of weights and\\nbiases are shown that allows the network to correctly compute the XOR function.\\nLet’s walk through what happens with the input x = [0, 0]. If we multiply each\\ninput value by the appropriate weight, sum, and then add the biasb, we get the vector',\n",
       "  'metadata': {'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'total_pages': 27,\n",
       "   'page': 4,\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'page_label': '5',\n",
       "   'doc_index': 27,\n",
       "   'content_length': 492,\n",
       "   'keywords': '',\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2'},\n",
       "  'similarity_score': 0.22052031755447388,\n",
       "  'distance': 0.7794796824455261,\n",
       "  'rank': 3},\n",
       " {'id': '44c36819-8c65-4d74-b035-03213b12c54d',\n",
       "  'content': '6.2 • T HE XOR PROBLEM 5\\nIt’s very easy to build a perceptron that can compute the logical AND and OR\\nfunctions of its binary inputs; Fig. 6.4 shows the necessary weights.\\nx1\\nx2\\n+1\\n-1\\n1\\n1\\nx1\\nx2\\n+1\\n0\\n1\\n1\\n(a) (b)\\nFigure 6.4 The weights w and bias b for perceptrons for computing logical functions. The\\ninputs are shown asx1 and x2 and the bias as a special node with value+1 which is multiplied\\nwith the bias weight b. (a) logical AND, with weights w1 = 1 and w2 = 1 and bias weight',\n",
       "  'metadata': {'keywords': '',\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'page': 4,\n",
       "   'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 27,\n",
       "   'page_label': '5',\n",
       "   'subject': '',\n",
       "   'content_length': 480,\n",
       "   'author': '',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'title': '',\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 22,\n",
       "   'moddate': '2025-08-24T11:50:53-07:00'},\n",
       "  'similarity_score': 0.20236635208129883,\n",
       "  'distance': 0.7976336479187012,\n",
       "  'rank': 4},\n",
       " {'id': '03dc0c56-ea08-433f-9c71-7fb9cfd7ec82',\n",
       "  'content': 'b = −1. (b) logical OR, with weights w1 = 1 and w2 = 1 and bias weight b = 0. These\\nweights/biases are just one from an inﬁnite number of possible sets of weights and biases that\\nwould implement the functions.\\nIt turns out, however, that it’s not possible to build a perceptron to compute\\nlogical XOR! (It’s worth spending a moment to give it a try!)\\nThe intuition behind this important result relies on understanding that a percep-',\n",
       "  'metadata': {'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 27,\n",
       "   'keywords': '',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 432,\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'author': '',\n",
       "   'title': '',\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'page_label': '5',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'subject': '',\n",
       "   'doc_index': 23,\n",
       "   'page': 4},\n",
       "  'similarity_score': 0.198178231716156,\n",
       "  'distance': 0.801821768283844,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is the XOR problem in Neural Networks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is feedforward neural network?'\n",
      "Top k:5, Score threshold: 0.0\n",
      "Generating embeddings for 1 documents:\n",
      "Query embedding dimension: 384\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '954479a7-2bf5-4b4c-b8bf-af088a8b8d46',\n",
       "  'content': '(2016).\\n6.3 Feedforward Neural Networks\\nLet’s now walk through a slightly more formal presentation of the simplest kind of\\nneural network, the feedforward network. A feedforward network is a multilayerfeedforward\\nnetwork\\nnetwork in which the units are connected with no cycles; the outputs from units in\\neach layer are passed to units in the next higher layer, and no outputs are passed\\nback to lower layers. (In Chapter 13 we’ll introduce networks with cycles, called\\nrecurrent neural networks.)',\n",
       "  'metadata': {'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'page_label': '7',\n",
       "   'subject': '',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'total_pages': 27,\n",
       "   'page': 6,\n",
       "   'doc_index': 35,\n",
       "   'author': '',\n",
       "   'content_length': 496,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'keywords': '',\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'file_type': 'pdf',\n",
       "   'title': '',\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.4933147430419922,\n",
       "  'distance': 0.5066852569580078,\n",
       "  'rank': 1},\n",
       " {'id': '6d5259b4-acb2-4381-9a34-cb28ab9b66b1',\n",
       "  'content': '6.6 Training Neural Nets\\nA feedforward neural net is an instance of supervised machine learning in which we\\nknow the correct output y for each observation x. What the system produces, via\\nEq. 6.13, is ˆy, the system’s estimate of the truey. The goal of the training procedure\\nis to learn parameters W[i] and b[i] for each layer i that make ˆy for each training\\nobservation as close as possible to the true y.\\nIn general, we do all this by drawing on the methods we introduced in Chapter 4',\n",
       "  'metadata': {'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'keywords': '',\n",
       "   'content_length': 488,\n",
       "   'trapped': '/False',\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'subject': '',\n",
       "   'total_pages': 27,\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'page': 17,\n",
       "   'author': '',\n",
       "   'doc_index': 101,\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'title': '',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'page_label': '18'},\n",
       "  'similarity_score': 0.32911360263824463,\n",
       "  'distance': 0.6708863973617554,\n",
       "  'rank': 2},\n",
       " {'id': '7de8dd1c-38a6-4f2e-9e4c-62204be89613',\n",
       "  'content': 'chapter we introduce the neural net applied to classiﬁcation. The architecture we\\nintroduce is called a feedforward network because the computation proceeds iter-feedforward\\natively from one layer of units to the next. The use of modern neural nets is often\\ncalled deep learning, because modern networks are often deep (have many layers).deep learning\\nNeural networks share much of the same mathematics as logistic regression. But',\n",
       "  'metadata': {'page': 0,\n",
       "   'keywords': '',\n",
       "   'author': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'page_label': '1',\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'total_pages': 27,\n",
       "   'doc_index': 2,\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'content_length': 430,\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'title': '',\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'trapped': '/False',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.32017117738723755,\n",
       "  'distance': 0.6798288226127625,\n",
       "  'rank': 3},\n",
       " {'id': '9739278a-0086-4193-977e-d26b2bbb05f6',\n",
       "  'content': 'point the name stuck.\\nSimple feedforward networks have three kinds of nodes: input units, hidden\\nunits, and output units.\\nFig. 6.8 shows a picture. The input layerx is a vector of simple scalar values just\\nas we saw in Fig. 6.2.\\nThe core of the neural network is thehidden layer h formed of hidden units hi,hidden layer\\neach of which is a neural unit as described in Section 6.1, taking a weighted sum of\\nits inputs and then applying a non-linearity. In the standard architecture, each layer',\n",
       "  'metadata': {'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'page': 6,\n",
       "   'doc_index': 37,\n",
       "   'page_label': '7',\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 27,\n",
       "   'title': '',\n",
       "   'subject': '',\n",
       "   'content_length': 491,\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'creator': 'LaTeX with hyperref'},\n",
       "  'similarity_score': 0.2849063277244568,\n",
       "  'distance': 0.7150936722755432,\n",
       "  'rank': 4},\n",
       " {'id': 'b399ff30-b053-40c8-9bd0-69b28d01dd81',\n",
       "  'content': 'tions are at the end of the chapter.\\n6.7 Summary\\n• Neural networks are built out ofneural units, originally inspired by biological\\nneurons but now simply an abstract computational device.\\n• Each neural unit multiplies input values by a weight vector, adds a bias, and\\nthen applies a non-linear activation function like sigmoid, tanh, or rectiﬁed\\nlinear unit.\\n• In a fully-connected, feedforward network, each unit in layer i is connected\\nto each unit in layer i +1, and there are no cycles.',\n",
       "  'metadata': {'page_label': '25',\n",
       "   'total_pages': 27,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
       "   'file_type': 'pdf',\n",
       "   'moddate': '2025-08-24T11:50:53-07:00',\n",
       "   'creationdate': '2025-08-24T11:50:53-07:00',\n",
       "   'title': '',\n",
       "   'content_length': 490,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'source_file': 'neural networks.pdf',\n",
       "   'trapped': '/False',\n",
       "   'producer': 'pdfTeX-1.40.21',\n",
       "   'doc_index': 140,\n",
       "   'subject': '',\n",
       "   'keywords': '',\n",
       "   'page': 24,\n",
       "   'author': '',\n",
       "   'source': '../data/text_files/pdf/neural networks.pdf'},\n",
       "  'similarity_score': 0.15259826183319092,\n",
       "  'distance': 0.8474017381668091,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is feedforward neural network?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-projectyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
